{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEMO of response format capabilities\n",
    "As you can see in the below made up example, the LLM is FORCED to produce outputs in the format laid out in the class called Test. It's important that Test inherits from Pydantic's BaseModel, as this is handled internally by the openai SDK and is also used to parse the text json output back into the class. Once you have the output object, you can access the attributes of the object like normal. The use of Typing is enforced by openai, ensuring that a string is always a string, an integer is always an integer, a List is always a list, and a Literal has to be one of the options you give it. This is perfect for ensuring LLM outputs are bulletproof in their conforming to your expected outputs, meaning that they do not break unexpectedly no matter what the input is.\n",
    "\n",
    "The use of Field, from pydantic allows you to add a description property to the attribute. This description is provided to the LLM too, and so are any docstrings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfd_toolkit.llm import LLM\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, create_model\n",
    "from typing import List, Literal\n",
    "load_dotenv()\n",
    "class Test(BaseModel):\n",
    "    \"\"\"An example response format case, this text is seen by the LLM too!\"\"\"\n",
    "    name: Literal['john', 'sam', 'other'] = Field(..., description='The name of the patient')\n",
    "    sibling_ages: List[str] = Field(..., description='The ages of their siblings')\n",
    "    death_reason: str = Field(..., description='A verbose reason for the patients death')\n",
    "    category: Literal['suicide', 'old-age', 'covid'] = Field(..., description='Which category the death reason falls into')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOPENAI_API_KEY\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/PFD-toolkit/src/pfd_toolkit/llm.py:31\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, api_key, model, base_url)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_url = openai.base_url\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/PFD-toolkit/.venv/lib/python3.12/site-packages/openai/_client.py:110\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    108\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    111\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    113\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "llm = LLM(api_key=os.getenv('OPENAI_API_KEY'), model='gpt-4o-mini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.generate(prompt='johns siblings are 28 and 29 respectively. john died from confusion and suffered from long-covid which caused cognitive impairment that ultimately resulted in his passing...', response_format=Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of PFD report demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportClassification(BaseModel):\n",
    "    \"\"\"The classification of the prevention of future death report based on the given context\"\"\"\n",
    "    classification: Literal['suicide', 'depression', 'covid', 'old age', 'confused']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = llm.generate(prompt='The cause of death in this case was old age, as the patient lived to an impressive 98 years of age.', response_format=ReportClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case.classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My testing of the grammar (response format) results. Please delete or let me know once you are happy and we'll clean the notebook up. Sorry it's here, I started here and don't wanna move it and re do the LLM calls just to regenerate the cell outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfd_toolkit import PFDScraper, llm\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load OpenAI API key\n",
    "load_dotenv(\"api.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm_client = llm.LLM(api_key=openai_api_key)\n",
    "\n",
    "# Run the scraper! :D\n",
    "scraper = PFDScraper(\n",
    "    llm=llm_client,\n",
    "    category=\"all\",\n",
    "    date_from=\"2024-01-10\",\n",
    "    date_to=\"2024-01-11\",\n",
    "    html_scraping=False,\n",
    "    pdf_fallback=False,\n",
    "    llm_fallback=True,\n",
    "    # docx_conversion=\"LibreOffice\", # Doesn't currently seem to work; need to debug.\n",
    "    include_time_stamp=False,\n",
    "    delay_range=None,\n",
    "    verbose=False,\n",
    ")\n",
    "scraper.scrape_reports()\n",
    "#scraper.estimate_api_costs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_report = scraper.reports.iloc[0:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_report.InvestigationAndInquest[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.verbose = True\n",
    "result = scraper.run_llm_fallback(reports_df=sample_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
