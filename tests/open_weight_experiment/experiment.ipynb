{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc171e8",
   "metadata": {},
   "source": [
    "# Open-weight model experiment\n",
    "\n",
    "Run the open-weight model comparison on the consensus dataset without relying on local data files. The notebook pulls the data directly from GitHub, loads any available API keys from env files next to the notebook, and saves results locally to a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5123e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q pfd_toolkit ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60204560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API key\n",
    "\n",
    "import os\n",
    "from ipywidgets import Password, Button, VBox, HTML\n",
    "\n",
    "api_key_input = Password(\n",
    "    description=\"API key:\",\n",
    "    placeholder=\"Paste your key here\"\n",
    ")\n",
    "status = HTML(\"\")\n",
    "button = Button(description=\"Set key\")\n",
    "\n",
    "def set_api_key(_):\n",
    "    if api_key_input.value:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key_input.value\n",
    "        # clear the visible value\n",
    "        api_key_input.value = \"\"\n",
    "        status.value = \"<b>API key stored in this session.</b>\"\n",
    "    else:\n",
    "        status.value = \"<b>No key entered.</b>\"\n",
    "\n",
    "button.on_click(set_api_key)\n",
    "\n",
    "ui = VBox([api_key_input, button, status])\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import tempfile\n",
    "from urllib.parse import quote\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "from pfd_toolkit import LLM, Screener\n",
    "from pfd_toolkit.config import GeneralConfig\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "def ensure_secret(env_name: str) -> str:\n",
    "    if env_name not in os.environ or not os.environ[env_name]:\n",
    "        os.environ[env_name] = getpass(f\"Enter {env_name}: \")\n",
    "    return os.environ[env_name]\n",
    "\n",
    "OPENAI_API_KEY = api_key\n",
    "\n",
    "GITHUB_BASE = \"https://raw.githubusercontent.com/Sam-Osian/PFD-toolkit/open-model-exp\"\n",
    "DATA_URL = f\"{GITHUB_BASE}/ons_replication/{quote('PFD Toolkit--Consensus Comparison.xlsx')}\"\n",
    "SHEET_NAME = \"Consensus annotations\"\n",
    "RESULTS_PATH = NOTEBOOK_DIR / \"model_comparison.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1245de2",
   "metadata": {},
   "source": [
    "## Start Ollama in Colab\n",
    "\n",
    "Install and start the Ollama daemon locally so the open-weight models can be downloaded and served within this runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8503706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "OLLAMA_PORT = 11434\n",
    "OLLAMA_BASE_URL = f\"http://localhost:{OLLAMA_PORT}/v1\"\n",
    "\n",
    "if not shutil.which(\"ollama\"):\n",
    "    print(\"Installing Ollama...\")\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Start the Ollama server if it isn't already running\n",
    "try:\n",
    "    requests.get(f\"http://localhost:{OLLAMA_PORT}/api/tags\", timeout=2)\n",
    "    print(\"Ollama is already running.\")\n",
    "except Exception:\n",
    "    print(\"Starting ollama serve...\")\n",
    "    ollama_process = subprocess.Popen(\n",
    "        [\"ollama\", \"serve\"],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "    for _ in range(60):\n",
    "        try:\n",
    "            requests.get(f\"http://localhost:{OLLAMA_PORT}/api/tags\", timeout=2)\n",
    "            print(\"Ollama is ready.\")\n",
    "            break\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        raise RuntimeError(\"Ollama failed to start. Check the logs above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e7e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OLLAMA_MODELS = [\n",
    "    {\"name\": \"mistral-nemo:12b\", \"temperature\": 0, \"timeout\": 10**9},\n",
    "    #{\"name\": \"mistral-small:22b\", \"temperature\": 0, \"timeout\": 10**9},\n",
    "    #{\"name\": \"mistral-small:24b\", \"temperature\": 0, \"timeout\": 10**9},\n",
    "]\n",
    "\n",
    "for spec in OLLAMA_MODELS:\n",
    "    print(f\"Pulling {spec['name']} (this may take a few minutes)...\")\n",
    "    subprocess.run([\"ollama\", \"pull\", spec[\"name\"]], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = Path(tempfile.gettempdir()) / \"pfd_toolkit_consensus.xlsx\"\n",
    "response = requests.get(DATA_URL, timeout=60)\n",
    "response.raise_for_status()\n",
    "data_path.write_bytes(response.content)\n",
    "\n",
    "print(f\"Dataset downloaded to {data_path}\")\n",
    "\n",
    "df = pd.read_excel(data_path, sheet_name=SHEET_NAME)\n",
    "consensus_column_name = \"Post-consensus verdict: Is this a child suicide case? (Yes or No)\"\n",
    "\n",
    "raw_consensus = df[consensus_column_name]\n",
    "raw_values = raw_consensus.dropna().astype(str).str.strip()\n",
    "print(\"Unique raw consensus values:\")\n",
    "for value in sorted(raw_values.unique()):\n",
    "    print(f\"- {value}\")\n",
    "\n",
    "renamed = df.rename(\n",
    "    columns={\n",
    "        \"Ref\": GeneralConfig.COL_ID,\n",
    "        \"Investigation section\": GeneralConfig.COL_INVESTIGATION,\n",
    "        \"Circumstances of death section\": GeneralConfig.COL_CIRCUMSTANCES,\n",
    "        \"Matters of concern section\": GeneralConfig.COL_CONCERNS,\n",
    "        consensus_column_name: \"consensus\",\n",
    "    }\n",
    ")\n",
    "\n",
    "reports = renamed[\n",
    "    [\n",
    "        GeneralConfig.COL_ID,\n",
    "        GeneralConfig.COL_INVESTIGATION,\n",
    "        GeneralConfig.COL_CIRCUMSTANCES,\n",
    "        GeneralConfig.COL_CONCERNS,\n",
    "        \"consensus\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "normalized_consensus = reports[\"consensus\"].astype(str).str.strip().str.lower()\n",
    "print(\"Normalised consensus values before boolean conversion:\")\n",
    "for value in sorted(normalized_consensus.unique()):\n",
    "    print(f\"- {value}\")\n",
    "\n",
    "reports[\"consensus\"] = normalized_consensus == \"yes\"\n",
    "print(\"Boolean consensus values after conversion:\")\n",
    "print(sorted(reports[\"consensus\"].unique().tolist()))\n",
    "\n",
    "reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a774f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_SPECS = [\n",
    "    # OpenAI API models\n",
    "    {\"name\": \"gpt-4.1\", \"temperature\": 0},\n",
    "    {\"name\": \"gpt-4.1-mini\", \"temperature\": 0},\n",
    "    {\"name\": \"gpt-4.1-nano\", \"temperature\": 0},\n",
    "]\n",
    "\n",
    "# Append Ollama-hosted models using the shared base URL and API key\n",
    "MODEL_SPECS += [\n",
    "    {\n",
    "        \"name\": spec[\"name\"],\n",
    "        \"temperature\": spec[\"temperature\"],\n",
    "        \"base_url\": OLLAMA_BASE_URL,\n",
    "        \"api_key\": \"ollama\",\n",
    "        \"timeout\": spec.get(\"timeout\", 20),\n",
    "    }\n",
    "    for spec in OLLAMA_MODELS\n",
    "]\n",
    "\n",
    "user_query = \"\"\"\n",
    "Identify cases where the deceased was aged 18 or younger *clearly at the time of death* **and**\n",
    "the death was due to suicide. If suicide is not explicitly stated, you can use a strict balance of\n",
    "probabilities threshold to determine it as such.\n",
    "\n",
    "Age may not be explicitly stated, but could be implied through references such as\n",
    "recent use of child or adolescent services (e.g. CAMHS), attending school years\n",
    "(e.g. \"Year 10\"), or similar contextual indicators of being under 18 (again, under a\n",
    "strict balance of probabilities threshold).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from adjustText import adjust_text\n",
    "\n",
    "results_columns = [\"model\", \"accuracy\", \"sensitivity\", \"specificity\", \"elapsed_seconds\"]\n",
    "if RESULTS_PATH.exists():\n",
    "    results_df = pd.read_csv(RESULTS_PATH)\n",
    "    for col in results_columns:\n",
    "        if col not in results_df:\n",
    "            results_df[col] = pd.NA\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=results_columns)\n",
    "\n",
    "completed_models = set(results_df[\"model\"].astype(str))\n",
    "\n",
    "for spec in MODEL_SPECS:\n",
    "    if spec[\"name\"] in completed_models:\n",
    "        print(f\"Skipping {spec['name']} (already evaluated)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Testing model: {spec['name']}\")\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    llm_client = LLM(\n",
    "        api_key=spec.get(\"api_key\", os.getenv(\"OPENAI_API_KEY\")),\n",
    "        base_url=spec.get(\"base_url\"),\n",
    "        max_workers=8,\n",
    "        model=spec[\"name\"],\n",
    "        seed=12345,\n",
    "        timeout=spec.get(\"timeout\", 20),\n",
    "        temperature=1 if spec[\"name\"].startswith(\"gpt-5\") else spec[\"temperature\"],\n",
    "    )\n",
    "\n",
    "    screener = Screener(\n",
    "        llm=llm_client,\n",
    "        reports=reports,\n",
    "        include_investigation=True,\n",
    "        include_circumstances=True,\n",
    "        include_concerns=True,\n",
    "    )\n",
    "\n",
    "    classified = screener.screen_reports(\n",
    "        search_query=user_query,\n",
    "        filter_df=False,\n",
    "        result_col_name=\"model_pred\",\n",
    "    )\n",
    "\n",
    "    elapsed_seconds = time.perf_counter() - start_time\n",
    "\n",
    "    pred = classified[\"model_pred\"].astype(bool)\n",
    "    truth = classified[\"consensus\"].astype(bool)\n",
    "\n",
    "    tp = (pred & truth).sum()\n",
    "    tn = ((~pred) & (~truth)).sum()\n",
    "    fp = (pred & ~truth).sum()\n",
    "    fn = ((~pred) & truth).sum()\n",
    "\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy = (tp + tn) / total if total else float(\"nan\")\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) else float(\"nan\")\n",
    "    specificity = tn / (tn + fp) if (tn + fp) else float(\"nan\")\n",
    "\n",
    "    results_df = pd.concat(\n",
    "        [\n",
    "            results_df,\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"model\": spec[\"name\"],\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"sensitivity\": sensitivity,\n",
    "                        \"specificity\": specificity,\n",
    "                        \"elapsed_seconds\": elapsed_seconds,\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    results_df.to_csv(RESULTS_PATH, index=False)\n",
    "    completed_models.add(spec[\"name\"])\n",
    "\n",
    "    if spec.get(\"base_url\") == OLLAMA_BASE_URL:\n",
    "        print(f\"Removing Ollama model {spec['name']} to free space...\")\n",
    "        subprocess.run([\"ollama\", \"rm\", spec[\"name\"]], check=False)\n",
    "\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = results_df.copy()\n",
    "plot_df[\"accuracy\"] = pd.to_numeric(plot_df[\"accuracy\"], errors=\"coerce\")\n",
    "\n",
    "PARAMS_BILLIONS = {\n",
    "    \"google/gemma-3-4b-it\": 4,\n",
    "    \"google/gemma-3-12b-it\": 12,\n",
    "    \"mistral-nemo:12b\": 12,\n",
    "    \"mistral-small:22b\": 22,\n",
    "    \"mistral-small:24b\": 24,\n",
    "    \"gemma3:12b\": 12,\n",
    "    \"gemma3:27b\": 27,\n",
    "    \"gemma2:27b\": 27,\n",
    "    \"qwen3:32b\": 32,\n",
    "    \"qwen3:30b\": 30,\n",
    "    \"qwen2.5:72b\": 72,\n",
    "    \"qwen2.5:32b\": 32,\n",
    "    \"llava:34b\": 34,\n",
    "    \"phi4:14b\": 14,\n",
    "    \"llama3:70b\": 70,\n",
    "}\n",
    "\n",
    "plot_df[\"params_billion\"] = plot_df[\"model\"].map(PARAMS_BILLIONS)\n",
    "plot_df = plot_df.dropna(subset=[\"accuracy\"])\n",
    "\n",
    "# Styling borrowed from tests/model_comparison_vis.py\n",
    "X_PAD_RIGHT = 1.0\n",
    "X_MIN_PAD = 0.5\n",
    "INIT_DX = 0.4\n",
    "INIT_DY = 0.0\n",
    "\n",
    "JITTER_X = 0.05\n",
    "JITTER_Y = 0.001\n",
    "\n",
    "EXP_TEXT = (1.02, 1.55)\n",
    "EXP_POINTS = (1.05, 1.20)\n",
    "FORCE_TEXT = 1\n",
    "FORCE_POINTS = 0.4\n",
    "N_STEPS = 1000\n",
    "\n",
    "ARROW = dict(arrowstyle='-', color='lightgray', lw=1, alpha=0.7)\n",
    "\n",
    "BLUE = \"#3B82F6\"\n",
    "GREEN = \"#10B981\"\n",
    "EDGE = \"black\"\n",
    "\n",
    "SHORT_LABELS = {\n",
    "    \"mistralai/devstral-small\": \"DevStral Small\",\n",
    "    \"mistralai/mistral-large-2411\": \"Mistral Large\",\n",
    "    \"google/gemma-3-4b-it\": \"Gemma 3 (4B)\",\n",
    "    \"deepseek/deepseek-chat-v3-0324\": \"DeepSeek v3\",\n",
    "    \"moonshotai/kimi-k2\": \"Kimi K2\",\n",
    "    \"qwen/qwen3-235b-a22b-2507\": \"Qwen3 A22B\",\n",
    "    \"meta-llama/llama-4-maverick\": \"Llama 4 Mav.\",\n",
    "    \"mistral-nemo:12b\": \"Mistral Nemo\",\n",
    "    \"mistral-small:22b\": \"Mistral Small (22B)\",\n",
    "    \"mistral-small:24b\": \"Mistral Small (24B)\",\n",
    "    \"gemma3:12b\": \"Gemma 3 (12B)\",\n",
    "    \"gemma3:27b\": \"Gemma 3 (27B)\",\n",
    "    \"gemma2:27b\": \"Gemma 2 (27B)\",\n",
    "    \"qwen3:32b\": \"Qwen3 (32B)\",\n",
    "    \"qwen3:30b\": \"Qwen3 (30B)\",\n",
    "    \"qwen2.5:72b\": \"Qwen 2.5 (72B)\",\n",
    "    \"qwen2.5:32b\": \"Qwen 2.5 (32B)\",\n",
    "    \"llava:34b\": \"LLaVA\",\n",
    "    \"phi4:14b\": \"Phi-4\",\n",
    "    \"llama3:70b\": \"Llama 3\",\n",
    "    \"google/gemma-3-12b-it\": \"Gemma 3 (12B)\",\n",
    "    \"cohere/command-a\": \"Command A\",\n",
    "    \"mistralai/codestral-2508\": \"Codestral\",\n",
    "    \"gpt-4.1\": \"GPT-4.1\",\n",
    "    \"gpt-4.1-mini\": \"GPT-4.1 mini\",\n",
    "    \"gpt-4.1-nano\": \"GPT-4.1 nano\",\n",
    "}\n",
    "\n",
    "FRONTIER_NAMES = {\"gpt-4.1\": \"GPT-4.1\"}\n",
    "\n",
    "# Split frontier vs plotted models\n",
    "frontier_df = plot_df[plot_df[\"model\"].isin(FRONTIER_NAMES)].copy()\n",
    "scatter_df = plot_df[~plot_df[\"model\"].isin(FRONTIER_NAMES)].copy()\n",
    "scatter_df = scatter_df.dropna(subset=[\"params_billion\"])\n",
    "\n",
    "if scatter_df.empty and frontier_df.empty:\n",
    "    print(\"No results available to plot yet.\")\n",
    "else:\n",
    "    rng = np.random.default_rng(123)\n",
    "    scatter_df[\"params_jit\"] = scatter_df[\"params_billion\"] + rng.uniform(-JITTER_X, JITTER_X, size=len(scatter_df))\n",
    "    scatter_df[\"accuracy_jit\"] = scatter_df[\"accuracy\"] + rng.uniform(-JITTER_Y, JITTER_Y, size=len(scatter_df))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(13, 7), dpi=300, constrained_layout=True)\n",
    "\n",
    "    sc_points = None\n",
    "    if not scatter_df.empty:\n",
    "        sc_points = ax.scatter(\n",
    "            scatter_df[\"params_jit\"],\n",
    "            scatter_df[\"accuracy_jit\"],\n",
    "            s=40,\n",
    "            zorder=2,\n",
    "            facecolor=BLUE,\n",
    "            edgecolor=EDGE,\n",
    "            linewidth=0.6,\n",
    "            alpha=0.95,\n",
    "            label=\"Models\",\n",
    "        )\n",
    "\n",
    "    texts = []\n",
    "    for i, r in scatter_df.reset_index(drop=True).iterrows():\n",
    "        label = SHORT_LABELS.get(str(r[\"model\"]), str(r[\"model\"]))\n",
    "        dx = INIT_DX\n",
    "        dy = (1 if (i % 2 == 0) else -1) * (INIT_DY * (1.0 + 0.3 * rng.random()))\n",
    "        ann = ax.annotate(\n",
    "            label,\n",
    "            xy=(r[\"params_jit\"], r[\"accuracy_jit\"]),\n",
    "            xytext=(r[\"params_jit\"] + dx, r[\"accuracy_jit\"] + dy),\n",
    "            textcoords=\"data\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontsize=7,\n",
    "            color=\"black\",\n",
    "            arrowprops=ARROW,\n",
    "        )\n",
    "        texts.append(ann)\n",
    "\n",
    "    ax.set_xlabel(\"Parameters (billions)\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    ax.set_title(\n",
    "        \"Comparing screened models â€” Accuracy vs Parameter Count\",\n",
    "        loc=\"left\",\n",
    "    )\n",
    "\n",
    "    x_min = max(0.0, float(scatter_df[\"params_jit\"].min()) - X_MIN_PAD) if not scatter_df.empty else 0.0\n",
    "    x_max = float(scatter_df[\"params_jit\"].max()) + X_PAD_RIGHT if not scatter_df.empty else 1.0\n",
    "    y_min = max(0.0, float(scatter_df[\"accuracy_jit\"].min()) - 0.03) if not scatter_df.empty else 0.0\n",
    "    y_max = 1.0\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    if not frontier_df.empty:\n",
    "        for _, r in frontier_df.iterrows():\n",
    "            ax.axhline(r[\"accuracy\"], color=\"grey\", lw=0.6, ls=(0, (6, 6)), zorder=1, alpha=0.9)\n",
    "            ax.text(\n",
    "                x_max - 0.5,\n",
    "                r[\"accuracy\"] + 0.002,\n",
    "                f\"{FRONTIER_NAMES[r['model']]} (Acc {r['accuracy']*100:.1f}%)\",\n",
    "                ha=\"right\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=7.5,\n",
    "                color=\"grey\",\n",
    "            )\n",
    "\n",
    "    add_objs = [h for h in [sc_points] if h is not None]\n",
    "    adjust_text(\n",
    "        texts,\n",
    "        ax=ax,\n",
    "        add_objects=add_objs,\n",
    "        expand_text=EXP_TEXT,\n",
    "        expand_points=EXP_POINTS,\n",
    "        force_text=FORCE_TEXT,\n",
    "        force_points=FORCE_POINTS,\n",
    "        only_move={\"points\": \"y\", \"text\": \"xy\"},\n",
    "        autoalign=True,\n",
    "        precision=0.01,\n",
    "        lim=N_STEPS,\n",
    "    )\n",
    "\n",
    "    if add_objs:\n",
    "        ax.legend(loc=\"lower right\", frameon=True, fontsize=8, title=\"Models\")\n",
    "\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(1.0, decimals=1))\n",
    "\n",
    "    fig.text(\n",
    "        0.01,\n",
    "        0.01,\n",
    "        \"Note: Dashed lines denote the frontier baseline (parameters undisclosed).\",\n",
    "        fontsize=8,\n",
    "        color=\"dimgray\",\n",
    "    )\n",
    "\n",
    "    fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5bd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to remove results cache\n",
    "# \n",
    "# ! rm model_comparison.csv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
