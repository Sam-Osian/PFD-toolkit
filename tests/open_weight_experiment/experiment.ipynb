{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc171e8",
   "metadata": {},
   "source": [
    "# Open-weight model experiment\n",
    "\n",
    "Run the open-weight model comparison on the consensus dataset without relying on local data files. The notebook pulls the data directly from GitHub, loads any available API keys from env files next to the notebook, and saves results locally to a CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5123e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "! pip install -q pfd_toolkit ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60204560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f1d69c34f8446aaf1d1ef237e5e8f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Password(description='API key:', placeholder='Paste your key here'), Button(description='Set ke…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get API key\n",
    "\n",
    "import os\n",
    "from ipywidgets import Password, Button, VBox, HTML\n",
    "\n",
    "api_key_input = Password(\n",
    "    description=\"API key:\",\n",
    "    placeholder=\"Paste your key here\"\n",
    ")\n",
    "status = HTML(\"\")\n",
    "button = Button(description=\"Set key\")\n",
    "\n",
    "def set_api_key(_):\n",
    "    if api_key_input.value:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = api_key_input.value\n",
    "        # clear the visible value\n",
    "        api_key_input.value = \"\"\n",
    "        status.value = \"<b>API key stored in this session.</b>\"\n",
    "    else:\n",
    "        status.value = \"<b>No key entered.</b>\"\n",
    "\n",
    "button.on_click(set_api_key)\n",
    "\n",
    "ui = VBox([api_key_input, button, status])\n",
    "ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b5a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b096b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "from urllib.parse import quote\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "from pfd_toolkit import LLM, Screener\n",
    "from pfd_toolkit.config import GeneralConfig\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "def ensure_secret(env_name: str) -> str:\n",
    "    if env_name not in os.environ or not os.environ[env_name]:\n",
    "        os.environ[env_name] = getpass(f\"Enter {env_name}: \")\n",
    "    return os.environ[env_name]\n",
    "\n",
    "OPENAI_API_KEY = api_key\n",
    "\n",
    "GITHUB_BASE = \"https://raw.githubusercontent.com/Sam-Osian/PFD-toolkit/open-model-exp\"\n",
    "DATA_URL = f\"{GITHUB_BASE}/ons_replication/{quote('PFD Toolkit--Consensus Comparison.xlsx')}\"\n",
    "SHEET_NAME = \"Consensus annotations\"\n",
    "RESULTS_PATH = NOTEBOOK_DIR / \"model_comparison.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "721e1fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to /tmp/pfd_toolkit_consensus.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-8fe4b5f3-b36c-4b73-808c-dddfa892dc89\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>investigation</th>\n",
       "      <th>circumstances</th>\n",
       "      <th>concerns</th>\n",
       "      <th>consensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-0452</td>\n",
       "      <td>On 12 August 2022 I commenced an investigation...</td>\n",
       "      <td>Madeleine Savory was 15 years old when they di...</td>\n",
       "      <td>The availability, nationally, of Tier 4 beds i...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-0445</td>\n",
       "      <td>On 15 May 2023, one of my assistant coroners, ...</td>\n",
       "      <td>Igor hanged himself in his room at the Depaul ...</td>\n",
       "      <td>On 28 March 2023, Igor refused to get out of t...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-0313</td>\n",
       "      <td>On 3 August 2022 I commenced an investigation ...</td>\n",
       "      <td>Allison Aules was referred to the mental healt...</td>\n",
       "      <td>The Inquest identified multiple failings in th...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-0283</td>\n",
       "      <td>On 12 June 2019 I commenced an investigation i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A detailed review of the evidence in this case...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-0146</td>\n",
       "      <td>On 31 August 2022 I opened an investigation to...</td>\n",
       "      <td>On 27 August 2022 Callum Wong was found having...</td>\n",
       "      <td>1. Consideration for exceptions to patient con...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fe4b5f3-b36c-4b73-808c-dddfa892dc89')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8fe4b5f3-b36c-4b73-808c-dddfa892dc89 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8fe4b5f3-b36c-4b73-808c-dddfa892dc89');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          id                                      investigation  \\\n",
       "0  2023-0452  On 12 August 2022 I commenced an investigation...   \n",
       "1  2023-0445  On 15 May 2023, one of my assistant coroners, ...   \n",
       "2  2023-0313  On 3 August 2022 I commenced an investigation ...   \n",
       "3  2023-0283  On 12 June 2019 I commenced an investigation i...   \n",
       "4  2023-0146  On 31 August 2022 I opened an investigation to...   \n",
       "\n",
       "                                       circumstances  \\\n",
       "0  Madeleine Savory was 15 years old when they di...   \n",
       "1  Igor hanged himself in his room at the Depaul ...   \n",
       "2  Allison Aules was referred to the mental healt...   \n",
       "3                                                NaN   \n",
       "4  On 27 August 2022 Callum Wong was found having...   \n",
       "\n",
       "                                            concerns  consensus  \n",
       "0  The availability, nationally, of Tier 4 beds i...       True  \n",
       "1  On 28 March 2023, Igor refused to get out of t...       True  \n",
       "2  The Inquest identified multiple failings in th...       True  \n",
       "3  A detailed review of the evidence in this case...       True  \n",
       "4  1. Consideration for exceptions to patient con...       True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(tempfile.gettempdir()) / \"pfd_toolkit_consensus.xlsx\"\n",
    "response = requests.get(DATA_URL, timeout=60)\n",
    "response.raise_for_status()\n",
    "data_path.write_bytes(response.content)\n",
    "\n",
    "print(f\"Dataset downloaded to {data_path}\")\n",
    "\n",
    "df = pd.read_excel(data_path, sheet_name=SHEET_NAME)\n",
    "renamed = df.rename(\n",
    "    columns={\n",
    "        \"Ref\": GeneralConfig.COL_ID,\n",
    "        \"Investigation section\": GeneralConfig.COL_INVESTIGATION,\n",
    "        \"Circumstances of death section\": GeneralConfig.COL_CIRCUMSTANCES,\n",
    "        \"Matters of concern section\": GeneralConfig.COL_CONCERNS,\n",
    "        \"Post-consensus verdict: Is this a child suicide case? (Yes or No)\": \"consensus\",\n",
    "    }\n",
    ")\n",
    "\n",
    "reports = renamed[\n",
    "    [\n",
    "        GeneralConfig.COL_ID,\n",
    "        GeneralConfig.COL_INVESTIGATION,\n",
    "        GeneralConfig.COL_CIRCUMSTANCES,\n",
    "        GeneralConfig.COL_CONCERNS,\n",
    "        \"consensus\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "reports[\"consensus\"] = reports[\"consensus\"].astype(str).str.strip().str.lower() == \"yes\"\n",
    "\n",
    "reports.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02a774f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SPECS = [\n",
    "    # OpenAI API models\n",
    "    {\"name\": \"gpt-4.1\", \"temperature\": 0},\n",
    "    {\"name\": \"gpt-4.1-mini\", \"temperature\": 0},\n",
    "    {\"name\": \"gpt-4.1-nano\", \"temperature\": 0},\n",
    "\n",
    "    # Ollama-hosted models\n",
    "    {\n",
    "        \"name\": \"mistral-nemo:12b\",\n",
    "        \"temperature\": 0,\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": \"ollama\",\n",
    "        \"timeout\": 10**9,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mistral-small:22b\",\n",
    "        \"temperature\": 0,\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": \"ollama\",\n",
    "        \"timeout\": 10**9,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mistral-small:24b\",\n",
    "        \"temperature\": 0,\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": \"ollama\",\n",
    "        \"timeout\": 10**9,\n",
    "    },\n",
    "]\n",
    "\n",
    "user_query = \"\"\"\n",
    "Identify cases where the deceased was aged 18 or younger *clearly at the time of death* **and**\n",
    "the death was due to suicide. If suicide is not explicitly stated, you can use a strict balance of\n",
    "probabilities threshold to determine it as such.\n",
    "\n",
    "Age may not be explicitly stated, but could be implied through references such as\n",
    "recent use of child or adolescent services (e.g. CAMHS), attending school years\n",
    "(e.g. \\\"Year 10\\\"), or similar contextual indicators of being under 18 (again, under a\n",
    "strict balance of probabilities threshold).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795d5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: gpt-4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending requests to the LLM: 100%|██████████| 146/146 [00:12<00:00, 11.39it/s]\n",
      "/tmp/ipython-input-3293819096.py:56: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: gpt-4.1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending requests to the LLM: 100%|██████████| 146/146 [00:09<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: gpt-4.1-nano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending requests to the LLM: 100%|██████████| 146/146 [00:09<00:00, 15.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: mistral-nemo:12b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sending requests to the LLM:   0%|          | 0/146 [00:00<?, ?it/s]INFO:backoff:Backing off _parse_with_backoff(...) for 0.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 2.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 4.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 7.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 7.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 7.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 15.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 23.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 45.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 9.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 23.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 14.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 29.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 34.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 21.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 29.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 22.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 11.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 4.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 9.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 8.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 8.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.3s (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 7 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 10 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 8 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 8 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 8 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 8 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 9 tries (openai.APIConnectionError: Connection error.)\n",
      "ERROR:backoff:Giving up _parse_with_backoff(...) after 8 tries (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.6s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 2.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 0.8s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 4.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 6.9s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 2.5s (openai.APIConnectionError: Connection error.)\n",
      "Sending requests to the LLM:   0%|          | 0/146 [01:15<?, ?it/s]\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 9.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 14.3s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 11.1s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 15.7s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 9.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 16.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 27.5s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 1.4s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 3.2s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 9.8s (openai.APIConnectionError: Connection error.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pfd_toolkit/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, images_list, response_format, max_workers, tqdm_extra_kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m             )\n\u001b[0;32m--> 353\u001b[0;31m             for fut in tqdm(\n\u001b[0m\u001b[1;32m    354\u001b[0m                 \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3293819096.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     classified = screener.screen_reports(\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0msearch_query\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mfilter_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pfd_toolkit/screener.py\u001b[0m in \u001b[0;36mscreen_reports\u001b[0;34m(self, reports, search_query, user_query, filter_df, result_col_name, produce_spans, drop_spans)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mresponse_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_topic_model_with_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduce_spans\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mTopicMatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         llm_results = self.llm.generate(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mprompts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompts_for_screening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pfd_toolkit/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, prompts, images_list, response_format, max_workers, tqdm_extra_kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meffective_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mfutures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_worker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mbar_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:backoff:Backing off _parse_with_backoff(...) for 7.0s (openai.APIConnectionError: Connection error.)\n",
      "INFO:backoff:Backing off _parse_with_backoff(...) for 5.6s (openai.APIConnectionError: Connection error.)\n"
     ]
    }
   ],
   "source": [
    "results_columns = [\"model\", \"accuracy\", \"sensitivity\", \"specificity\"]\n",
    "if RESULTS_PATH.exists():\n",
    "    results_df = pd.read_csv(RESULTS_PATH)\n",
    "else:\n",
    "    results_df = pd.DataFrame(columns=results_columns)\n",
    "\n",
    "completed_models = set(results_df[\"model\"].astype(str))\n",
    "\n",
    "for spec in MODEL_SPECS:\n",
    "    if spec[\"name\"] in completed_models:\n",
    "        print(f\"Skipping {spec['name']} (already evaluated)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Testing model: {spec['name']}\")\n",
    "\n",
    "    llm_kwargs = {\n",
    "        \"api_key\": spec.get(\"api_key\", os.getenv(\"OPENAI_API_KEY\")),\n",
    "        \"max_workers\": 8,\n",
    "        \"model\": spec[\"name\"],\n",
    "        \"seed\": 12345,\n",
    "        \"timeout\": spec.get(\"timeout\", 20),\n",
    "        \"temperature\": 1 if spec[\"name\"].startswith(\"gpt-5\") else spec[\"temperature\"],\n",
    "    }\n",
    "\n",
    "    if \"base_url\" in spec:\n",
    "        llm_kwargs[\"base_url\"] = spec[\"base_url\"]\n",
    "\n",
    "    llm_client = LLM(**llm_kwargs)\n",
    "    screener = Screener(\n",
    "        llm=llm_client,\n",
    "        reports=reports,\n",
    "        include_investigation=True,\n",
    "        include_circumstances=True,\n",
    "        include_concerns=True,\n",
    "    )\n",
    "\n",
    "    classified = screener.screen_reports(\n",
    "        search_query=user_query,\n",
    "        filter_df=False,\n",
    "        result_col_name=\"model_pred\",\n",
    "    )\n",
    "\n",
    "    pred = classified[\"model_pred\"].astype(bool)\n",
    "    truth = classified[\"consensus\"].astype(bool)\n",
    "\n",
    "    tp = (pred & truth).sum()\n",
    "    tn = ((~pred) & (~truth)).sum()\n",
    "    fp = (pred & ~truth).sum()\n",
    "    fn = ((~pred) & truth).sum()\n",
    "\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy = (tp + tn) / total if total else float(\"nan\")\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) else float(\"nan\")\n",
    "    specificity = tn / (tn + fp) if (tn + fp) else float(\"nan\")\n",
    "\n",
    "    results_df = pd.concat(\n",
    "        [\n",
    "            results_df,\n",
    "            pd.DataFrame(\n",
    "                [\n",
    "                    {\n",
    "                        \"model\": spec[\"name\"],\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"sensitivity\": sensitivity,\n",
    "                        \"specificity\": specificity,\n",
    "                    }\n",
    "                ]\n",
    "            ),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    results_df.to_csv(RESULTS_PATH, index=False)\n",
    "    completed_models.add(spec[\"name\"])\n",
    "\n",
    "results_df\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
