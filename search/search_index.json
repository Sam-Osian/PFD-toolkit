{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#background","title":"Background","text":"<p>PFD Toolkit is an open-source Python package created to transform how researchers, policymakers, and analysts access and analyse Prevention of Future Death (PFD) reports from coroners in England and Wales.</p>"},{"location":"#the-problem","title":"The problem","text":"<p>PFD reports have long served as urgent public warnings \u2014 issued when coroners identified risks that could, if ignored, lead to further deaths. Yet despite being freely available, these reports are chronically underused. This was for one simple reason: PFD reports are a pain to analyse. Common issues included:</p> <ul> <li> <p>No straightforward way to download report content in bulk</p> </li> <li> <p>Wildly inconsistent formats, making traditional web scraping unreliable</p> </li> <li> <p>No reliable way of automatically screening/filtering reports based on a custom query</p> </li> <li> <p>No system for surfacing recurring themes, or extracting other key pieces of information</p> </li> <li> <p>Widespread miscategorisation of reports, creating research limitations</p> </li> </ul> <p>As a result, valuable insights often ended up buried beneath months or even years of manual admin. Researchers were forced to sift through thousands of reports one by one, wrestle with patchy metadata, and code themes by hand. </p>"},{"location":"#our-solution","title":"Our solution","text":"<p>PFD Toolkit acts as a one-stop-shop for extracting, screening and analysing PFD report data.</p> <p>The package brings together every PFD report (around 6000) and makes them available in a single, downloadable dataset, ready for instant analysis. </p> <p>Here is a sample of the PFD dataset:</p> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m... <p>PFD Toolkit was built to break down every major barrier to PFD report analysis. Out of the box, you can:</p> <ol> <li> <p>Load live PFD data in seconds</p> </li> <li> <p>Query and filter reports with natural language</p> </li> <li> <p>Summarise reports to highlight key messages</p> </li> <li> <p>Automatically discover recurring themes</p> </li> <li> <p>Tag and organise reports based on these themes (or provide your own themes!)</p> </li> </ol> <p>Note</p> <p>The dataset is refreshed each week, with newly published reports being added to the dataset.  You don't have to update PFD Toolkit to access new reports.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install PFD Toolkit using pip:</p> <pre><code>pip install pfd_toolkit\n</code></pre>"},{"location":"#contribute","title":"Contribute","text":"<p>PFD Toolkit is designed as a research-enabling tool, and we\u2019re keen to work with the community to make sure it genuinely meets your needs. If you have feedback, ideas, or want to get involved, head to our Feedback &amp; contributions page.</p>"},{"location":"changelog/","title":"\ud83d\udcc6 Changelog","text":"<p>Welcome to the project changelog. All notable changes to this project will be documented below.</p>"},{"location":"changelog/#030-2025-07-01","title":"0.3.0 - 2025-07-01","text":"<p>First public release! \u2728</p>"},{"location":"contact/","title":"How to get in touch","text":"<p>Reach out to the lead developer Sam via email or LinkedIn.</p>"},{"location":"contribute/","title":"Feedback &amp; contributions","text":"<p>Thank you for your interest in contributing to PFD Toolkit! We welcome input from researchers, data scientists, developers, and anyone passionate about improving access to coroners\u2019 Prevention of Future Death (PFD) reports.</p>"},{"location":"contribute/#how-you-can-get-involved","title":"How you can get involved","text":""},{"location":"contribute/#report-issues-bugs","title":"Report issues &amp; bugs \ud83d\udc1b","text":"<p>If you encounter a bug, data problem, or unexpected behaviour, please open an issue on Github.</p> <p>Include a clear description, steps to reproduce (if possible), and your Python version or environment details.</p>"},{"location":"contribute/#suggest-features-or-improvements","title":"Suggest features or improvements \ud83d\udca1","text":"<p>Have an idea for a new feature, a better workflow, or an additional data cleaning/categorisation option?</p> <p>Submit a feature request as a GitHub issue with as much detail as possible.</p>"},{"location":"contribute/#code-contributions","title":"Code contributions \ud83e\uddd1\u200d\ud83d\udcbb","text":"<p>Contributions are welcome - whether fixing bugs, adding new features, improving documentation, or expanding tests.</p> <p>Please fork the repository, create a new branch for your work, and submit a pull request with a clear description of your changes.</p> <p>If your change is significant, consider opening an issue first to discuss it.</p>"},{"location":"contribute/#local-setup","title":"Local setup","text":"<p>PFD Toolkit uses <code>uv</code> as its dependency management tool. Once <code>uv</code> is running on your system, install dependencies with:</p> <pre><code>uv sync\n</code></pre> <p>Run the test suite with:</p> <pre><code>uv run pytest\n</code></pre>"},{"location":"contribute/#anything-else-get-in-touch","title":"Anything else, get in touch \ud83d\udcac","text":"<p>If you have any questions, feedback or would like to contribute in a way not outlined above, please contact Sam on samoand@liverpool.ac.uk.</p>"},{"location":"llm_setup/","title":"Creating an LLM Client","text":"<p>PFD Toolkit uses a Large Language Model (LLM) client for advanced features. This page explains how to set up your LLM, what an API key is, and why you might need these features.</p>"},{"location":"llm_setup/#setting-up-your-llm-client","title":"Setting up your LLM client","text":"<p>To use AI-powered features, you need to create an LLM client and supply your OpenAI API key (how to get one below). You do not need an LLM client to simply load report data (using <code>load_reports</code>).</p> <p>Basic setup:</p> <pre><code>from pfd_toolkit import LLM\n\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace YOUR-API-KEY with your real API key\n</code></pre> <p>You can now use LLM-powered features! For example, to screen for reports about medication purchased online:</p> <pre><code>from pfd_toolkit import Screener\n\nquery = \"Deaths that followed ordering medication(s) online.\"\n\nscreener = Screener(llm=llm_client, reports=reports)\nonline_med_reports = screener.screen_reports(user_query=query)\n</code></pre>"},{"location":"llm_setup/#how-do-i-get-an-openai-api-key","title":"How do I get an OpenAI API key?","text":"<ol> <li>Sign up or log in at OpenAI Platform.</li> <li>Go to API Keys.</li> <li>Click \u201cCreate new secret key\u201d and copy the string.</li> <li>Store your key somewhere safe. Never share or publish it.</li> <li>Add credit to your account (just $5 is enough for most research uses).</li> </ol> <p>For more information about usage costs, see OpenAI pricing.</p>"},{"location":"llm_setup/#why-do-i-need-an-llm-client-anyway","title":"Why do I need an LLM client anyway?","text":"<p>Many toolkit features - like advanced cleaning, screening, and assigning themes - rely on AI, which goes far beyond what\u2019s possible with rule-based scripts. If you want to use these features, you\u2019ll need to set up an LLM client as described above.</p> <p>We\u2019ve made the setup as simple as possible, especially if you\u2019re new to APIs. If you get stuck, please reach out: we\u2019re happy to help.</p>"},{"location":"non_coders/","title":"For Non-Coders","text":"<p>We appreciate that not all researchers are Python coders, and we also believe that this shouldn't stop you from unlocking the insights contained within PFD reports.</p> <p>If you have a research question, policy project, or just curiosity about PFD reports, please get in touch with me (Sam).</p>"},{"location":"non_coders/#how-i-can-help","title":"How I can help","text":"<p>If you want to use the PFD Toolkit but are not sure where to start, or you find the technical side a bit daunting, I am genuinely very happy to help. Here is what I can offer:</p> <ul> <li>Explain PFD reports. I can tell you more about what kind of information is contained within PFD reports and whether they could support your research or policy goals.</li> <li>Custom Scripts. Tell me what you are trying to do, for example, \"I want all reports related to mental health in the North West from 2022,\" or \"Can I get a table of reports mentioning medication errors?\" I can then write the script for you.</li> <li>Ready-to-Use Results. I will run the toolkit for you and provide you with the outputs you need, whether it is a spreadsheet, a summary, or visualisations.</li> <li>Ongoing Support. If you want to learn how to do it yourself next time, I can guide you at your pace.</li> </ul>"},{"location":"non_coders/#how-to-get-in-touch","title":"How to get in touch","text":"<p>Just reach out via email or LinkedIn.</p> <p>I want the toolkit to be useful to as many people as possible, not just programmers. If you are a policy worker, academic, journalist, campaigner, or someone affected by PFD reports, please reach out. I would be delighted to help you get the most out of this resource.</p>"},{"location":"pfd_reports/","title":"More about PFD reports","text":"<p>Prevention of Future Death (PFD) reports are a unique, under-recognised mechanism within the English and Welsh legal system for flagging hazards that threaten lives. These documents, written by coroners at the close of certain inquests, have the potential to drive real change \u2014 but only if their warnings are heard and acted upon.</p>"},{"location":"pfd_reports/#what-is-a-pfd-report","title":"What is a PFD report?","text":"<p>When a coroner concludes an inquest and believes that action should be taken to prevent future deaths, they are legally obliged (under Regulation 28 of the Coroners (Investigations) Regulations 2013) to issue a PFD report. The report is sent to any person or organisation the coroner thinks could take action. This could be an NHS trust, a regulator, a private company, a local council, or even the government.</p> <p>The aim is simple: to prevent further deaths by highlighting risks, missed opportunities, or avoidable harm that have already claimed a life.</p> <p>Other than through this toolkit, reports are publicly available here.</p>"},{"location":"pfd_reports/#what-do-pfd-reports-look-like","title":"What do PFD reports look like?","text":"<p>PFD reports tend to be short, factual documents. PFD Toolkit collects the following sections from each report:</p> PFD report section What it contains Recipient(s) The addressee list \u2013 every person, body or department the coroner believes has the power to act on the concerns. Coroner name Identifies the coroner by name. Coroner area States the area of the coroner. Each area typically covers one or more local authorities. Investigation and inquest Provides the inquest conclusion (medical cause and verdict). Circumstances of the death A concise, factual summary of how the death occurred, setting the scene for the concerns that follow. Coroner's concerns Lists specific matters revealed by the evidence that give rise to a risk of future deaths. Date and signature The coroner dates and signs the report. <p>Note</p> <p>The above table is a rough guide. In practice, each coroner may approach the writing of PFD reports slightly differently. For example, there is occasional overlap between the \"Investigation and inquest\" and \"Circumstances of the death\" sections.</p> <p>Although each section of the report has value, when we analyse PFD reports for recurring themes, we are typically most interested in the Concerns section. However, information contained within other sections can help to contextualise these concerns.</p>"},{"location":"pfd_reports/#why-do-pfd-reports-matter","title":"Why do PFD reports matter?","text":"<p>PFD reports offer us a rare window into risks and failures that may not appear in routine data. They can expose themes including, but not limited to:</p> <ul> <li> <p>Missed diagnoses and medical errors</p> </li> <li> <p>Gaps in mental health or social care provision</p> </li> <li> <p>Unsafe systems or environments (e.g. railway safety, housing, road design)</p> </li> <li> <p>Inadequate policies or regulatory oversight</p> </li> </ul> <p>Because coroners have a statutory duty to write them, PFD reports sometimes identify entirely new risks \u2014 before they turn into trends. For researchers and policymakers, they are a critical early-warning system.</p>"},{"location":"pfd_reports/#how-does-pfd-toolkit-help","title":"How does PFD Toolkit help?","text":"<p>Before PFD Toolkit, there was no automated way of screening, discovering themes, or extracting information from these reports. Researchers would have to manually screen them report-by-report, demanding months or even years of researcher time.</p> <p>Through this, we are hoping to lower the barrier to research for those interested in using PFD reports.</p>"},{"location":"extractor/","title":"Analysing PFD reports","text":"<p>PFD Toolkit ships with an <code>Extractor</code> class to pull \"features\" (i.e. key pieces of information) from Prevention of Future Death (PFD) reports. </p> <p>These features could be recurring themes, or more specific bits of information (e.g. age, sex, cause of death, etc.).</p> <p>The guides below walk through the main features:</p> <ul> <li>Basic usage \u2013 create a basic feature model to identify features from report data.</li> <li>Summaries &amp; token counts \u2013 generate short summaries and estimate the token cost of your data.</li> <li>Tagging reports with themes \u2013 automatically discover recurring themes or label reports with your own taxonomy.</li> <li>Capturing text spans \u2013 keep short excerpts (\"spans\") showing where each feature came from.</li> <li>Caching and exporting results \u2013 reuse completions to save time and API costs.</li> </ul>"},{"location":"extractor/basics/","title":"Basic usage","text":"<p>Start by defining a feature model with <code>pydantic</code>. Each attribute represents a piece of information you want to pull out of the report. <code>Extractor</code> accepts any valid <code>BaseModel</code>, so feel free to mix strings, numbers or more complex types:</p> <pre><code>from pydantic import BaseModel, Field\nfrom pfd_toolkit import load_reports, LLM, Extractor\n\n# Define feature model with pydantic\nclass MyFeatures(BaseModel):\n    age: int\n    cause_of_death: str\n</code></pre> <p>Next, load some report data and set up your LLM client. You then pass the feature model, the reports and the LLM client to an <code>Extractor</code> instance and call <code>.extract_features()</code>:</p> <pre><code>reports = load_reports(category=\"all\", start_date=\"2024-01-01\", end_date=\"2024-12-31\")\nllm_client = LLM(api_key=\"YOUR-API-KEY\")\n\nextractor = Extractor(\n    feature_model=MyFeatures,\n    reports=reports,\n    llm=llm_client\n)\n\nresult_df = extractor.extract_features()\n</code></pre> <p><code>result_df</code> now contains the new <code>age</code> and <code>cause_of_death</code> columns. You can repeat the call with a different feature model to extract further information \u2013 the cached results mean previously processed rows will not be re-sent to the LLM unless you clear the cache with <code>.reset()</code>.</p>"},{"location":"extractor/basics/#choosing-which-sections-the-llm-reads","title":"Choosing which sections the LLM reads","text":"<p><code>Extractor</code> lets you decide exactly which parts of the report are presented to the model. Each <code>include_*</code> flag mirrors one of the columns loaded by <code>load_reports</code>. Turning fields off reduces the amount of text sent to the LLM which often speeds up requests and lowers token usage.</p> <pre><code>extractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n    include_investigation=True,\n    include_circumstances=True,\n    include_concerns=False  # Skip coroner's concerns if not relevant\n)\n</code></pre> <p>In this example only the investigation and circumstances sections are provided to the LLM. The coroner's concerns are omitted entirely. Limiting the excerpt like this often improves accuracy and drastically reduces token costs. However, be careful you're not turning 'off' a report section which is genuinely useful for your query.</p>"},{"location":"extractor/caching/","title":"Caching and exporting results","text":"<p><code>Extractor</code> caches every LLM response so repeated calls with the same prompt reuse previous results. Export the cache before you shut down and import it in a future session to avoid paying for the same completions twice.</p> <pre><code>extractor.export_cache(\"my_cache.pkl\")\n...\nextractor.import_cache(\"my_cache.pkl\")\n</code></pre> <p>If you want to start fresh, call <code>reset()</code> to clear cached feature values and token estimates. This is useful when you wish to re-run <code>extract_features</code> on the same DataFrame with a different feature model. <code>reset</code> returns the instance so you can immediately chain another call:</p> <pre><code>clean_df = extractor.reset().extract_features(feature_model=NewModel)\n</code></pre> <p>The returned DataFrame contains your newly extracted features and an empty cache ready for further runs.</p>"},{"location":"extractor/spans/","title":"Capturing text spans","text":"<p>Sometimes you want to know exactly which lines from the report led the model to a assign a particular value to a given report's field. </p> <p>For example, say we asked the model to identify whether the deceased is a child and the model outputs <code>True</code> for a particular report, we might want to know whether this was because age is explicitly recorded (e.g. \"The deceased was aged 16\") or implied based on context (e.g. \"The deceased was being seen by CAMHS prior to their death\").</p> <p><code>Extractor</code> can add these quotations (or 'spans') automatically. This is...</p> <ul> <li>Great for performance, because we're instructing the model to identify evidence for a feature value before it is assigned, reducing the risk of false positives.</li> <li>Great for human verification, because we can easily verify whether the model's evidence matches its assignment for each report.</li> </ul>"},{"location":"extractor/spans/#include-spans","title":"Include spans","text":"<p><code>.extract_features()</code> accepts a <code>produce_spans</code> flag. When enabled, a new column starting with <code>spans_</code> is created for every feature.</p> <p>For example, in our above example where we extract feature \"child\", a separate column called \"spans_child\" will be created. Each <code>spans_</code> column contains verbatim snippets from the report which justify the extracted value.</p> <pre><code>class ChildID(BaseModel):\n    child: bool = Field(..., description=\"Whether the deceased is a child (under 18)\")\n\nresult = extractor.extract_features(\n    feature_model=ChildID,\n    produce_spans=True,\n)\nresult\n</code></pre> <p>The quotes returned in the spans are kept as short as possible but should always match the original text verbatim. Multiple snippets are separated with semicolons.</p>"},{"location":"extractor/spans/#include-drop-spans","title":"Include &amp; drop spans","text":"<p>If you're not interested in verifying the output, you might want to remove the identified spans from the returned DataFrame after extraction. Set <code>drop_spans=True</code> to remove all <code>spans_</code> columns.</p> <p>As mentioned before, producing but later dropping spans is still likely to improve performance, because you're forcing the model to generate evidence as part of its internal workings out.</p> <pre><code>extractor.extract_features(\n    feature_model=DemoModel,\n    produce_spans=True,\n    drop_spans=True,\n)\n</code></pre>"},{"location":"extractor/summarising/","title":"Summaries &amp; token counts","text":"<p>Use <code>.summarise()</code> to condense each report into a short text snippet. The <code>trim_intensity</code> option controls how terse the summary should be. Calling <code>summarise</code> adds a <code>summary</code> column to your stored reports and keeps a copy on the instance under <code>extractor.summarised_reports</code> for later reuse.</p> <pre><code>summary_df = extractor.summarise(trim_intensity=\"medium\")\nsummary_df[[\"summary\"]].head()\n</code></pre> <p>The resulting DataFrame contains a new column (default name <code>summary</code>). You can specify a different column name via <code>result_col_name</code> if desired.</p>"},{"location":"extractor/summarising/#estimating-token-counts","title":"Estimating token counts","text":"<p>Token usage is important when working with paid APIs. The <code>estimate_tokens()</code> helper provides a quick approximation of how many tokens a text column will consume.</p> <pre><code>total = extractor.estimate_tokens()\nprint(f\"Total tokens in summaries: {total}\")\n</code></pre> <p><code>estimate_tokens</code> defaults to the summary column, but you can pass any text series via <code>col_name</code>. Set <code>return_series=True</code> to get a per-row estimate instead of the total.</p>"},{"location":"extractor/themes/","title":"Tagging reports with themes","text":"<p><code>Extractor</code> can be used to label reports with your own themes. Each field on the feature model represents a potential tag. In the model below the <code>falls_in_custody</code> field indicates whether a death occurred in police custody.</p> <p>Set <code>force_assign=True</code> so the LLM always returns either <code>True</code> or <code>False</code> for each field. <code>allow_multiple=True</code> lets a single report be marked with more than one theme if required.</p> <pre><code># For themes, we recommend always using the `bool` flag\nclass Themes(BaseModel):\n    falls_in_custody: bool = Field(description=\"Death occurred in police custody\")\n    medication_error: bool = Field(description=\"Issues with medication or dosing\")\n\nextractor = Extractor(\n    llm=llm_client,\n    feature_model=Themes,\n    reports=reports,\n    force_assign=True,\n    allow_multiple=True,\n)\n\nlabelled = extractor.extract_features()\n</code></pre> <p>The returned DataFrame includes a boolean column for each theme.</p>"},{"location":"extractor/themes/#discovering-themes-automatically","title":"Discovering themes automatically","text":"<p>Instead of having a prescribed list of themes ahead of time, you may wish to automatically discover themes contained within your selection of reports.</p> <p>Once summaries are available you can call <code>.discover_themes()</code> to let the LLM propose a list of recurring themes. <code>.discover_themes()</code> reads the <code>summary</code> column created by <code>.summarise()</code> (see Summaries &amp; token counts).</p> <p>The function returns a <code>pydantic</code> model describing the discovered themes. You can immediately feed that model back into <code>extract_features</code> to label each report.</p> <pre><code>IdentifiedThemes = extractor.discover_themes()\n\n# Optionally, inspect the newly identified themes:\n# print(IdentifiedThemes)\n\nassigned_reports = extractor.extract_features(\n                              feature_model=IdentifiedThemes,\n                              force_assign=True,\n                              allow_multiple=True)\n</code></pre> <p><code>discover_themes</code> accepts several parameters:</p> <ul> <li><code>warn_exceed</code> and <code>error_exceed</code> \u2013 soft and hard limits for the estimated token count of the combined summaries. Exceeding <code>error_exceed</code> raises an exception.</li> <li><code>max_themes</code> / <code>min_themes</code> \u2013 bound the number of themes the model should return.</li> <li><code>seed_topics</code> \u2013 either a string, list or <code>BaseModel</code> of starter topics. The LLM will incorporate these into the final list.</li> <li><code>extra_instructions</code> \u2013 free\u2011form text appended to the prompt, allowing you to steer the LLM towards particular areas of interest.</li> </ul>"},{"location":"getting_started/discover_themes/","title":"Discover themes in your filtered dataset","text":"<p>With your subset of reports screened for Mental Health Act detention concerns, the next step is to uncover the underlying themes. This lets you see 'at a glance' what issues the coroners keep raising.</p> <p>We'll use the <code>Extractor</code> class to automatically identify themes from the concerns section of each report.</p>"},{"location":"getting_started/discover_themes/#set-up-the-extractor","title":"Set up the Extractor","text":"<p>The Extractor reads the text from the reports you provide. Each <code>include_*</code> flag controls which report columns are sent to the LLM. Here we are only interested in the coroner's concerns, so we turn everything else off:</p> <pre><code>from pfd_toolkit import Extractor\n\nextractor = Extractor(\n    llm=llm_client,             # The same client you created earlier\n    reports=filtered_reports,   # Your screened DataFrame\n    include_date=False,\n    include_coroner=False,\n    include_area=False,\n    include_receiver=False,\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=True       # Only supply the 'concerns' text\n)\n</code></pre> <p>Keeping the prompt focused on the coroner's concerns reduces cost and may result in more cohesive themes.</p>"},{"location":"getting_started/discover_themes/#summarise-then-discover-themes","title":"Summarise then discover themes","text":"<p>Before discovering themes, we need to summarise each report. </p> <p>We do this because the length of PFD report varies from coroner-to-coroner. By summarising the reports, we're centering on the key messages, keeping the prompt short for the LLM.</p> <pre><code># Create short summaries of the concerns\nextractor.summarise(trim_intensity=\"medium\")\n\n# Ask the LLM to propose recurring themes\nIdentifiedThemes = extractor.discover_themes(\n    max_themes=6,  # Limit the list to keep things manageable\n)\n</code></pre> <p>Note</p> <p><code>Extractor</code> will warn you if the word count of your summaries is too high. In these cases, you might want to set your <code>trim_intensity</code> to <code>high</code> or <code>very high</code> (though please note that the more we trim, the more detail we lose).</p> <p><code>IdentifiedThemes</code> is a Pydantic model whose boolean fields represent the themes the LLM found. You can print it to inspect the descriptions of each theme.</p> <p><code>IdentifiedThemes</code> is not printable in itself, but it is replicated as a JSON in <code>self.identified_themes</code> which we can print:</p> <pre><code>print(extractor.identified_themes)\n</code></pre> <p>This gives us a record of each proposed theme with an accompanying description:</p> <pre><code>{\n  \"bed_shortage\": {\n    \"type\": \"bool\",\n    \"description\": \"Shortage of inpatient mental health beds causing prolonged waits, inappropriate placements, and increased risks.\"\n  },\n  \"risk_assessment\": {\n    \"type\": \"bool\",\n    \"description\": \"Failures or inadequacies in assessing, documenting, and managing patient risks including suicide, self-harm, and violence.\"\n  },\n  \"communication_failures\": {\n    \"type\": \"bool\",\n    \"description\": \"Breakdowns in communication and information sharing between healthcare staff, agencies, families, and police.\"\n  },\n  \"staff_training\": {\n    \"type\": \"bool\",\n    \"description\": \"Insufficient or inconsistent training of staff on policies, clinical knowledge, risk management, and emergency procedures.\"\n  },\n  \"policy_implementation\": {\n    \"type\": \"bool\",\n    \"description\": \"Lack of or poor adherence to policies, protocols, and guidance leading to unsafe practices and delays.\"\n  },\n  \"observation_monitoring\": {\n    \"type\": \"bool\",\n    \"description\": \"Failures in patient observation practices, including inadequate monitoring, falsification of records, and unclear procedures.\"\n  }\n}\n</code></pre>"},{"location":"getting_started/discover_themes/#tag-the-reports","title":"Tag the reports","text":"<p>Above, we've only identified the themes: we haven't assigned these themes to the reports.</p> <p>Once you have the theme model, pass it back into the extractor to assign themes to every report in the dataset:</p> <pre><code>labelled_reports = extractor.extract_features(\n    feature_model=IdentifiedThemes,\n    force_assign=True,\n    allow_multiple=True  # A single report might touch on several themes\n)\n</code></pre> <p>The resulting DataFrame now contains a column for each discovered theme, filled with <code>True</code> or <code>False</code> depending on whether that theme was present in the coroner's concerns.</p> <p>Finally, we can count how often a theme appears in our collection of reports:</p> <pre><code>from pfd_toolkit import _tabulate\n\n_tabulate(labelled_reports, columns=[\n    \"bed_shortage\",\n    \"risk_assessment\",\n    \"communication_failures\",\n    \"staff_training\",\n    \"policy_implementation\",\n    \"observation_monitoring\"])\n</code></pre> <pre><code>| Category              | Count | Percentage |\n|-----------------------|-------|------------|\n| risk_assessment       | 69    | 70.41      |\n| information_sharing   | 50    | 51.02      |\n| bed_shortage          | 18    | 18.37      |\n| staff_training        | 49    | 50.00      |\n| policy_compliance     | 58    | 59.18      |\n| environmental_safety  | 17    | 17.35      |\n</code></pre> <p>That's it! You've gone from a mass of PFD reports, to a focused set of cases relating to Mental Health Act detention, to a theme\u2011tagged dataset ready for deeper exploration.</p> <p>From here we can either save our <code>labelled_reports</code> dataset via <code>pandas</code> for qualitative analysis, or we can use even more analytical features of PFD Toolkit.</p> <pre><code>labelled_reports.to_csv()\n</code></pre> <p>Note</p> <p>On our machine, the entire workflow contained within this page and Load &amp; screen reports took just 1 minute and 5 seconds. Adjust the <code>max_workers</code> parameter in the <code>LLM</code> class to control concurrency, but note that higher values could result in rate limit errors.</p>"},{"location":"getting_started/load_and_screen/","title":"Getting started","text":"<p>This page talks you through an example workflow using PFD Toolkit: loading a dataset and screening for relevant cases related to \"detention under the Mental Health Act\". </p> <p>This is just an example. PFD reports contain a breadth of information across a whole range of topics and domains.</p> <p>It doesn't cover everything: for more, we strongly suggest browsing through the pages in the top panel.</p>"},{"location":"getting_started/load_and_screen/#installation","title":"Installation","text":"<p>PFD Toolkit can be installed from pip as <code>pfd_toolkit</code>:</p> <pre><code>pip install pfd_toolkit\n</code></pre> <p>Note</p> <p>PFD Toolkit is not currently available via Anaconda. If you'd like this to change, please make a GitHub Issue. Personally, we love using <code>uv</code> as an alternative to (Ana)conda for dependency management.</p>"},{"location":"getting_started/load_and_screen/#load-your-first-dataset","title":"Load your first dataset","text":"<p>First, you'll need to load a PFD dataset. These datasets are updated weekly, meaning you always have access to the latest reports with minimal setup.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load all PFD reports from January 2024 to May 2025\nreports = load_reports(\n    start_date=\"2024-01-01\",\n    end_date=\"2025-05-01\")\n\nreports.head(n=5)\n</code></pre> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m..."},{"location":"getting_started/load_and_screen/#screen-for-relevant-reports","title":"Screen for relevant reports","text":"<p>You're likely using PFD Toolkit because you want to answer a specific question. For example: \"Do any PFD reports raise concerns related to detention under the Mental Health Act?\"</p> <p>PFD Toolkit lets you query reports in plain English \u2014 no need to know precise keywords or categories. Just describe the cases you care about, and the toolkit will return matching reports.</p>"},{"location":"getting_started/load_and_screen/#set-up-an-llm-client","title":"Set up an LLM client","text":"<p>Screening and other advanced features use AI, and require you to first set up an LLM client. You'll need to head to platform.openai.com and create an API key. Once you've got this, simply feed it to the <code>LLM</code>.</p> <p>For more information, see Setting up an LLM client.</p> <pre><code>from pfd_toolkit import LLM\n\n# Set up LLM client\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace with actual API key\n</code></pre>"},{"location":"getting_started/load_and_screen/#screen-reports-in-plain-english","title":"Screen reports in plain English","text":"<p>Now, all we need to do is specify our <code>user_query</code> (the statement the LLM will use to filter reports), and set up our <code>Screener</code> engine.</p> <pre><code>from pfd_toolkit import Screener\n\n# Create a user query to filter\nuser_query = \"Concerns related to detention under the Mental Health Act **only**\"\n\n# Screen reports\nscreener = Screener(llm = llm_client,\n                        reports = reports) # Reports that you loaded earlier\n\nfiltered_reports = screener.screen_reports(user_query=user_query)\n</code></pre> <p><code>filtered_reports</code> returns a DataFrame, only containing reports that matched your query.</p> <p>For more information on Screening reports, see Screening relevant reports.</p>"},{"location":"loader/","title":"Load PFD reports","text":"<p>PFD Toolkit offers two ways to bring reports into a pandas DataFrame. Most users should call <code>load_reports</code> to download the weekly dataset that ships with the package.  Advanced users can take full control of the scraping pipeline using the <code>Scraper</code> class.</p> <p>The pages below explain each approach in detail:</p> <ul> <li>Loading report data \u2013 get a ready-made DataFrame with a single function call.</li> <li>Scraping module \u2013 build your own scraping workflow for maximum flexibility.</li> </ul>"},{"location":"loader/load_reports/","title":"Loading report data","text":"<p><code>load_reports()</code> is the quickest way to access PFD reports.  It loads a clean CSV that ships with the package and returns a pandas <code>DataFrame</code>. Each row represents a single report with columns mirroring the main sections.</p> <pre><code>from pfd_toolkit import load_reports\n\nreports = load_reports(\n    start_date=\"2025-01-01\",\n    end_date=\"2025-05-01\")\n\nreports.head()\n</code></pre> <p>Pass a <code>start_date</code> and <code>end_date</code> to restrict the date range, and optionally use <code>n_reports</code> to trim the DataFrame to the most recent n entries. Results are always sorted newest first.</p> <p>The dataset is refreshed weekly.  Simply run <code>pip install --upgrade pfd_toolkit</code> whenever a new snapshot is published.</p> <p>Note</p> <p>The dataset loaded when you call <code>load_reports()</code> is cleaned and fully processed. If you wish to load an uncleaned version of the dataset, we suggest running your own scrape via <code>Scraper</code>.</p>"},{"location":"loader/load_reports/#caveats","title":"Caveats","text":"<p>To collect PFD reports, we run a scraping pipeline on the judiciary.uk website every week.  Our scraping methods assume that the host website will not change its basic layout. Should  the host change their website structure, our pipeline may fail to update its catelogue of  reports. The existing catelogue of reports will be unaffected.</p> <p>Should this happen, we will notify users at the top of the Home page and provide updates on when we can remedy the issue.</p>"},{"location":"loader/scraper/","title":"Scraping module","text":"<p><code>Scraper</code> lets you download PFD reports straight from the judiciary website and control each step of the extraction process. For most projects <code>load_reports()</code> is sufficient, but the scraping module gives you full transparency over how reports are gathered and how missing values are filled in. Use it when you need to customise request behaviour, adjust fallback logic or troubleshoot tricky reports.</p>"},{"location":"loader/scraper/#why-run-a-custom-scrape","title":"Why run a custom scrape?","text":"<p>The weekly datasets shipped with the package cover the majority of use cases. However there are two scenarios when direct scraping may be preferable:</p> <ul> <li>Rapid updates \u2013 the PFD Toolkit dataset lags up to a week behind new publications. Running your own scrape means you can see the newest reports immediately.</li> <li>Custom logic \u2013 while the dataset bundled with the package is a product of Vision-LLM scraping, you may also wish to enable HTML and .pdf scraping.</li> </ul>"},{"location":"loader/scraper/#creating-a-scraper","title":"Creating a scraper","text":"<pre><code>from pfd_toolkit import Scraper\n\nscraper = Scraper(\n    category=\"suicide\",           # judiciary.uk slug or \"all\"\n    llm=llm_client,         # assumes you've already set up your LLM client\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\",\n    scraping_strategy=[1, 2, 3],   # html \u2192 pdf \u2192 llm\n    max_workers=10,\n    delay_range=(1, 2),\n)\n</code></pre> <p>Pass in a category slug (or use <code>\"all\"</code>), a date range and any optional settings such as worker count, request delay or timeout. The <code>scraping_strategy</code> list defines which stages run and in what order. Each entry refers to the HTML, PDF and LLM steps respectively \u2013 set an index to <code>-1</code> to skip a step entirely.</p> <p>Note</p> <p>For example, setting <code>scraping_strategy</code> to <code>[2, 1, -1]</code> runs HTML scraping first, .pdf scraping second, and disables Vision-LLM scraping. Setting it to <code>[2, -1, 1]</code> runs Vision-LLM scraping first, HTML scraping second, and disables .pdf scraping. </p> <p>This latter configuration is exactly what PFD Toolkit uses under the hood to construct the dataset you see when you call <code>load_reports()</code>.</p>"},{"location":"loader/scraper/#a-closer-look-at-the-pipeline","title":"A closer look at the pipeline","text":"<ol> <li>HTML scraping collects data directly from the report landing page. This is the fastest approach and usually recovers most metadata fields (e.g. coroner name, area, receiver) but struggles where the HTML make up of a given report differs, even slightly, from the majority of reports.</li> <li>.pdf scraping downloads the report .pdf and extracts text with PyMuPDF. This approach also recovers most fields, but will often scrape page numbers, footnotes and other .pdf 'juice'. It will fail where a report uses a non-standard heading (e.g. uses just \"Concerns\" instead of the more common \"Coroner's concerns\").</li> <li>LLM scraping is by far the most reliable method, but also the longest. The LLM understands the reports in context, meaning it doesn't matter if a report is configured slightly differently.</li> </ol> <p>The stages cascade automatically\u2014if HTML scraping gathers everything you need, the PDF and LLM steps are skipped. You can reorder or disable steps entirely by tweaking <code>scraping_strategy</code>.</p>"},{"location":"loader/scraper/#running-a-scrape","title":"Running a scrape","text":"<p>After initialisation, call <code>scrape_reports()</code> to run the full scrape:</p> <pre><code>df = scraper.scrape_reports()\n</code></pre> <p>The results are cached on <code>scraper.reports</code> as a pandas DataFrame. This cache lets you rerun individual stages without hitting the network again. If more reports are published later you can update the existing DataFrame with <code>top_up()</code>:</p> <pre><code>updated = scraper.top_up(existing_df=df, end_date=\"2025-01-31\", clean=True)\n</code></pre> <p><code>top_up()</code> only fetches new pages, meaning you avoid repeating work and keep the original ordering intact. When <code>clean=True</code> the new and existing rows are passed through <code>Cleaner.clean_reports()</code> for optional LLM-powered tidying.</p>"},{"location":"loader/scraper/#applying-the-llm-fallback-separately","title":"Applying the LLM fallback separately","text":"<p>Sometimes you may want to review scraped results before running the LLM stage. <code>run_llm_fallback()</code> accepts a DataFrame (typically the output of <code>scrape_reports()</code> or <code>top_up()</code>) and attempts to fill any remaining blanks using your configured language model:</p> <pre><code>llm_df = scraper.run_llm_fallback(df)\n</code></pre>"},{"location":"loader/scraper/#threading-and-polite-scraping","title":"Threading and polite scraping","text":"<p><code>Scraper</code> uses a thread pool to speed up network requests. The <code>max_workers</code> and <code>delay_range</code> settings let you tune throughput and avoid overloading the server. The default one\u2013two second delay between requests mirrors human browsing behaviour and greatly reduces the risk of your IP address being flagged.</p>"},{"location":"loader/scraper/#inspecting-results","title":"Inspecting results","text":"<p>Every scrape writes a timestamp column when <code>include_time_stamp=True</code>. This can be useful for auditing your scraping pipeline. </p> <p>All fields that could not be extracted are set to missing values, making gaps explicit in the final dataset.</p>"},{"location":"loader/scraper/#caveats","title":"Caveats","text":"<p>Since scraping relies on the judiciary.uk website, any changes to layout could easily break parsers. The toolkit aims to handle common edge cases, but if you rely on scraping for production work you should keep an eye on logs and be ready to adapt your strategy. Also remember that running the LLM scraping stage incurs API costs if you use a paid provider.</p> <p>See the API reference for a detailed breakdown of every argument and attribute.</p>"},{"location":"reference/cleaner/","title":"<code>Cleaner</code>","text":"<p>Batch-clean PFD report fields with an LLM.</p> <p>The cleaner loops over selected columns, builds field-specific prompts and writes the returned text back into a copy of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>Input DataFrame to clean.</p> required <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>include_coroner</code> <code>bool</code> <p>Clean the <code>Coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Clean the <code>Receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Clean the <code>Area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Clean the <code>InvestigationAndInquest</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Clean the <code>CircumstancesOfDeath</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Clean the <code>MattersOfConcern</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>coroner_prompt</code> <code>str or None</code> <p>Custom prompt for the coroner field. Defaults to <code>None</code>.</p> <code>None</code> <code>area_prompt</code> <code>str or None</code> <p>Custom prompt for the area field. Defaults to <code>None</code>.</p> <code>None</code> <code>receiver_prompt</code> <code>str or None</code> <p>Custom prompt for the receiver field. Defaults to <code>None</code>.</p> <code>None</code> <code>investigation_prompt</code> <code>str or None</code> <p>Custom prompt for the investigation field. Defaults to <code>None</code>.</p> <code>None</code> <code>circumstances_prompt</code> <code>str or None</code> <p>Custom prompt for the circumstances field. Defaults to <code>None</code>.</p> <code>None</code> <code>concerns_prompt</code> <code>str or None</code> <p>Custom prompt for the concerns field. Defaults to <code>None</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Emit info-level logs for each batch when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>cleaned_reports</code> <code>DataFrame</code> <p>Result of the last call to <code>clean_reports</code>.</p> <code>coroner_prompt_template, area_prompt_template, ...</code> <code>str</code> <p>Finalised prompt strings actually sent to the model.</p> <p>Examples:</p> <p>Basic usage::</p> <pre><code>cleaner = Cleaner(df, llm, include_coroner=False, verbose=True)\ncleaned_df = cleaner.clean_reports()\ncleaned_df.head()\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.clean_reports","title":"clean_reports","text":"<pre><code>clean_reports(anonymise=False)\n</code></pre> <p>Run LLM-based cleaning for the configured columns.</p> <p>The method operates in place on a copy of <code>self.reports</code> so the original DataFrame is never mutated.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which the selected columns have been replaced by the LLM output (or left unchanged when the model returns an error marker).</p> <p>Parameters:</p> Name Type Description Default <code>anonymise</code> <code>bool</code> <p>When <code>True</code> append an instruction to anonymise names and pronouns in the investigation, circumstances and concerns fields. Defaults to <code>False</code>.</p> <code>False</code> <p>Examples:</p> <p>Basic usage::</p> <pre><code>cleaned = cleaner.clean_reports()\ncleaned.equals(df)\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.generate_prompt_template","title":"generate_prompt_template","text":"<pre><code>generate_prompt_template()\n</code></pre> <p>Return the prompt templates used for each field.</p> <p>The returned dictionary maps DataFrame column names to the full prompt text with a <code>[TEXT]</code> placeholder appended to illustrate how the prompt will look during <code>clean_reports</code>.</p>"},{"location":"reference/extractor/","title":"<code>Extractor</code>","text":"<p>Extract custom features from Prevention of Future Death reports using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>reports</code> <code>DataFrame</code> <p>DataFrame of PFD reports. When provided it is copied and stored on the instance. Defaults to <code>None</code>.</p> <code>None</code> <code>include_date</code> <code>bool</code> <p>Include the <code>Date</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>Coroner</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Include the <code>Area</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>Receiver</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>InvestigationAndInquest</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>CircumstancesOfDeath</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>MattersOfConcern</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Emit extra logging when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.discover_themes","title":"discover_themes","text":"<pre><code>discover_themes(\n    *,\n    warn_exceed=100000,\n    error_exceed=500000,\n    max_themes=None,\n    min_themes=None,\n    extra_instructions=None,\n    seed_topics=None,\n)\n</code></pre> <p>Use an LLM to automatically discover report themes.</p> <p>The method expects <code>summarise</code> to have been run so that a summary column exists. All summaries are concatenated into one prompt sent to the LLM. The LLM should return a JSON object mapping theme names to descriptions. A new <code>pydantic</code> model is built from this mapping and stored as <code>feature_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>warn_exceed</code> <code>int</code> <p>Emit a warning if the estimated token count exceeds this value. Defaults to <code>100000</code>.</p> <code>100000</code> <code>error_exceed</code> <code>int</code> <p>Raise a <code>ValueError</code> if the estimated token count exceeds this value. Defaults to <code>500000</code>.</p> <code>500000</code> <code>max_themes</code> <code>int or None</code> <p>Instruct the LLM to identify no more than this number of themes when provided.</p> <code>None</code> <code>min_themes</code> <code>int or None</code> <p>Instruct the LLM to identify at least this number of themes when provided.</p> <code>None</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions appended to the theme discovery prompt.</p> <code>None</code> <code>seed_topics</code> <code>str | list[str] | BaseModel</code> <p>Optional seed topics to include in the prompt. These are treated as starting suggestions and the model should incorporate them into a broader list of themes.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The generated feature model containing discovered themes.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(col_name=None, return_series=False)\n</code></pre> <p>Estimate token counts for all rows of a given column using the <code>tiktoken</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Name of the column containing report summaries. Defaults to <code>summary_col</code>, which is generated after running <code>summarise</code>.</p> <code>None</code> <code>return_series</code> <code>bool</code> <p>Returns a pandas.Series of per-row token counts for that field if <code>True</code>, or an integer if <code>False</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, Series]</code> <p>If <code>return_series</code> is <code>False</code>, returns an <code>int</code> representing the total sum of all token counts across all rows for the provided field. If <code>return_series</code> is <code>True</code>, returns a <code>pandas.Series</code> of token counts aligned to <code>self.reports</code> for the provided field.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.export_cache","title":"export_cache","text":"<pre><code>export_cache(path='extractor_cache.pkl')\n</code></pre> <p>Save the current cache to <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be created inside it.</p> <code>'extractor_cache.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the written cache file.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(\n    reports=None,\n    *,\n    feature_model=None,\n    produce_spans=False,\n    drop_spans=False,\n    force_assign=False,\n    allow_multiple=False,\n    schema_detail=\"minimal\",\n    extra_instructions=None,\n    skip_if_present=True,\n)\n</code></pre> <p>Run feature extraction for the given reports.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>DataFrame of reports to process. Defaults to the instance's stored reports if omitted.</p> <code>None</code> <code>feature_model</code> <code>type[BaseModel]</code> <p>Pydantic model describing the features to extract. Must be provided on first call or after calling <code>discover_themes</code>.</p> <code>None</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code>, create <code>spans_</code> versions of each feature to capture the supporting text snippets. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, remove all <code>spans_</code> columns from the returned DataFrame after extraction. If <code>produce_spans</code> is <code>False</code> a warning is emitted and no columns are dropped. Defaults to <code>False</code>.</p> <code>False</code> <code>force_assign</code> <code>bool</code> <p>When <code>True</code>, the LLM is instructed to avoid returning :data:<code>GeneralConfig.NOT_FOUND_TEXT</code> for any feature.</p> <code>False</code> <code>allow_multiple</code> <code>bool</code> <p>Allow a report to be assigned to multiple categories when <code>True</code>.</p> <code>False</code> <code>schema_detail</code> <code>('full', 'minimal')</code> <p>Level of detail for the feature schema included in the prompt.</p> <code>\"full\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions injected into each prompt before the schema.</p> <code>None</code> <code>skip_if_present</code> <code>bool</code> <p>When <code>True</code> (default), skip rows when any feature column already holds a non-missing value that is not equal to :data:<code>GeneralConfig.NOT_FOUND_TEXT</code>. This assumes the row has been processed previously and is logged in an instance of <code>Extractor.cache</code></p> <code>True</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.import_cache","title":"import_cache","text":"<pre><code>import_cache(path='extractor_cache.pkl')\n</code></pre> <p>Load cache from <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be loaded from inside it.</p> <code>'extractor_cache.pkl'</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset internal caches and intermediate state.</p> <p>This clears any cached feature extraction results and token estimations so that <code>extract_features</code> can be run again on the same reports. The instance itself is returned to allow method chaining, e.g. <code>extractor.reset().extract_features()</code>.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.summarise","title":"summarise","text":"<pre><code>summarise(\n    result_col_name=\"summary\",\n    trim_intensity=\"medium\",\n    extra_instructions=None,\n)\n</code></pre> <p>Summarise selected report fields into one column using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>result_col_name</code> <code>str</code> <p>Name of the summary column. Defaults to <code>\"summary\"</code>.</p> <code>'summary'</code> <code>trim_intensity</code> <code>('low', 'medium', 'high', 'very high')</code> <p>Controls how concise the summary should be. Defaults to <code>\"medium\"</code>.</p> <code>\"low\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions to append to the prompt before the report excerpt.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame identical to the one provided at initialisation with an extra summary column.</p>"},{"location":"reference/llm/","title":"<code>LLM</code>","text":"<p>Wrapper around the OpenAI Python SDK for batch prompting and PDF vision fallback.</p> <p>The helper provides:</p> <ul> <li><code>generate</code> for plain or vision-enabled prompts with optional pydantic   validation.</li> <li><code>_call_llm_fallback</code> used by the scraper when HTML and PDF heuristics   fail.</li> <li>Built-in back-off and host-wide throttling via a semaphore.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI (or proxy) API key. Defaults to <code>None</code> which expects the environment variable to be set.</p> <code>None</code> <code>model</code> <code>str</code> <p>Chat model name. Defaults to <code>\"gpt-4.1-mini\"</code>.</p> <code>'gpt-4.1-mini'</code> <code>base_url</code> <code>str or None</code> <p>Override the OpenAI endpoint. Defaults to <code>None</code>.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum parallel workers for batch calls and for the global semaphore. Defaults to <code>8</code>.</p> <code>8</code> <code>temperature</code> <code>float</code> <p>Sampling temperature used for all requests. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>seed</code> <code>int or None</code> <p>Deterministic seed value passed to the API. Defaults to <code>None</code>.</p> <code>None</code> <code>validation_attempts</code> <code>int</code> <p>Number of times to retry parsing LLM output into a pydantic model. Defaults to <code>2</code>.</p> <code>2</code> <code>timeout</code> <code>float | Timeout | None</code> <p>Override the HTTP timeout in seconds. <code>None</code> uses the OpenAI client default of 600 seconds.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>_sem</code> <code>Semaphore</code> <p>Global semaphore that limits concurrent requests to max_workers.</p> <code>client</code> <code>Client</code> <p>Low-level SDK client configured with key and base URL.</p> <p>Examples:</p> <p>Basic usage::</p> <pre><code>llm = LLM(api_key=\"sk-...\", model=\"gpt-4o-mini\", temperature=0.2,\n          timeout=600)\nout = llm.generate([\"Hello world\"])\nout[0]\n</code></pre>"},{"location":"reference/llm/#pfd_toolkit.LLM.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(texts, model=None)\n</code></pre> <p>Return token counts for text using <code>tiktoken</code>.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str] | str</code> <p>Input strings to tokenise.</p> required <code>model</code> <code>str</code> <p>Model name for selecting the encoding. Defaults to <code>self.model</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>Token counts in the same order as <code>texts</code>.</p>"},{"location":"reference/llm/#pfd_toolkit.LLM.generate","title":"generate","text":"<pre><code>generate(\n    prompts,\n    images_list=None,\n    response_format=None,\n    max_workers=None,\n    tqdm_extra_kwargs=None,\n)\n</code></pre> <p>Run many prompts either sequentially or in parallel.</p> <pre><code>    Parameters\n</code></pre> <pre><code>    prompts : list[str]\n        List of user prompts. One prompt per model call.\n\n    images_list : list[list[bytes]] or None, optional\n        For vision models: a parallel list where each inner list\n        holds **base64-encoded** JPEG pages for that prompt.  Use\n        *None* to send no images.\n\n    response_format : type[pydantic.BaseModel] or None, optional\n        If provided, each response is parsed into that model via the\n        *beta/parse* endpoint; otherwise a raw string is returned.\n\n    max_workers : int or None, optional\n        Thread count just for this batch. ``None`` uses the instance-wide\n        ``max_workers`` value. Defaults to ``None``.\n</code></pre> <pre><code>    Returns\n</code></pre> <pre><code>    list[Union[pydantic.BaseModel, str]]\n        Results in the same order as `prompts`.\n</code></pre> <pre><code>    Raises\n</code></pre> <pre><code>    openai.RateLimitError\n        Raised only if the exponential back-off exhausts all retries.\n    openai.APIConnectionError\n        Raised if network issues persist beyond the retry window.\n    openai.APITimeoutError\n        Raised if the API repeatedly times out.\n</code></pre> <pre><code>    Examples\n</code></pre> <pre><code>    Generate multiple summaries::\n\n        msgs = [\"Summarise:\n</code></pre> <p>\" + txt for txt in docs]             summaries = llm.generate(msgs, max_workers=8)</p>"},{"location":"reference/loader/","title":"<code>load_reports</code>","text":"<p>Load the bundled Prevention of Future Death reports as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>category</code> <code>str</code> <p>Judiciary category slug. Defaults to <code>\"all\"</code>.</p> <code>'all'</code> <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2000-01-01\"</code>.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2050-01-01\"</code>.</p> <code>'2050-01-01'</code> <code>n_reports</code> <code>int or None</code> <p>Keep only the most recent <code>n_reports</code> rows after filtering by date. <code>None</code> (the default) returns all rows.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Reports filtered by date, sorted newest first and optionally limited to <code>n_reports</code> rows.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If start_date is after end_date.</p> <code>FileNotFoundError</code> <p>If the bundled CSV cannot be located.</p> <p>Examples:</p> <p>Load reports for a specific period::</p> <pre><code>from pfd_toolkit import load_reports\ndf = load_reports(start_date=\"2020-01-01\", end_date=\"2022-12-31\", n_reports=1000)\ndf.head()\n</code></pre>"},{"location":"reference/scraper/","title":"<code>Scraper</code>","text":"<p>Scrape UK \u201cPrevention of Future Death\u201d (PFD) reports into a pandas.DataFrame.</p> <p>The extractor runs in three cascading layers (<code>html \u2192 pdf \u2192 llm</code>), each independently switchable.</p> <ol> <li>HTML scrape \u2013 parse metadata and rich sections directly from    the web page.</li> <li>PDF fallback \u2013 download the attached PDF and extract text with    PyMuPDF for any missing fields.</li> <li>LLM fallback \u2013 delegate unresolved gaps to a Large Language    Model supplied via llm.</li> </ol> <p>Each layer can be enabled or disabled via <code>scraping_strategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM | None</code> <p>Client implementing <code>_call_llm_fallback()</code>; required only when the LLM stage is enabled.</p> <code>None</code> <code>category</code> <code>str</code> <p>Judiciary category slug (e.g. <code>\"suicide\"</code>, <code>\"hospital_deaths\"</code>) or <code>\"all\"</code>.</p> <code>'all'</code> <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2050-01-01'</code> <code>max_workers</code> <code>int</code> <p>Thread-pool size for concurrent scraping.</p> <code>10</code> <code>max_requests</code> <code>int</code> <p>Maximum simultaneous requests per host (enforced with a semaphore).</p> <code>5</code> <code>delay_range</code> <code>tuple[float, float] | None</code> <p>Random delay (seconds) before every request. Use <code>None</code> to disable (not recommended).</p> <code>(1, 2)</code> <code>timeout</code> <code>int</code> <p>Per-request timeout in seconds.</p> <code>60</code> <code>scraping_strategy</code> <code>list[int] | tuple[int, int, int]</code> <p>Defines the order in which HTML, PDF and LLM scraping are attempted. The sequence indexes correspond to <code>(HTML, PDF, LLM)</code>. Provide <code>-1</code> to disable a stage.  For example <code>[1, 2, -1]</code> runs HTML first, then PDF, and disables LLM scraping.</p> <code>[1, 2, 3]</code> <code>include_url</code> <code>bool</code> <p>Include the <code>url</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_id</code> <code>bool</code> <p>Include the <code>id</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_date</code> <code>bool</code> <p>Include the <code>date</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Include the <code>area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>investigation</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>circumstances</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>concerns</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_time_stamp</code> <code>bool</code> <p>Include a <code>date_scraped</code> column. Defaults to <code>False</code>.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Emit debug-level logs when True.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>reports</code> <code>DataFrame | None</code> <p>Cached result of the last call to <code>scrape_reports</code> or <code>top_up</code>.</p> <code>report_links</code> <code>list[str]</code> <p>URLs discovered by <code>get_report_links</code>.</p> <code>NOT_FOUND_TEXT</code> <code>str</code> <p>Placeholder value set when a field cannot be extracted.</p> <p>Examples:</p> <p>Basic usage::</p> <pre><code>from pfd_toolkit import Scraper\nscraper = Scraper(\n    category=\"suicide\",\n    start_date=\"2020-01-01\",\n    end_date=\"2022-12-31\",\n    scraping_strategy=[1, 2, 3],\n    llm=my_llm_client,\n)\ndf = scraper.scrape_reports()          # full scrape\nnewer_df = scraper.top_up(df)          # later \"top-up\"\nadded_llm_df = scraper.run_llm_fallback(df)  # apply LLM retro-actively\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.get_report_links","title":"get_report_links","text":"<pre><code>get_report_links()\n</code></pre> <p>Discover individual report URLs for the current query, across all pages.</p> <p>Iterates through _get_report_href_values (which collects URLs for a single page).</p> <p>Pagination continues until a page yields zero new links.</p> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>All discovered URLs, or None if no links were found for the given category/date window.</p>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.run_llm_fallback","title":"run_llm_fallback","text":"<pre><code>run_llm_fallback(reports_df=None)\n</code></pre> <p>Ask the LLM to fill cells still set to <code>self.NOT_FOUND_TEXT</code>.</p> <p>Only the missing fields requested via <code>include_*</code> flags are sent to the model, along with the report\u2019s PDF bytes (when available).</p> <p>Parameters:</p> Name Type Description Default <code>reports_df</code> <code>DataFrame | None</code> <p>DataFrame to process. Defaults to <code>self.reports</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same shape as <code>reports_df</code>, updated in place and re-cached to <code>self.reports</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no LLM client was supplied at construction time.</p> <p>Examples:</p> <p>Run the fallback step after scraping::</p> <pre><code>updated_df = scraper.run_llm_fallback()\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.scrape_reports","title":"scrape_reports","text":"<pre><code>scrape_reports()\n</code></pre> <p>Execute a full scrape with the Class configuration.</p> Workflow <ol> <li>Call <code>get_report_links</code>.</li> <li>Extract each report according to <code>scraping_strategy</code>.</li> <li>Cache the final DataFrame to <code>self.reports</code>.</li> </ol> <p>Returns:</p> Type Description <code>DataFrame</code> <p>One row per report.  Column presence matches the <code>include_*</code> flags. The DataFrame is empty if nothing was scraped.</p> <p>Examples:</p> <p>Scrape reports and inspect columns::</p> <pre><code>df = scraper.scrape_reports()\ndf.columns\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.top_up","title":"top_up","text":"<pre><code>top_up(\n    old_reports=None,\n    start_date=None,\n    end_date=None,\n    clean=False,\n)\n</code></pre> <p>Check for and append new PFD reports within the current parameters.</p> <p>If new links are found they are scraped and appended to <code>self.reports</code>. Any URL (or ID) already present in old_reports is skipped.</p> <p>Optionally, you can override the start_date and end_date parameters from <code>self</code> for this call only.</p> <p>Parameters:</p> Name Type Description Default <code>old_reports</code> <code>DataFrame | None</code> <p>Existing DataFrame. Defaults to <code>self.reports</code>.</p> <code>None</code> <code>start_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>end_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>clean</code> <code>bool</code> <p>When <code>True</code>, run the <code>Cleaner</code> on the newly scraped rows before merging them with existing reports.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>Updated DataFrame if new reports were added; None if no new records were found and old_reports was None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If old_reports lacks columns required for duplicate checks.</p> <p>Examples:</p> <p>Add new reports to an existing DataFrame::</p> <pre><code>updated = scraper.top_up(df, end_date=\"2023-01-01\")\nlen(updated) - len(df)  # number of new reports\n</code></pre>"},{"location":"reference/screener/","title":"<code>Screener</code>","text":"<p>Classifies a list of report texts against a user-defined topic using an LLM.</p> <p>This class takes a DataFrame of reports, a user query, and various configuration options to classify whether each report matches the query. It can either filter the DataFrame to return only matching reports or add a classification column to the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>An instance of the LLM class from <code>pfd_toolkit</code>.</p> <code>None</code> <code>reports</code> <code>DataFrame</code> <p>A DataFrame containing Prevention of Future Death reports.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print more detailed logs. Defaults to False.</p> <code>False</code> <code>include_date</code> <code>bool</code> <p>Flag to determine if the 'Date' column is included. Defaults to False.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Flag to determine if the 'CoronerName' column is included. Defaults to False.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Flag to determine if the 'Area' column is included. Defaults to False.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Flag to determine if the 'Receiver' column is included. Defaults to False.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Flag to determine if the 'InvestigationAndInquest' column is included. Defaults to True.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Flag to determine if the 'CircumstancesOfDeath' column is included. Defaults to True.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Flag to determine if the 'MattersOfConcern' column is included. Defaults to True.</p> <code>True</code> <p>Examples:</p> <p>Basic usage::</p> <pre><code>user_topic = \"medication errors\"\nllm_client = LLM()\nscreener = Screener(llm=llm_client, reports=reports_df)\nscreened_reports = screener.screen_reports(user_query=user_topic)\nprint(f\"Found {len(screened_reports)} report(s) on '{user_topic}'.\")\n</code></pre>"},{"location":"reference/screener/#pfd_toolkit.Screener.screen_reports","title":"screen_reports","text":"<pre><code>screen_reports(\n    reports=None,\n    user_query=None,\n    filter_df=True,\n    result_col_name=\"matches_query\",\n    produce_spans=False,\n    drop_spans=False,\n)\n</code></pre> <p>Classifies reports in the DataFrame against the user-defined topic using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>If provided, this DataFrame will be used for screening, replacing any DataFrame stored in the instance for this call.</p> <code>None</code> <code>user_query</code> <code>str</code> <p>If provided, this query will be used, overriding any query stored in the instance for this call. The prompt template will be rebuilt.</p> <code>None</code> <code>filter_df</code> <code>bool</code> <p>If <code>True</code> the returned DataFrame is filtered to only matching reports. Defaults to <code>True</code>.</p> <code>True</code> <code>result_col_name</code> <code>str</code> <p>Name of the boolean column added when <code>filter_df</code> is <code>False</code>. Defaults to <code>\"matches_query\"</code>.</p> <code>'matches_query'</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code> a <code>spans_matches_topic</code> column is created containing the text snippet that justified the classification. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, the <code>spans_matches_topic</code> column is removed from the returned DataFrame. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Either a filtered DataFrame (if <code>filter_df</code> is <code>True</code>), or the original DataFrame with an added classification column.</p> <p>Examples:</p> <p>Example session::</p> <pre><code>reports_df = pd.DataFrame(data)\nscreener = Screener(LLM(), reports=reports_df)\n\n# Screen reports with the initial query\nfiltered_df = screener.screen_reports(user_query=\"medication safety\")\n\n# Screen the same reports with a new query and add a classification column\nclassified_df = screener.screen_reports(user_query=\"tree safety\", filter_df=False)\n</code></pre>"},{"location":"screener/","title":"Screen reports for relevancy","text":"<p>The <code>Screener</code> class filters PFD reports according to a plain\u2011English query. The following pages cover how to use it:</p> <ul> <li>Getting started \u2013 run your first screen and understand why an LLM beats simple keyword search.</li> <li>Additional options \u2013 annotate instead of filtering and control which columns are read.</li> <li>Tips for writing a good user query \u2013 craft prompts that get accurate results.</li> </ul>"},{"location":"screener/basics/","title":"Getting started","text":"<p>Natural-language filtering is one of the headline features of PFD Toolkit. The <code>Screener</code> class lets you describe a topic in plain English \u2013 e.g. \"deaths in police custody\" \u2013 and have an LLM screen reports, delivering you a curated dataset.</p> <p>You do not have to use <code>Screener</code> to benefit from the toolkit. If the built-in category tags are good enough, the <code>load_reports()</code> helper will give you a ready-made dataset without any LLM calls. <code>Screener</code> is offered for those projects that need something more nuanced than the provided broad category tags.</p> <p>To use the <code>Screener</code> you'll need to set up an LLM client.</p>"},{"location":"screener/basics/#a-minimal-example","title":"A minimal example","text":"<p>First, import the necessary modules, load reports and set up an <code>LLM</code> client:</p> <pre><code>from pfd_toolkit import load_reports, LLM, Screener\n\n# Grab the pre-processed April 2025 dataset\nreports = load_reports(category=\"all\",\n                       start_date=\"2025-04-01\",\n                       end_date=\"2025-04-30\")\n\n# Set up your LLM client (see \u201cCreating an LLM client\u201d for details)\nllm_client = LLM(api_key=\"YOUR-API-KEY\")\n</code></pre> <p>Then describe the reports you're interested in, pass the query to <code>Screener</code> and you'll be given a filtered dataset containing matching reports.</p> <pre><code>user_query = \"deaths in police custody\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports\n)\n\npolice_df = screener.screen_reports(user_query=user_query, filter_df=True)\n\nprint(f\"{len(police_df)} reports matched.\")\n&gt;&gt; \"13 reports matched.\"\n</code></pre>"},{"location":"screener/basics/#why-not-just-have-a-normal-search-function","title":"Why not just have a normal \"search\" function?","text":"<p>A keyword search is only as good as the exact words you type. Coroners, however, don't always follow a shared vocabulary. The same idea can surface in wildly different forms:</p> <ul> <li>Under-staffing might be written as \"staff shortages,\" \"inadequate nurse cover,\" or even \"resource constraints.\"</li> <li>Suicide in prisons may masquerade as \"self-inflicted injury while remanded,\" \"ligature event in cell,\" or appear across separate sentences.</li> </ul> <p>A keyword filter misses these variants unless you guess every synonym in advance. By contrast, an <code>LLM</code> understands the context behind your query and links the phrasing for you, which is exactly what <code>Screener</code> taps into.</p>"},{"location":"screener/options/","title":"Additional options","text":""},{"location":"screener/options/#annotation-vs-filtering","title":"Annotation vs. filtering","text":"<p>If <code>filter_df</code> is True (the default) <code>Screener</code> returns a trimmed DataFrame that contains only the reports the LLM marked as relevant to your query.</p> <p>Setting it to False activates annotate mode: every report/row from your original DataFrame is kept, and a boolean column is added denoting whether the report met your query or not. You can also rename this column with <code>result_col_name</code>.</p> <p>A common workflow is to screen once with <code>filter_df=False</code>, inspect a few borderline cases, then rerun with <code>filter_df=True</code> once you trust the settings.</p> <pre><code>screener = Screener(\n    llm=llm_client,\n    reports=reports,\n)\n\nannotated = screener.screen_reports(\n    user_query=user_query,\n    filter_df=False,    # &lt;--- create annotation column; don't filter out\n    result_col_name='custody_match'     # &lt;--- name of annotation column\n)\n</code></pre>"},{"location":"screener/options/#choosing-which-columns-the-llm-sees","title":"Choosing which columns the LLM 'sees'","text":"<p>By default the LLM model reads the narrative heavyweight sections of each report: investigation, circumstances and concerns. You can expose or hide any field with <code>include_*</code> flags.</p> <p>For example, if you are screening based on a specific cause of death, then you should consider setting <code>include_concerns</code> to False, as including this won't benefit your search.</p> <p>By contrast, if you are searching for a specific concern, then setting <code>include_investigation</code> and <code>include_circumstances</code> to False may improve accuracy, speed up your code, and lead to cheaper LLM calls.</p> <pre><code>user_query = \"Death from insulin overdose due to misprogrammed insulin pumps.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n    include_concerns=False    # &lt;--- Our query doesn't need this section\n)\n\nresult = screener.screen_reports(user_query=user_query)\n</code></pre> <p>In another example, let's say we are only interested in reports sent to a Member of Parliament. We'll want to turn off all default sections and only read from the receiver column.</p> <pre><code>user_query = \"Whether the report was sent to a Member of Parliament (MP)\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n\n    # Turn off the defaults...\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=False,\n\n    include_receiver=True       # &lt;--- Read from receiver section\n)\n\nresult = screener.screen_reports(user_query=user_query)\n</code></pre>"},{"location":"screener/options/#all-options-and-defaults","title":"All options and defaults","text":"Flag Report section What it's useful for Default <code>include_coroner</code> Coroner\u2019s name Simply the name of the coroner. Rarely needed for screening. <code>False</code> <code>include_area</code> Coroner\u2019s area Useful for geographic questions, e.g.\u00a0deaths in South-East England. <code>False</code> <code>include_receiver</code> Receiver(s) of the report Great for accountability queries, e.g. reports sent to NHS Wales. <code>False</code> <code>include_investigation</code> \u201cInvestigation &amp; Inquest\u201d section Contains procedural detail about the inquest. <code>True</code> <code>include_circumstances</code> \u201cCircumstances of Death\u201d section Describes what actually happened; holds key facts about the death. <code>True</code> <code>include_concerns</code> \u201cCoroner\u2019s Concerns\u201d section Lists the issues the coroner wants addressed \u2014 ideal for risk screening. <code>True</code>"},{"location":"screener/options/#returning-text-spans","title":"Returning text spans","text":"<p>Set <code>produce_spans=True</code> when calling <code>.screen_reports()</code> to capture the exact lines from the report that justified each classification. A new column called <code>spans_matches_topic</code> will be created containing these verbatim snippets. If you only want to use the spans internally, pass <code>drop_spans=True</code> to remove the column from the returned DataFrame after screening.</p> <pre><code>screener = Screener(llm=llm_client,\n                    reports=reports)\nannotated = screener.screen_reports(user_query=\"needle\", produce_spans=True, drop_spans=False)\n</code></pre>"},{"location":"screener/tips/","title":"Tips for writing a good user query","text":"<ol> <li>Stick to one core idea. Give the LLM a single, clear subject: \u201cfalls from hospital beds,\u201d \u201ccarbon-monoxide poisoning at home.\u201d In general, the shorter the prompt, the less room for misinterpretation.</li> <li>Avoid nested logic. Complex clauses like \u201csuicide and medication error but not in custody\u201d dilute the signal. Consider running separate screens (suicide; medication error; in custody) and combine or subtract results later with pandas.</li> <li>Let the model handle synonyms. You don\u2019t need \u201cdefective, faulty, malfunctioning\u201d all in the same query; \u201cmalfunctioning defibrillators\u201d is enough.</li> <li>Use positive phrasing. Negations (e.g. \u201cnot related to COVID-19\u201d) can flip the model\u2019s reasoning. Screen positively, set <code>filter_df</code> to False, then drop rows in pandas.</li> <li>Keep it readable. If your query needs multiple commas or parentheses, break it up. A one-line statement without side notes usually performs best.</li> </ol> <p>Examples:</p>  Less-effective query Why it struggles  Better query \u201cDeaths where someone slipped or fell in hospital corridors or patient rooms and maybe had fractures but not clinics\u201d Too long, multiple settings, negative clause \u201cFalls on inpatient wards\u201d \u201cFires or explosions causing death at home including gas leaks but not industrial accidents\u201d Mixes two ideas (home vs. industrial) plus a negation \u201cDomestic gas explosions\u201d \u201cCases involving children and allergic reactions to nuts during school outings\u201d Several concepts (age, allergen, setting) \u201cFatal nut allergy on school trip\u201d \u201cRailway incidents that resulted in death due to being hit by train while trespassing or at crossings\u201d Two scenarios joined by \u201cor\u201d; verbose \u201cTrespasser struck by train\u201d \u201cPatients dying because an ambulance was late or there was delay in emergency services arrival or they couldn't get one\u201d Chain of synonyms and clauses \u201cDeath from delayed ambulance\u201d \u201cErrors in giving anaesthesia, like too much anaesthetic, wrong drug, problems with intubation, etc.\u201d Long list invites confusion; \u201cetc.\u201d is vague \u201cAnaesthesia error\u201d"}]}