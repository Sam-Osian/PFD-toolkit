{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PFD Toolkit \u2013 Turn PFD reports into actionable insights","text":"<p>PFD Toolkit is a text-to-table engine for Prevention of Future Deaths (PFD) reports. It\u2019s an open-source Python package that lets you automatically extract, structure, and analyse PFD reports from coroners in England and Wales \u2014 turning an unwieldy archive into a clean, research-ready dataset in minutes.</p> <p>PFD reports are vital public warnings, but until now, they\u2019ve been a nightmare to analyse: inconsistent formats, missing metadata, no way to mass download report content into a neat, tabular dataset \u2013 among many other issues.</p> <p>PFD Toolkit solves this by using AI (including LLMs and Vision models) to read, clean, and standardise every report, whether typed or scanned.</p> <p>What does this mean in practice? Instead of weeks of manual coding, you get instant access to:</p> <ul> <li>Bulk-download structured datasets of PFD reports</li> <li>Screen and filter reports by any research question (e.g. \u201croad safety\u201d, \u201csuicide\u201d, or any other query)</li> <li>Generate concise summaries and extract key variables, themes, or concerns</li> <li>Automatically spot patterns and recurring issues across reports</li> <li>Output ready-to-use tables for charts, analysis, or policy work</li> </ul> <p>Here\u2019s a sample of the PFD dataset you can load:</p> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m... <p>Each row is an individual report, while each column reflects a section of the report. For more information on each of these columns, see here.</p>"},{"location":"#why-does-this-matter","title":"Why does this matter?","text":"<p>Despite being public warnings, PFD reports are chronically underused. That\u2019s because they\u2019ve historically been a pain to work with: scattered formats, inconsistent categorisation, and a lack of easy access for researchers. As a result, it\u2019s been almost impossible to spot trends or respond to risks quickly.</p> <p>PFD Toolkit changes this. By automating the messy admin typically involved in a PFD research project, it transforms unstructured coroners\u2019 text into neat datasets \u2014 making it finally practical to do timely systematic research, policy analysis, or audit work on preventable deaths.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install PFD Toolkit using pip:</p> <pre><code>pip install pfd_toolkit\n</code></pre> <p>To update, run:</p> <pre><code>pip install -U pfd_toolkit\n</code></pre>"},{"location":"#licence","title":"Licence","text":"<p>This project is distributed under the GNU Affero General Public License v3.0 (AGPL-3.0).</p> <p>Note</p> <ul> <li>You are welcome to use, modify, and share this code under the terms of the AGPL-3.0.</li> <li>If you use this code to provide a networked service, you are required to make the complete source code available to users of that service.</li> <li>Some project dependencies may have their own licence terms, which could affect certain types of use (e.g. commercial use).</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>PFD Toolkit is designed as a research-enabling tool, and we\u2019re keen to work with the community to make sure it genuinely meets your needs. If you have feedback, ideas, or want to get involved, head to our Feedback &amp; contributions page.</p>"},{"location":"#how-to-cite","title":"How to cite","text":"<p>If you use PFD Toolkit in your research, please cite the archived release:</p> <p>Osian, S., &amp; Pytches, J. (2025). PFD Toolkit: Unlocking Prevention of Future Death Reports for Research (Version 0.3.5) [Software]. Zenodo. https://doi.org/10.5281/zenodo.15729717</p> <p>Or, in BibTeX:</p> <pre><code>@software{osian2025pfdtoolkit,\n  author       = {Sam Osian and Jonathan Pytches},\n  title        = {PFD Toolkit: Unlocking Prevention of Future Death Reports for Research},\n  year         = {2025},\n  version      = {0.3.5},\n  doi          = {10.5281/zenodo.15729717},\n  url          = {https://github.com/sam-osian/PFD-toolkit}\n}\n</code></pre>"},{"location":"changelog/","title":"\ud83d\udcc6 Changelog","text":"<p>Welcome to the project changelog. All notable changes to this project will be documented below.</p>"},{"location":"changelog/#035-2025-07-07","title":"0.3.5 - 2025-07-07","text":"<ul> <li>Attempt to fix issue where PFD Toolkit refused to run in Google Colab</li> </ul>"},{"location":"changelog/#034-2025-07-07","title":"0.3.4 - 2025-07-07","text":"<ul> <li>Deprecate <code>user_query</code> in <code>Screener</code> in favour of <code>search_query</code>. <code>user_query</code> will be removed in a future release.</li> <li>Dropping spans in <code>extract_features()</code> no longer removes spans added during screening.</li> <li>Downgrade pandas from 2.3.0 to 2.2.2</li> <li>Fix text cleaning bug that expanded dates and removed paragraph spacing.</li> <li>Added tests covering span removal behaviour.</li> </ul>"},{"location":"changelog/#033-2025-06-25","title":"0.3.3 - 2025-06-25","text":"<ul> <li>Improve package installation time</li> <li>Changed default LLM model from GPT-4.1-mini to GPT-4.1</li> </ul>"},{"location":"changelog/#032-2025-06-23","title":"0.3.2 - 2025-06-23","text":"<ul> <li>You no longer need to manually update the <code>pfd_toolkit</code> package to get access to freshly published reports. Instead, run <code>load_reports(refresh=True)</code>.</li> <li>Improve robustness of Scraping module in handling missing data between different scraping strategies.</li> <li>Fix typos and improve documentation.</li> </ul>"},{"location":"changelog/#031-2025-06-19","title":"0.3.1 - 2025-06-19","text":"<ul> <li>Improve reliability of weekly dataset top-ups.</li> </ul>"},{"location":"changelog/#030-2025-06-18","title":"0.3.0 - 2025-06-18","text":"<p>First public release! \u2728</p>"},{"location":"contact/","title":"How to get in touch","text":"<p>Reach out to the lead developer Sam via email or LinkedIn.</p>"},{"location":"contribute/","title":"Feedback &amp; contributions","text":"<p>Thank you for your interest in contributing to PFD Toolkit! We welcome input from researchers, data scientists, developers, and anyone passionate about improving access to coroners\u2019 Prevention of Future Death reports.</p>"},{"location":"contribute/#how-you-can-get-involved","title":"How you can get involved","text":""},{"location":"contribute/#report-issues-bugs","title":"Report issues &amp; bugs","text":"<p>If you encounter a bug, data problem, or unexpected behaviour, please open an issue on Github.</p> <p>Include a clear description and, if possible, steps to reproduce, your Python version, and environment details.</p>"},{"location":"contribute/#suggest-features-or-improvements","title":"Suggest features or improvements","text":"<p>Have an idea for a new feature, a better workflow, or an additional data cleaning/categorisation option?</p> <p>Submit a feature request as a GitHub issue with as much detail as possible.</p>"},{"location":"contribute/#code-contributions","title":"Code contributions","text":"<p>Contributions are welcome - whether fixing bugs, adding new features, improving documentation, or expanding tests.</p> <p>If your change is significant, consider opening an issue first to discuss it.</p> <p>Note</p> <p>Code contributors should: </p> <ul> <li>Fork the repository on GitHub using the Fork button on the repo home page.</li> <li>Clone your fork locally:</li> </ul> <pre><code>git clone https://github.com/&lt;your-github-username&gt;/PFD-toolkit.git\n</code></pre> <ul> <li>Install <code>uv</code> and sync project dependencies, including developer-only dependencies:</li> </ul> <pre><code>uv sync --dev\n</code></pre> <ul> <li> <p>Create a branch for your work</p> </li> <li> <p>Submit a pull request when you're ready</p> </li> </ul>"},{"location":"contribute/#anything-else-get-in-touch","title":"Anything else, get in touch \ud83d\udcac","text":"<p>If you have any questions, feedback or would like to contribute in a way not outlined above, please contact Sam on samoand@liverpool.ac.uk.</p>"},{"location":"llm_advanced/","title":"Advanced usage","text":""},{"location":"llm_advanced/#use-a-custom-endpoint","title":"Use a custom endpoint","text":"<p>You can set a custom endpoint (e.g. for Azure, Ollama, etc.). Your endpoint must support the OpenAI SDK. </p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    base_url=\"https://...\"   # Set your custom endpoint\n)\n</code></pre>"},{"location":"llm_advanced/#set-temperature-and-seed","title":"Set temperature and seed","text":"<p>LLMs are highly stochastic, and may produce slightly different outputs for identical calls. </p> <p>We can reduce variation in responses through supplying <code>temperature</code> and <code>seed</code> parameters:</p> <pre><code>llm_client = LLM(api_key=openai_api_key, \n                 temperature=0, \n                 seed=123)\n</code></pre> <p><code>temperature</code> defaults to 0. To increase variation, you may increment its value to something like <code>0.2</code>. </p> <p>The <code>seed</code> parameter works similarly to random seed parameters used for other packages; it acts as a starting point for the model's random number generator during text generation.</p> <p>Warning</p> <p>While we can reduce randomness in LLM responses, these models are non-deterministic and therefore may still produce varying responses even when the <code>seed</code> is set and <code>temperature</code> reduced to <code>0</code>.</p>"},{"location":"llm_advanced/#set-timeout","title":"Set timeout","text":"<p>The <code>timeout</code> parameters dictates the maximum number of seconds the LLM should spend on processing each report. Essentially, if the LLM hasn't provided a response by the specified <code>timeout</code> value, then it gives up and moves on to the next.</p> <pre><code>llm_client = LLM(api_key=openai_api_key, \n                 timeout=60)\n</code></pre>"},{"location":"llm_change_model/","title":"Change the model","text":""},{"location":"llm_change_model/#change-the-model","title":"Change the model","text":"<p>By default, PFD Toolkit uses <code>gpt-4.1</code>. We love this model as it balances cost, speed, and accuracy. We also recommend its smaller equivalent, <code>gpt-4.1-mini</code>, which offers decent performance at a lower API cost.</p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    model=\"gpt-4.1\"     # Set model here\n)\n</code></pre> <p>See OpenAI's documentation for a complete list of their models.</p>"},{"location":"llm_setup/","title":"Setting up an LLM","text":"<p>PFD Toolkit uses a Large Language Model (LLM) client for advanced features. This page explains how to set up your LLM, including how to get an API key.</p>"},{"location":"llm_setup/#setting-up-your-llm-client","title":"Setting up your LLM client","text":"<p>To use AI-powered features, you need to create an LLM client and supply your OpenAI API key (how to get one below). You do not need an LLM client to simply load report data (i.e. using <code>load_reports</code>).</p> <p>Basic setup:</p> <pre><code>from pfd_toolkit import LLM\n\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace YOUR-API-KEY with your real API key\n</code></pre> <p>You can now use LLM-powered features! For example, to screen for reports about medication purchased online:</p> <pre><code>from pfd_toolkit import Screener\n\nquery = \"Deaths that followed ordering medication(s) online.\"\n\nscreener = Screener(llm=llm_client, reports=reports)\nonline_med_reports = screener.screen_reports(search_query=query)\n</code></pre>"},{"location":"llm_setup/#how-do-i-get-an-openai-api-key","title":"How do I get an OpenAI API key?","text":"<ol> <li>Sign up or log in at OpenAI Platform.</li> <li>Go to API Keys.</li> <li>Click \u201cCreate new secret key\u201d and copy the string.</li> <li>Store your key somewhere safe. Never share or publish it.</li> <li>Add credit to your account (just $5 is enough for most research uses).</li> </ol> <p>For more information about usage costs, see OpenAI pricing.</p> <p>Warning</p> <p>OpenAI currently mandate that your account must be at least than 48 hours old before being able to run LLMs as normal. If you're setting up your account for this first time, you might have to wait a couple of days before using PFD Toolkit's advanced features.</p>"},{"location":"llm_setup/#hide-your-api-key","title":"Hide your API key","text":"<p>It's important that you never share your API key. This includes making sure you don't commit any code containing your API to GitHub. </p> <p>The below code shows you how to stealthily import your API key without ever printing it in your code or console:</p> <pre><code>from pfd_toolkit import LLM\nfrom dotenv import load_dotenv\nimport os\n\n# Load OpenAI API key from file\nload_dotenv(\"api.env\")\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialise LLM client\nllm_client = LLM(api_key=openai_api_key)\n</code></pre> <p>This code assumes you've got a file called <code>api.env</code> in the same directory as your script. <code>api.env</code> should look like this:</p> <pre><code>OPENAI_API_KEY = [...]\n</code></pre> <p>where <code>[...]</code> is your API key.</p> <p>Finally, create a file called <code>.gitignore</code> at the front of your directory. Inside it, simply provide the directory of your <code>api.env</code> file:</p> <pre><code>path/to/your/api.env/file\n</code></pre> <p>This tells GitHub not to commit your <code>api.env</code> file, keeping it protected.</p> <p>For more information on <code>.gitignore</code>, see here.</p>"},{"location":"llm_speed/","title":"Speed up your LLM","text":""},{"location":"llm_speed/#speed-up-your-llm","title":"Speed up your LLM","text":"<p>Process more reports in parallel by increasing the <code>max_workers</code> parameter. By default, this is set to <code>8</code>, but larger values can lead to faster run-times.</p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    max_workers=20      # Increase parallelisation\n)\n</code></pre> <p>Note</p> <p>OpenAI enforces rate limits for each account and model. If you set <code>max_workers</code> too high, you may hit these limits and see errors or slowdowns. PFD Toolkit will automatically pause and retry if a rate limit is reached, but it\u2019s best to keep <code>max_workers</code> within a reasonable range (usually 8 to 20 for most users).</p> <p>Your exact rate limit may depend on the 'tier' of your OpenAI account as well as the model you're using. If you need higher limits, you may be able to apply for an increase in your OpenAI account settings.</p>"},{"location":"non_coders/","title":"For Non-Coders","text":"<p>We appreciate that not all researchers are Python coders, and we also believe that this shouldn't stop you from unlocking the insights contained within PFD reports.</p> <p>If you have a research question, policy project, or just curiosity about PFD reports, please get in touch with me (Sam).</p>"},{"location":"non_coders/#how-i-can-help","title":"How I can help","text":"<p>If you want to use the PFD Toolkit but are not sure where to start, or you find the technical side a bit daunting, I am genuinely very happy to help. Here is what I can offer:</p> <ul> <li>Explain PFD reports. I can tell you more about what kind of information is contained within PFD reports and whether they could support your research or policy goals.</li> <li>Custom Scripts. Tell me what you are trying to do, for example, \"I want all reports related to mental health in the North West from 2022,\" or \"Can I get a table of reports mentioning medication errors?\" I can then write the script for you.</li> <li>Ready-to-Use Results. I will run the toolkit for you and provide you with the outputs you need, whether it is a spreadsheet, a summary, or visualisations.</li> <li>Ongoing Support. If you want to learn how to do it yourself next time, I can guide you at your pace.</li> </ul>"},{"location":"non_coders/#how-to-get-in-touch","title":"How to get in touch","text":"<p>Just reach out via email or LinkedIn.</p> <p>I want the toolkit to be useful to as many people as possible, not just programmers. If you are a policy worker, academic, journalist, campaigner, or someone affected by PFD reports, please reach out. I would be delighted to help you get the most out of this resource.</p>"},{"location":"pfd_reports/","title":"More about PFD reports","text":"<p>Prevention of Future Death (PFD) reports are a unique, under-recognised mechanism within the English and Welsh legal system for flagging hazards that threaten lives. These documents, written by coroners at the close of certain inquests, have the potential to drive real change \u2014 but only if their warnings are heard and acted upon.</p>"},{"location":"pfd_reports/#what-is-a-pfd-report","title":"What is a PFD report?","text":"<p>When a coroner concludes an inquest and believes that action should be taken to prevent future deaths, they are legally obliged (under Regulation 28 of the Coroners (Investigations) Regulations 2013) to issue a PFD report. The report is sent to any person or organisation the coroner thinks could take action. This could be an NHS trust, a regulator, a private company, a local council, or even the government.</p> <p>The aim is simple: to prevent further deaths by highlighting risks, missed opportunities, or avoidable harm that have already claimed a life.</p> <p>Other than through this toolkit, reports are publicly available here.</p>"},{"location":"pfd_reports/#what-do-pfd-reports-look-like","title":"What do PFD reports look like?","text":"<p>PFD reports tend to be short, factual documents. PFD Toolkit collects the following sections from each report:</p> PFD report section What it contains URL The URL of the report. ID The ID number associated with the report. Date The date that the report was published. Note that this is not the date of death. Coroner name Identifies the coroner by name. Coroner area States the area of the coroner. Each area typically covers one or more local authorities. Recipient(s) The addressee list \u2013 every person, body or department the coroner believes has the power to act on the concerns. Investigation and inquest Provides the inquest conclusion (medical cause and verdict). Circumstances of the death A concise, factual summary of how the death occurred, setting the scene for the concerns that follow. Coroner's concerns Lists specific matters revealed by the evidence that give rise to a risk of future deaths. <p>Note</p> <p>The above table is a rough guide. In practice, each coroner may approach the writing of PFD reports slightly differently. For example, there is occasional overlap between the \"Investigation and inquest\" and \"Circumstances of the death\" sections.</p>"},{"location":"pfd_reports/#why-do-pfd-reports-matter","title":"Why do PFD reports matter?","text":"<p>PFD reports offer us a rare window into risks and failures that may not appear in routine data. They can expose themes including, but absolutely not limited to:</p> <ul> <li> <p>Missed diagnoses and medical errors</p> </li> <li> <p>Gaps in mental health or social care provision</p> </li> <li> <p>Unsafe systems or environments (e.g. railway safety, housing, road design)</p> </li> <li> <p>Inadequate policies or regulatory oversight</p> </li> </ul> <p>Because coroners have a statutory duty to write them, PFD reports sometimes identify entirely new risks \u2014 before they turn into trends. For researchers and policymakers, they are therefore a critical early-warning system.</p>"},{"location":"pfd_reports/#how-does-pfd-toolkit-help","title":"How does PFD Toolkit help?","text":"<p>Before PFD Toolkit, there was no automated way of screening, discovering themes, or extracting information from these reports. Researchers would have to manually screen them report-by-report, demanding months or even years of researcher time.</p> <p>Through this, we are hoping to lower the barrier to research for those interested in using PFD reports.</p>"},{"location":"extractor/","title":"Analysing PFD reports","text":"<p>PFD Toolkit's <code>Extractor</code> class gives you an analytical toolbox for making sense of PFD reports. </p> <p>The guides below walk through the main features:</p> <ul> <li>Produce summaries of report text \u2013 generate short summaries and estimate the token cost of your data.</li> <li>Discover recurring themes \u2013 automatically discover recurring themes or label reports with your own taxonomy.</li> <li>Pull out structured data from reports \u2013 create a basic feature model to identify specific features from report data.</li> <li>Capture text spans \u2013 keep short excerpts (\"spans\") showing where each feature came from.</li> <li>Cache and reset output \u2013 reuse completions to save time and API costs.</li> </ul>"},{"location":"extractor/basics/","title":"Pull out structured data from reports","text":"<p><code>Extractor</code> lets you turn free text into structured columns. This is useful for running statistics, tracking patterns, and comparing cases at scale.</p> <p>Start by defining a feature model with <code>pydantic</code>. </p> <p>Each feature attribute should contain three pieces of information: the variable name (e.g. <code>\"age\"</code>), its type (e.g. <code>int</code>) and a short description to aid the LLM in extracting the desired information:</p> <pre><code>from pydantic import BaseModel, Field\nfrom pfd_toolkit import load_reports, LLM, Extractor\n\n# Define feature model with pydantic\nclass MyFeatures(BaseModel):\n    age: int = Field(description=\"The age of the deceased\")\n    sex: str = Field(description=\"The sex of the deceased. You may infer sex from pronouns (e.g. 'He', 'Her', etc.)\")\n    cause_of_death: str = Field(description=\"A one-sentence summary of the cause of death\")\n</code></pre> <p>Note</p> <p>As per the example above, your BaseModel instance must specify each of the field <code>types</code>. Use <code>int</code> for numbers, <code>str</code> for strings, and <code>bool</code> for binary values.</p> <p><code>Extractor</code> accepts any valid BaseModel configuration. For more customisation, please read Pydantic's documentation.</p> <p>Next, load some report data and set up your LLM. You then pass the feature model, the reports and the LLM client to an <code>Extractor</code> instance and call <code>extract_features()</code>:</p> <pre><code>reports = load_reports(start_date=\"2024-01-01\", end_date=\"2024-12-31\")\nllm_client = LLM(api_key=YOUR-API-KEY)\n\nextractor = Extractor(\n    reports=reports,\n    llm=llm_client\n)\n\nresult_df = extractor.extract_features(\n        feature_model=MyFeatures,\n        allow_multiple=True, \n        force_assign=False\n)\n</code></pre> <p><code>result_df</code> now contains the new <code>age</code>, <code>sex</code>, and <code>cause_of_death</code> columns. </p> <p>Note</p> <p>Where the model was unable to extract any given piece of structured data, it will output missing data. Setting <code>force_assign</code> to <code>True</code> forces the model to output a value for each feature, even if it cannot be found. </p> <p>In general, this is only recommended if you are working with binary values (of type <code>bool</code>). For example:</p> <pre><code>class MyFeatures(BaseModel):\ncare_home: bool = Field(description=\"Whether or not the death took place in a care home\")\n\nresult_df = extractor.extract_features(\n    feature_model=MyFeatures,\n    allow_multiple=True, \n    force_assign=True  #  &lt;-- force the model to output either `True` or `False`\n)\n</code></pre>"},{"location":"extractor/basics/#choosing-which-sections-the-llm-reads","title":"Choosing which sections the LLM reads","text":"<p><code>Extractor</code> lets you decide exactly which parts of the report are presented to the model. Each <code>include_*</code> flag mirrors one of the columns loaded by <code>load_reports</code>. Turning fields off reduces the amount of text sent to the LLM which often speeds up requests and lowers token usage.</p> <pre><code>extractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n    include_investigation=True,\n    include_circumstances=True,\n    include_concerns=False  # Skip coroner's concerns if not relevant\n)\n</code></pre> <p>In this example only the investigation and circumstances sections are provided to the LLM. The coroner's concerns are omitted entirely. Limiting the excerpt like this may improve accuracy and reduce token costs. However, be careful you're not turning 'off' a report section which could contain information relevant for one of your features.</p>"},{"location":"extractor/basics/#re-run-the-extraction","title":"Re-run the extraction","text":"<p>By default, the <code>Extractor</code> class won't run on the same data with the same configuration twice. </p> <p>If you want to start fresh, call <code>reset()</code> to clear cached feature values, and chain it into a new <code>extract_features()</code> call:</p> <pre><code>clean_df = extractor.reset().extract_features(feature_model=NewModel)\n</code></pre>"},{"location":"extractor/caching/","title":"Caching and resetting output","text":"<p><code>Extractor</code> caches every LLM response so repeated calls with the same configuration reuse previous results.</p> <p>Export the cache before you shut down and import it in a future session to avoid running the model on reports that have already been extracted:</p> <pre><code>extractor.export_cache(\"my_cache.pkl\")\n...\nextractor.import_cache(\"my_cache.pkl\")\n</code></pre> <p>If you want to start fresh, call <code>reset()</code> to clear cached feature values and token estimates. This is useful when you wish to re-run <code>extract_features</code> on the same DataFrame with a different feature model. <code>reset</code> returns the instance so you can immediately chain another call:</p> <pre><code>clean_df = extractor.reset().extract_features(feature_model=NewModel)\n</code></pre> <p>The returned DataFrame contains your newly extracted features and an empty cache ready for further runs.</p>"},{"location":"extractor/spans/","title":"Capturing text spans","text":"<p>Sometimes you want to know exactly which lines from the report led the model to assign a particular value to a given report's field. </p> <p>For example, say we asked the model to identify whether the deceased is a child and the model outputs <code>True</code> for a particular report, we might want to know whether this was because age is explicitly recorded (e.g. \"The deceased was aged 16\") or implied based on context (e.g. \"The deceased was being seen by CAMHS prior to their death\").</p> <p><code>Extractor</code> can add these quotations (or 'spans') automatically. This is...</p> <ul> <li>Great for performance, because we're instructing the model to identify evidence for a feature value before it is assigned, reducing the risk of false positives.</li> <li>Great for human verification, because we can easily verify whether the model's evidence matches its assignment for each report.</li> </ul>"},{"location":"extractor/spans/#include-spans","title":"Include spans","text":"<p><code>.extract_features()</code> accepts a <code>produce_spans</code> flag. When enabled, a new column starting with <code>spans_</code> is created for every feature.</p> <p>For example, in our above example where we extract feature \"child\", a separate column called \"spans_child\" will be created. Each <code>spans_</code> column contains verbatim snippets from the report which justify the extracted value.</p> <pre><code>class ChildID(BaseModel):\n    child: bool = Field(..., description=\"Whether the deceased is a child (under 18)\")\n\nresult = extractor.extract_features(\n    feature_model=ChildID,\n    produce_spans=True,\n)\nresult\n</code></pre> <p>The quotes returned in the spans are kept as short as possible but should always match the original text verbatim. Multiple snippets are separated with semicolons.</p>"},{"location":"extractor/spans/#include-drop-spans","title":"Include &amp; drop spans","text":"<p>If you're not interested in verifying the output, you might want to remove the identified spans from the returned DataFrame after extraction. Set <code>drop_spans=True</code> to remove all <code>spans_</code> columns.</p> <p>As mentioned before, producing but later dropping spans is still likely to improve performance, because you're forcing the model to generate evidence as part of its internal workings out.</p> <pre><code>extractor.extract_features(\n    feature_model=DemoModel,\n    produce_spans=True,\n    drop_spans=True,\n)\n</code></pre>"},{"location":"extractor/summarising/","title":"Produce summaries of report text","text":"<p>Short, consistent summaries make it possible to skim hundreds or even of thousands of reports. You might use these summaries to quickly identify notable case studies, or to get an 'at a glance' understanding of the breadth contained within PFD reports.</p> <p>Producing summaries also paves the way for automated theme discovery (see Discover recurring themes). </p> <p>Use <code>summarise()</code> to condense each report into a short text snippet. The <code>trim_intensity</code> option controls how terse the summary should be. Calling <code>summarise</code> adds a <code>summary</code> column to your stored reports and keeps a copy on the instance under <code>extractor.summarised_reports</code> for later reuse.</p>"},{"location":"extractor/summarising/#getting-started","title":"Getting started","text":"<p>You'll likely wish to screen/filter reports with <code>Screener</code> before generating summaries. For example:</p> <pre><code>from pfd_toolkit import load_reports, LLM, Screener\n\n# Load reports\nreports = load_reports()\n\n# Set up your LLM client\nllm_client = LLM(api_key=YOUR-API-KEY)\n\n# Screen reports by search query\nsearch_query = \"Deaths in police custody **only**.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports\n)\n\npolice_df = screener.screen_reports(\n    search_query=search_query)\n</code></pre> <p>Note</p> <p>For more information on filtering reports, see Filter reports with a query.</p> <p>Following this, we can generate summaries of our screened/filtered reports:</p> <pre><code>from pfd_toolkit import Extractor\n\n# Set up Extractor\nextractor = Extractor(\n    llm=llm_client,\n    reports=reports\n)\n\nsummary_df = extractor.summarise(trim_intensity=\"medium\")\n</code></pre> <p>The resulting DataFrame contains a new column (default name <code>summary</code>). </p> <p>You can specify a different column name via <code>result_col_name</code> if desired. You can also set a different <code>trim_intensity</code> (options range from <code>low</code> to <code>very high</code>) if desired.</p>"},{"location":"extractor/summarising/#specify-which-sections-to-summarise","title":"Specify which sections to summarise","text":"<p>By default, the <code>summarise()</code> method will trim the Investigation, Circumstances of Death and Coroner's Concerns sections. You can override this by setting the <code>include_*</code> flags. For example:</p> <pre><code># Set up Extractor\nextractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n\n    # Decide which sections to include:\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=True\n)\n\nsummary_df = extractor.summarise(trim_intensity=\"medium\")\n</code></pre>"},{"location":"extractor/summarising/#all-options-and-defaults","title":"All options and defaults","text":"Flag Report section What it's useful for Default <code>include_coroner</code> Coroner\u2019s name Simply the name of the coroner. <code>False</code> <code>include_area</code> Coroner\u2019s area The local area the coroner operates within. <code>False</code> <code>include_receiver</code> Receiver(s) of the report The recipient(s) of the reports. <code>False</code> <code>include_investigation</code> \u201cInvestigation &amp; Inquest\u201d section Contains procedural detail about the inquest. <code>True</code> <code>include_circumstances</code> \u201cCircumstances of Death\u201d section Describes what actually happened; holds key facts about the death. <code>True</code> <code>include_concerns</code> \u201cCoroner\u2019s Concerns\u201d section Lists the issues the coroner believes should be addressed. <code>True</code>"},{"location":"extractor/summarising/#estimating-token-counts","title":"Estimating token counts","text":"<p>Token usage is important when working with paid APIs. The <code>estimate_tokens()</code> helper provides a quick approximation of how many tokens a text column will consume.</p> <pre><code>total = extractor.estimate_tokens()\nprint(f\"Total tokens in summaries: {total}\")\n</code></pre> <p><code>estimate_tokens</code> defaults to the summary column, but you can pass any text series via <code>col_name</code>. Set <code>return_series=True</code> to get a per-row estimate instead of the total.</p>"},{"location":"extractor/themes/","title":"Searching for themes","text":"<p>Spotting common themes across many reports helps reveal systemic problems and policy gaps. <code>Extractor</code> can be used to identify &amp; label these themes easily.</p> <p>Important</p> <p>Extracting themes works best if you've already screened for reports that are relevant to your research. For more information, see the guide on search for matching cases.</p>"},{"location":"extractor/themes/#discovering-themes","title":"Discovering themes","text":"<p>The <code>discover_themes()</code> method allows you to identify recurring topics contained within a selection of PFD reports. </p> <p>Once summaries are available, you can instruct the LLM to identify a list of recurring themes. This method expects that the <code>summary</code> column has already been created by <code>summarise()</code> (see Produce summaries of report text).</p> <pre><code>from pfd_toolkit import Extractor, LLM\n\n# Set up Extractor\nextractor = Extractor(\n    llm=llm_client,\n    reports=reports\n)\n\nsummary_df = extractor.summarise(trim_intensity=\"medium\")\n\nIdentifiedThemes = extractor.discover_themes()\n\n# Optionally, inspect the themes that the model has identified:\n#print(extractor.identified_themes)\n</code></pre> <p><code>IdentifiedThemes</code> is essentially a set of detailed instructions that you can pass to the LLM via <code>extract_features()</code>:</p> <pre><code>assigned_reports = extractor.extract_features(\n                              feature_model=IdentifiedThemes,\n                              force_assign=True,\n                              allow_multiple=True)\n</code></pre> <p><code>assigned_reports</code> now contains your original dataset, along with new fields denoting whether the LLM assigned each report to a particular theme or not.  </p>"},{"location":"extractor/themes/#tabulate-themes","title":"Tabulate themes","text":"<p>To create a table containing counts and percentages for each of your themes, run:</p> <pre><code>extractor.tabulate()\n</code></pre>"},{"location":"extractor/themes/#more-customisation","title":"More customisation","text":"<p><code>Extractor</code> contains a suite of options to help you customise the thematic discovery process.</p>"},{"location":"extractor/themes/#choosing-which-sections-the-llm-reads","title":"Choosing which sections the LLM reads","text":"<p><code>Extractor</code> lets you decide exactly which parts of the report are presented to the model. Each <code>include_*</code> flag mirrors one of the columns loaded by <code>load_reports</code>. Turning fields off reduces the amount of text sent to the LLM which often speeds up requests and lowers token usage.</p> <pre><code>extractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n    include_investigation=True,\n    include_circumstances=True,\n    include_concerns=False  # Skip coroner's concerns if not relevant\n)\n</code></pre>"},{"location":"extractor/themes/#guided-topic-modelling","title":"Guided topic modelling","text":"<p>Guided topic modelling is a strategy of discovering themes where you provide a number of topics that are sure to be in your selection of reports. </p> <p>We can set one or more <code>seed_topics</code>, which the model will draw from while also discovering new themes. For example:</p> <pre><code># Set up Extractor\nextractor = Extractor(\n    llm=llm_client,\n    reports=reports\n)\n\nsummary_df = extractor.summarise(trim_intensity=\"medium\")\n\nIdentifiedThemes = extractor.discover_themes(\n    seed_topics=\"Risk assessment failures; understaffing; information sharing failures\"\n)\n</code></pre> <p>Above, we provide 3 seed topics. The model will attempt to identify these topics in the text, while also searching for other, unspecified topics.</p>"},{"location":"extractor/themes/#providing-additional-instructions","title":"Providing additional instructions","text":"<p>You can also provide additional instructions to help guide the model. This is somewhat similar to the guided topic modelling above, except instead of providing examples of themes, we can provide other kinds of guidance. For example:</p> <pre><code>summary_df = extractor.summarise(trim_intensity=\"medium\")\n\nextra_instructions=\"\"\"\nMy research question is: What are the various consequences of transitioning from youth to adult mental health services?\"\n\"\"\"\n\nIdentifiedThemes = extractor.discover_themes(\n    extra_instructions=extra_instructions\n)\n</code></pre> <p>Above, we guide the model by specifying our specific area of interest. This will help to keep themes focused around our core research question. </p>"},{"location":"extractor/themes/#controlling-the-number-of-themes","title":"Controlling the number of themes","text":"<p>You can control how many themes the model discovers through <code>min_themes</code> and <code>max_themes</code> arguments:</p> <pre><code>summary_df = extractor.summarise(trim_intensity=\"medium\")\n\nIdentifiedThemes = extractor.discover_themes(\n    min_theme=8,\n    max_theme=12\n)\n</code></pre> <p><code>discover_themes</code> will now produce at least 8 themes, but not more than 12, themes.</p>"},{"location":"extractor/themes/#manual-topic-modelling","title":"Manual topic modelling","text":"<p>Finally, you can bypass <code>discover_themes()</code> altogether by providing a complete set of themes to extract via a feature model. Here, the model only assigns the themes; it does not identify the themes.</p> <p>For each of your themes, you should provide 3 pieces of information: the column name (e.g. <code>falls_in_custody</code>), its type (e.g. <code>bool</code>) and a brief description.</p> <p>Set <code>force_assign=True</code> so the LLM always returns either <code>True</code> or <code>False</code> for each field. <code>allow_multiple=True</code> lets a single report be marked with more than one theme if required.</p> <pre><code>from pydantic import BaseModel, Field\n\n# For themes, we recommend always setting the type to `bool`\nclass Themes(BaseModel):\n    falls_in_custody: bool = Field(description=\"Death occurred in police custody\")\n    medication_error: bool = Field(description=\"Issues with medication or dosing\")\n\nextractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n)\n\nlabelled = extractor.extract_features(\n    feature_model=Themes,\n    force_assign=True,\n    allow_multiple=True)\n</code></pre> <p>Note</p> <p>Tip: always select type <code>bool</code> for your themes for more reliable performance.</p> <p>The returned DataFrame includes a boolean column for each of your themes.</p>"},{"location":"getting_started/basic_tutorial/","title":"Tutorial: Detention under the Mental Health Act","text":"<p>This page talks you through an example workflow using PFD Toolkit. Here, we will load a dataset and screen for relevant cases related to \"detention under the Mental Health Act\" (often referring to as 'being sectioned'). </p> <p>We will also discover themes to understand more about the issues coroners keep raising.</p> <p>This is just an example. PFD reports contain a breadth of information across a whole range of topics and domains. Through this workflow, we hope to give you a sense of how the toolkit can be used, and how it might support your own project.</p>"},{"location":"getting_started/basic_tutorial/#load-your-first-dataset","title":"Load your first dataset","text":"<p>First, you'll need to load a PFD dataset. As this is just an example workflow, we'll only load reports between January 2024 and May 2025.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load all PFD reports from January 2024 to May 2025\nreports = load_reports(\n    start_date=\"2024-01-01\",\n    end_date=\"2025-05-01\")\n\nreports.head(n=5)\n</code></pre> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m..."},{"location":"getting_started/basic_tutorial/#set-up-an-llm-client","title":"Set up an LLM client","text":"<p>Before exploring some of the other features of PFD toolkit, we first need to set up an LLM client. This is the AI engine that powers all other features of the toolkit.</p> <p>You'll need to head to platform.openai.com and create an API key. Once you've got this, simply feed it to the <code>LLM</code>.</p> <pre><code>from pfd_toolkit import LLM\n\n# Set up LLM client\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace with actual API key\n</code></pre> <p>Note</p> <p>For a more detailed guide on using LLMs in this toolkit, see Setting up an LLM.</p>"},{"location":"getting_started/basic_tutorial/#screen-for-relevant-reports","title":"Screen for relevant reports","text":"<p>You're likely using PFD Toolkit because you want to answer a specific question. In our example, we're asking: \"Do any PFD reports raise concerns related to detention under the Mental Health Act?\"</p> <p>PFD Toolkit lets you query reports in plain English \u2014 no need to know precise keywords or categories. Just describe the cases you care about, and the toolkit will return matching reports.</p> <pre><code>from pfd_toolkit import Screener\n\n# Create a search query to screen/filter reports by\nsearch_query = \"Concerns about detention under the Mental Health Act **only**\"\n\n# Set up &amp; run our Screener\nscreener = Screener(llm = llm_client, # LLM client you set up above\n                        reports = reports) # Reports that you loaded earlier\n\nfiltered_reports = screener.screen_reports(\n    search_query=search_query)\n\n# Optionally, count number of identified reports\nlen(filtered_reports)\n</code></pre> <pre><code>&gt;&gt; 51\n</code></pre> <p><code>filtered_reports</code> returns a filtered version of our original PFD <code>DataFrame</code>, containing the 51 reports the LLM believed matches our query. </p> <p>Note</p> <p>For more information on filtering reports, see Filter reports with a query.</p>"},{"location":"getting_started/basic_tutorial/#discover-recurring-themes","title":"Discover recurring themes","text":"<p>Now that we've loaded and screened our reports for relevance to being detained under the Mental Health Act, our next step is to discover recurring themes. In other words, concerns that coroners keep raising.</p>"},{"location":"getting_started/basic_tutorial/#set-up-the-extractor","title":"Set up the Extractor","text":"<p>Before we get the model to generate a list of themes for us, we first need to set up our <code>Extractor</code>. This class dictates how the model interacts with your filtered list of PFD reports.</p> <p>Each <code>include_*</code> flag controls whether a specific section of the report are sent to the LLM for analysis. </p> <p>For example, if we were only interested in patterns related to coroner's concerns, we would set the <code>include_concerns</code> flag to <code>True</code>:</p> <pre><code>from pfd_toolkit import Extractor\n\nextractor = Extractor(\n    llm=llm_client,             # The same client you created earlier\n    reports=filtered_reports,   # Your screened reports\n\n    include_date=False,\n    include_coroner=False,\n    include_area=False,\n    include_receiver=False,\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=True       # &lt;--- Only supply the 'concerns' text\n)\n</code></pre> <p>Note</p> <p>The main reason why we're hiding all reports sections other than the coroners' concerns is to help keep the LLM's instructions short &amp; focused. LLMs often perform better when they are given only relevant information.</p> <p>Your own research question might be different. For example, you might be interested in discovering recurring themes related to 'cause of death', in which case you'll likely want to set <code>include_investigation</code> and <code>include_circumstances</code> to <code>True</code>.</p> <p>To understand more about what information is contained within each of the report sections, please see About the data.</p>"},{"location":"getting_started/basic_tutorial/#summarise-reports","title":"Summarise reports","text":"<p>Some PFD reports can be long. Because of this, we need to summarise reports before we discover themes:</p> <pre><code># Create short summaries of the concerns\nextractor.summarise(trim_intensity=\"medium\")\n</code></pre>"},{"location":"getting_started/basic_tutorial/#get-a-list-of-themes","title":"Get a list of themes","text":"<p>Now that we've done this, we can run the <code>discover_themes</code> method and assign the result to a new class, which we've named <code>ThemeInstructions</code>:</p> <pre><code># Ask the LLM to propose recurring themes\nThemeInstructions = extractor.discover_themes(\n    max_themes=6,  # Limit the list to keep things manageable\n)\n</code></pre> <p>Note</p> <p><code>discover_themes()</code> will warn you if the word count of your summaries is still too high. In these cases, you might want to set your <code>trim_intensity</code> to <code>high</code> or <code>very high</code> (though please note that the more we trim, the more detail we lose).</p> <p>To print our list of themes, run:</p> <pre><code>print(extractor.identified_themes)\n</code></pre> <p>...which gives us:</p> <pre><code>{\n  \"bed_shortage\": \"Insufficient availability of inpatient mental health beds or suitable placements, leading to delays, inappropriate care environments, or patients being placed far from home.\",\n\n  \"staff_training\": \"Inadequate staff training, knowledge, or awareness regarding policies, risk assessment, clinical procedures, or the Mental Health Act.\",\n\n  \"record_keeping\": \"Poor, inconsistent, or falsified documentation and record keeping, including failures in care planning, observation records, and communication of key information.\",\n\n  \"policy_gap\": \"Absence, inconsistency, or lack of clarity in policies, protocols, or guidance, resulting in confusion or unsafe practices.\",\n\n  \"communication_failures\": \"Breakdowns in communication or information sharing between staff, agencies, families, or across systems, impacting patient safety and care continuity.\",\n\n  \"risk_assessment\": \"Failures or omissions in risk assessment, escalation, or monitoring, including inadequate recognition of suicide risk, self-harm, or other patient safety concerns.\"\n}\n</code></pre>"},{"location":"getting_started/basic_tutorial/#tag-the-reports-with-our-themes","title":"Tag the reports with our themes","text":"<p>Above, we've only identified a list of themes: we haven't yet assigned these themes to each of our reports.</p> <p>Here, we take <code>ThemeInstructions</code> that we created earlier and pass it back into the extractor to assign themes to reports via <code>extract_features()</code>:</p> <pre><code>labelled_reports = extractor.extract_features(\n    feature_model=ThemeInstructions,\n    force_assign=True,  # (Force the model to make a decision)\n    allow_multiple=True  # (A single report might touch on several themes)\n)\n\nlabelled_reports.head()\n</code></pre> <p>The resulting <code>DataFrame</code> now contains our existing columns along with a suite of new ones: each filled with either <code>True</code> or <code>False</code>, depending on whether the theme was present.</p> url id date coroner area receiver investigation circumstances concerns bed_shortage staff_training record_keeping policy_gap communication_failures risk_assessment [\u2026] 2025-0172 2025-04-07 S. Reeves South London South London and Maudsley NHS \u2026 On 21 March 2023 an inquest \u2026 Christopher McDonald was \u2026 The evidence heard \u2026 False True False False False True [\u2026] 2025-0144 2025-03-17 S. Horstead Essex Chief Executive Officer of Essex \u2026 On 31 October 2023 I \u2026 On the 23rd September 2023 \u2026 (a) Failures in care \u2026 False False True False True True [\u2026] 2025-0104 2025-03-13 A. Harris South London Oxleas NHS Foundation Trust; \u2026 On 15 January 2020 an \u2026 Mr Paul Dunne had a \u2026 Individual mental health \u2026 False True True True True True [\u2026] 2025-0124 2025-03-06 D. Henry Coventry Chair of the Coventry and \u2026 On 13 August 2021 I \u2026 Mr Gebrsselasi\u00e9 on the 2nd \u2026 The inquest explored issues \u2026 False False False True False True [\u2026] 2025-0119 2025-03-04 L. Hunt Birmingham and Solihull Birmingham and Solihull Mental \u2026 On 20 July 2023 I \u2026 Mr Lynch resided in room 1 \u2026 To Birmingham and Solihull \u2026 False True True True True True"},{"location":"getting_started/basic_tutorial/#tabulate-reports","title":"Tabulate reports","text":"<p>Finally, we can count how often a theme appears in our collection of reports:</p> <pre><code>extractor.tabulate()\n</code></pre> Category Count Percentage bed_shortage 14 27.5 staff_training 22 43.1 record_keeping 13 25.5 policy_gap 35 68.6 communication_failures 19 37.3 risk_assessment 34 66.7 <p>That's it! You've gone from a mass of PFD reports, to a focused set of cases relating to being detained under the Mental Health Act to a theme\u2011tagged dataset ready for deeper exploration.</p> <p>From here, you might want to export your curated dataset to a .csv for any final qualitative/manual analysis:</p> <pre><code>labelled_reports.to_csv()\n</code></pre> <p>Alternatively, you might want to check out the other analytical features that PFD Toolkit offers.</p> <p>For a deeper dive into each capability, head over to the Explore section.</p>"},{"location":"getting_started/key_features/","title":"Key features","text":"<p>PFD Toolkit turns raw PFD reports into structured insights. This page provides a short tour of the core features so you can hit the ground running. Each section links to more detailed guidance.</p> <p>This is only an overview \u2014 for full walkthroughs start with the Load live report data guide in the Explore section.</p>"},{"location":"getting_started/key_features/#load-live-report-data","title":"Load live report data","text":"<p>Access the latest PFD reports in a single line. <code>load_reports()</code> returns a <code>DataFrame</code> containing the main sections from each report.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load reports from January 2024 onwards\nreports = load_reports(\n    start_date=\"2024-01-01\" # YYYY-MM-DD format\n)\n</code></pre> <p>Learn more on the loading report data page.</p>"},{"location":"getting_started/key_features/#set-up-an-llm-client","title":"Set up an LLM client","text":"<p>Most features rely on an OpenAI model. Create an <code>LLM</code> client and supply your API key:</p> <pre><code>from pfd_toolkit import LLM\n\nllm_client = LLM(api_key=YOUR_API_KEY)\n</code></pre> <p>For more information on LLM setup (including how to get an OpenAI API key) see Setting up an LLM.</p> <p>Warning</p> <p>OpenAI currently mandate that your account must be at least than 48 hours old before being able to run LLMs as normal. If you're setting up your account for this first time, you might have to wait a couple of days before using PFD Toolkit's advanced features.</p>"},{"location":"getting_started/key_features/#hiding-your-api-key","title":"Hiding your API key","text":"<p>We heavily recommend keeping your API key safe by hiding it from your script. Create a separate file called <code>api.env</code> which contains:</p> <pre><code>OPENAI_API_KEY = [copy &amp; paste your API key here]\n</code></pre> <p>...and import it via:</p> <pre><code>from pfd_toolkit import LLM\nfrom dotenv import load_dotenv\nimport os\n\n# Load OpenAI API key\nload_dotenv(\"api.env\")\nopenai_api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# Initialise LLM client\nllm_client = LLM(api_key=openai_api_key)\n</code></pre>"},{"location":"getting_started/key_features/#screen-reports-with-natural-language","title":"Screen reports with natural language","text":"<p>Use <code>Screener</code> to filter thousands of reports according to a plain\u2011English query. For example, if you wanted to filter reports concerning deaths in police custody, you could run:</p> <pre><code>from pfd_toolkit import Screener\n\nquery = \"Deaths in police custody **only**.\"\n\nscreener = Screener(llm=llm_client, reports=reports)\npolice_reports = screener.screen_reports(search_query=query)\n</code></pre> <p><code>police_reports</code> only contains reports that the LLM believed matched your query.</p> <p>See Filter reports with a query for further tips and options.</p>"},{"location":"getting_started/key_features/#create-concise-summaries","title":"Create concise summaries","text":"<p>Long reports can be trimmed into short summaries for quick review. The <code>Extractor</code> class handles this via <code>summarise()</code>.</p> <pre><code>from pfd_toolkit import Extractor\n\nextractor = Extractor(llm=llm_client, reports=police_reports)\nsummary_df = extractor.summarise()\n</code></pre> <p>Read more on producing summaries.</p>"},{"location":"getting_started/key_features/#discover-recurring-themes","title":"Discover recurring themes","text":"<p>Once summaries exist, you can instruct the LLM to find common topics across your dataset.</p> <pre><code>IdentifiedThemes = extractor.discover_themes()\n\n# Optionally, print the identified themes + description\n#print(extractor.identified_themes)\n</code></pre> <p>The returned model can then be used to label each report:</p> <pre><code>from pfd_toolkit import Extractor\n\nextractor = Extractor(llm=llm_client, reports=police_reports)\n\nlabelled = extractor.extract_features(\n    feature_model=IdentifiedThemes,\n    force_assign=True,\n    allow_multiple=True,\n)\n</code></pre> <p>Detailed guidance is available under discover recurring themes.</p>"},{"location":"getting_started/key_features/#extract-structured-features","title":"Extract structured features","text":"<p>Finally, define your own variables with <code>pydantic</code> and pull them from the text.</p> <pre><code>from pfd_toolkit import Extractor\nfrom pydantic import BaseModel, Field\n\nextractor = Extractor(llm=llm_client, reports=police_reports)\n\nclass MyFeatures(BaseModel):\n    age: int = Field(description=\"Age of the deceased, if provided\")\n    sex: str = Field(description=\"Sex of the deceased (you can infer from pronouns)\")\n\nfeatures = extractor.extract_features(\n    feature_model=MyFeatures,\n    allow_multiple=True,\n)\n</code></pre> <p><code>features</code> now contains <code>age</code> and <code>sex</code> columns.</p> <p>See pull out structured data for advanced usage and configuration.</p> <p>With these six steps \u2014 loading data, creating an LLM client, screening reports, summarising, discovering themes and extracting features \u2014 you can transform months of manual work into a streamlined workflow.</p>"},{"location":"loader/cleaner/","title":"Cleaning scraped data","text":"<p><code>Cleaner</code> provides an optional step for polishing scraped reports with the same LLM used in the fallback stage. It loops over selected columns, sends field specific prompts in batches and writes the corrected text back into a copy of your DataFrame.</p>"},{"location":"loader/cleaner/#basic-usage","title":"Basic usage","text":"<pre><code>from pfd_toolkit import Cleaner\n\ncleaner = Cleaner(df, llm, include_receiver=False)\nclean_df = cleaner.clean_reports(anonymise=True)\n</code></pre> <p>The class works field by field so you decide which columns to modify. Use the boolean flags like <code>include_receiver</code> or <code>include_area</code> to toggle each field. Custom prompt strings allow advanced users to steer how the LLM rewrites the text.</p>"},{"location":"loader/cleaner/#anonymising-sensitive-details","title":"Anonymising sensitive details","text":"<p>When <code>anonymise=True</code> the investigation, circumstances and concerns fields are automatically de\u2011identified by swapping names and pronouns for gender\u2011neutral placeholders.</p> <p>The underlying logic constructs a field-specific prompt for every text snippet, sends them to the LLM in batches and writes the results back into a copy of your DataFrame.</p> <p>Any prompts that return an error marker are ignored so the original text remains untouched. Call <code>generate_prompt_template()</code> to preview the finalised prompts before you run the clean.</p>"},{"location":"loader/cleaner/#prompt-templates-and-error-handling","title":"Prompt templates and error handling","text":"<p><code>Cleaner</code> exposes the final prompt for each field via <code>generate_prompt_template()</code>. This lets you inspect or tweak the wording before sending anything to the model. If the LLM returns a recognised error marker, the class reverts to the original text rather than introducing blank values.</p> <p>See the API reference for a detailed breakdown of all arguments and attributes.</p>"},{"location":"loader/load_reports/","title":"Load live report data","text":""},{"location":"loader/load_reports/#load-live-report-data","title":"Load live report data","text":"<p><code>load_reports()</code> is the quickest way to access PFD reports.  While before, researchers would have to manually download reports one-by-one, this function allows users to immediately access all reports.</p> <p>The function returns a pandas <code>DataFrame</code>, with each row representing an individual report and the columns representing the main report sections.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load all PFD reports from January 2024 to May 2025\nreports = load_reports(\n    start_date=\"2024-01-01\", # YYYY-MM-DD format\n    end_date=\"2025-05-01\")\n\nreports.head()\n</code></pre> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m... <p>If you don't pass <code>start_date</code> or <code>end_date</code> parameters, <code>load_reports()</code> will pull the entire collection of PFD reports.</p> <p>Note</p> <p>Please note that the date ranges denote when the report was published, not the date of death.</p>"},{"location":"loader/load_reports/#get-n-latest-reports","title":"Get n latest reports","text":"<p>Optionally, use <code>n_reports</code> to trim the DataFrame to the most recent n entries. For example...</p> <pre><code>reports = load_reports(\n    n_reports=1000)\n</code></pre> <p>...loads the 1000 latest reports.</p> <p>You can combine this with the date parameters to get the most recent n entries within a given date range.</p>"},{"location":"loader/load_reports/#refresh-reports","title":"Refresh reports","text":"<p>Reports are updated once a week (Monday 1:00am, universal time). <code>load_reports()</code> caches reports for faster loading, so to retrieve the latest reports you may need to set <code>refresh</code> to <code>True</code>:</p> <pre><code>reports = load_reports(refresh=True)\n</code></pre> <p>Note</p> <p>The dataset loaded when you call <code>load_reports()</code> is cleaned and fully processed. This means spelling and grammatical errors have been corrected and boilerplate text removed.</p> <p>If you wish to load an uncleaned version of the dataset, we suggest running your own scrape via <code>Scraper</code>.</p>"},{"location":"loader/scraper/","title":"Scraping module","text":"<p><code>Scraper</code> lets you download PFD reports straight from the judiciary website and control each step of the extraction process. For most projects <code>load_reports()</code> is sufficient, but the scraping module gives you full transparency over how reports are gathered and how missing values are filled in. Use it when you need to customise request behaviour, adjust fallback logic or troubleshoot tricky reports.</p>"},{"location":"loader/scraper/#why-run-a-custom-scrape","title":"Why run a custom scrape?","text":"<p>The weekly datasets cover the majority of use cases. However there are two scenarios when direct scraping may be preferable:</p> <ul> <li>Rapid updates \u2013 the PFD Toolkit dataset lags up to a week behind new publications. Running your own scrape means you can see the newest reports immediately.</li> <li>Custom logic \u2013 while the bundled dataset is a product of Vision-LLM scraping, you may also wish to enable HTML and .pdf scraping.</li> </ul>"},{"location":"loader/scraper/#creating-a-scraper","title":"Creating a scraper","text":"<pre><code>from pfd_toolkit import Scraper\n\nscraper = Scraper(\n    category=\"suicide\",           # judiciary.uk slug or \"all\"\n    llm=llm_client,         # assumes you've already set up your LLM client\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\",\n    scraping_strategy=[1, 2, 3],   # html \u2192 pdf \u2192 llm\n    max_workers=10,\n    delay_range=(1, 2),\n)\n</code></pre> <p>Pass in a category slug (or use <code>\"all\"</code>), a date range and any optional settings such as worker count, request delay or timeout. The <code>scraping_strategy</code> list defines which stages run and in what order. Each entry refers to the HTML, PDF and LLM steps respectively \u2013 set an index to <code>-1</code> to skip a step entirely.</p> <p>Note</p> <p>For example, setting <code>scraping_strategy</code> to <code>[2, 1, -1]</code> runs HTML scraping first, .pdf scraping second, and disables Vision-LLM scraping. Setting it to <code>[2, -1, 1]</code> runs Vision-LLM scraping first, HTML scraping second, and disables .pdf scraping. </p> <p>This latter configuration is exactly what PFD Toolkit uses under the hood to construct the dataset you see when you call <code>load_reports()</code>.</p>"},{"location":"loader/scraper/#a-closer-look-at-the-pipeline","title":"A closer look at the pipeline","text":"<ol> <li>HTML scraping collects data directly from the report landing page. This is the fastest approach and usually recovers most metadata fields (e.g. coroner name, area, receiver) but struggles where the HTML make up of a given report differs, even slightly, from the majority of reports.</li> <li>.pdf scraping downloads the report .pdf and extracts text with PyMuPDF. This approach also recovers most fields, but will often scrape page numbers, footnotes and other .pdf 'juice'. It will fail where a report uses a non-standard heading (e.g. uses just \"Concerns\" instead of the more common \"Coroner's concerns\").</li> <li>Vision-LLM scraping is by far the most reliable method, but also the longest. The LLM understands the reports in context, meaning it doesn't matter if a report has unusual formatting or different section headings.</li> </ol> <p>The stages cascade automatically \u2014 if HTML scraping gathers everything you need, the PDF and LLM steps are skipped. You can reorder or disable steps entirely by tweaking <code>scraping_strategy</code>.</p>"},{"location":"loader/scraper/#running-a-scrape","title":"Running a scrape","text":"<p>After initialisation, call <code>scrape_reports()</code> to run the full scrape:</p> <pre><code>df = scraper.scrape_reports()\n</code></pre> <p>The results are cached on <code>scraper.reports</code> as a pandas DataFrame. This cache lets you rerun individual stages without hitting the network again. If more reports are published later you can update the existing DataFrame with <code>top_up()</code>:</p> <pre><code>updated = scraper.top_up(existing_df=df, end_date=\"2025-01-31\", clean=True)\n</code></pre> <p><code>top_up()</code> only fetches new pages, meaning you avoid repeating work and keep the original ordering intact. When <code>clean=True</code> the new and existing rows are passed through <code>Cleaner.clean_reports()</code> for optional LLM-powered tidying.</p>"},{"location":"loader/scraper/#applying-the-llm-fallback-separately","title":"Applying the LLM fallback separately","text":"<p>Sometimes you may want to review scraped results before running the LLM stage. <code>run_llm_fallback()</code> accepts a DataFrame (typically the output of <code>scrape_reports()</code> or <code>top_up()</code>) and attempts to fill any remaining blanks using your configured language model:</p> <pre><code>llm_df = scraper.run_llm_fallback(df)\n</code></pre>"},{"location":"loader/scraper/#cleaning-scraped-data","title":"Cleaning scraped data","text":"<p>To tidy up scraped fields using the same language model, see the dedicated Cleaner page. It explains how to batch correct scraped text, anonymise personal information and fine\u2011tune prompts for each column.</p>"},{"location":"loader/scraper/#threading-and-polite-scraping","title":"Threading and polite scraping","text":"<p><code>Scraper</code> uses a thread pool to speed up network requests. The <code>max_workers</code> and <code>delay_range</code> settings let you tune throughput and avoid overloading the server. The default one\u2013two second delay between requests mirrors human browsing behaviour and greatly reduces the risk of your IP address being flagged.</p>"},{"location":"loader/scraper/#inspecting-results","title":"Inspecting results","text":"<p>Every scrape writes a timestamp column when <code>include_time_stamp=True</code>. This can be useful for auditing your scraping pipeline. </p> <p>All fields that could not be extracted are set to missing values, making gaps explicit in the final dataset.</p>"},{"location":"reference/cleaner/","title":"<code>Cleaner</code>","text":"<p>Batch-clean PFD report fields with an LLM.</p> <p>The cleaner loops over selected columns, builds field-specific prompts and writes the returned text back into a copy of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>Input DataFrame to clean.</p> required <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>include_coroner</code> <code>bool</code> <p>Clean the <code>coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Clean the <code>receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Clean the <code>area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Clean the <code>investigation</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Clean the <code>circumstances</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Clean the <code>concerns</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>coroner_prompt</code> <code>str or None</code> <p>Custom prompt for the coroner field. Defaults to <code>None</code>.</p> <code>None</code> <code>area_prompt</code> <code>str or None</code> <p>Custom prompt for the area field. Defaults to <code>None</code>.</p> <code>None</code> <code>receiver_prompt</code> <code>str or None</code> <p>Custom prompt for the receiver field. Defaults to <code>None</code>.</p> <code>None</code> <code>investigation_prompt</code> <code>str or None</code> <p>Custom prompt for the investigation field. Defaults to <code>None</code>.</p> <code>None</code> <code>circumstances_prompt</code> <code>str or None</code> <p>Custom prompt for the circumstances field. Defaults to <code>None</code>.</p> <code>None</code> <code>concerns_prompt</code> <code>str or None</code> <p>Custom prompt for the concerns field. Defaults to <code>None</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Emit info-level logs for each batch when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>cleaned_reports</code> <code>DataFrame</code> <p>Result of the last call to <code>clean_reports</code>.</p> <code>coroner_prompt_template, area_prompt_template, ...</code> <code>str</code> <p>Finalised prompt strings actually sent to the model.</p> <p>Examples:</p> <pre><code>cleaner = Cleaner(df, llm, include_coroner=False, verbose=True)\ncleaned_df = cleaner.clean_reports()\ncleaned_df.head()\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.clean_reports","title":"clean_reports","text":"<pre><code>clean_reports(anonymise=False)\n</code></pre> <p>Run LLM-based cleaning for the configured columns.</p> <p>The method operates in place on a copy of <code>self.reports</code> so the original DataFrame is never mutated.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which the selected columns have been replaced by the LLM output (or left unchanged when the model returns an error marker).</p> <p>Parameters:</p> Name Type Description Default <code>anonymise</code> <code>bool</code> <p>When <code>True</code> append an instruction to anonymise names and pronouns in the investigation, circumstances and concerns fields. Defaults to <code>False</code>.</p> <code>False</code> <p>Examples:</p> <pre><code>cleaner = Cleaner(llm=llm_client, reports=reports)\ncleaned = cleaner.clean_reports()\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.generate_prompt_template","title":"generate_prompt_template","text":"<pre><code>generate_prompt_template()\n</code></pre> <p>Return the prompt templates used for each field.</p> <p>The returned dictionary maps DataFrame column names to the full prompt text with a <code>[TEXT]</code> placeholder appended to illustrate how the prompt will look during <code>clean_reports</code>.</p>"},{"location":"reference/extractor/","title":"<code>Extractor</code>","text":"<p>Extract custom features from Prevention of Future Death reports using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>reports</code> <code>DataFrame</code> <p>DataFrame of PFD reports. When provided it is copied and stored on the instance. Defaults to <code>None</code>.</p> <code>None</code> <code>include_date</code> <code>bool</code> <p>Include the <code>date</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>coroner</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Include the <code>area</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>receiver</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>investigation</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>circumstances</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>concerns</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Emit extra logging when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.discover_themes","title":"discover_themes","text":"<pre><code>discover_themes(\n    *,\n    warn_exceed=100000,\n    error_exceed=500000,\n    max_themes=None,\n    min_themes=None,\n    extra_instructions=None,\n    seed_topics=None,\n)\n</code></pre> <p>Use an LLM to automatically discover report themes.</p> <p>The method expects <code>summarise</code> to have been run so that a summary column exists. All summaries are concatenated into one prompt sent to the LLM. The LLM should return a JSON object mapping theme names to descriptions. A new <code>pydantic</code> model is built from this mapping and stored as <code>feature_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>warn_exceed</code> <code>int</code> <p>Emit a warning if the estimated token count exceeds this value. Defaults to <code>100000</code>.</p> <code>100000</code> <code>error_exceed</code> <code>int</code> <p>Raise a <code>ValueError</code> if the estimated token count exceeds this value. Defaults to <code>500000</code>.</p> <code>500000</code> <code>max_themes</code> <code>int or None</code> <p>Instruct the LLM to identify no more than this number of themes when provided.</p> <code>None</code> <code>min_themes</code> <code>int or None</code> <p>Instruct the LLM to identify at least this number of themes when provided.</p> <code>None</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions appended to the theme discovery prompt.</p> <code>None</code> <code>seed_topics</code> <code>str | list[str] | BaseModel</code> <p>Optional seed topics to include in the prompt. These are treated as starting suggestions and the model should incorporate them into a broader list of themes.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The generated feature model containing discovered themes.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(col_name=None, return_series=False)\n</code></pre> <p>Estimate token counts for all rows of a given column using the <code>tiktoken</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Name of the column containing report summaries. Defaults to <code>summary_col</code>, which is generated after running <code>summarise</code>.</p> <code>None</code> <code>return_series</code> <code>bool</code> <p>Returns a pandas.Series of per-row token counts for that field if <code>True</code>, or an integer if <code>False</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, Series]</code> <p>If <code>return_series</code> is <code>False</code>, returns an <code>int</code> representing the total sum of all token counts across all rows for the provided field. If <code>return_series</code> is <code>True</code>, returns a <code>pandas.Series</code> of token counts aligned to <code>self.reports</code> for the provided field.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.export_cache","title":"export_cache","text":"<pre><code>export_cache(path='extractor_cache.pkl')\n</code></pre> <p>Save the current cache to <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be created inside it.</p> <code>'extractor_cache.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the written cache file.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(\n    reports=None,\n    *,\n    feature_model=None,\n    produce_spans=False,\n    drop_spans=False,\n    force_assign=False,\n    allow_multiple=False,\n    schema_detail=\"minimal\",\n    extra_instructions=None,\n    skip_if_present=True,\n)\n</code></pre> <p>Run feature extraction for the given reports.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>DataFrame of reports to process. Defaults to the instance's stored reports if omitted.</p> <code>None</code> <code>feature_model</code> <code>type[BaseModel]</code> <p>Pydantic model describing the features to extract. Must be provided on first call or after calling <code>discover_themes</code>.</p> <code>None</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code>, create <code>spans_</code> versions of each feature to capture the supporting text snippets. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, remove any <code>spans_</code> columns created during this call from the returned DataFrame. Spans columns from other sources (e.g. Screener) are preserved. If <code>produce_spans</code> is <code>False</code> a warning is emitted and no columns are dropped. Defaults to <code>False</code>.</p> <code>False</code> <code>force_assign</code> <code>bool</code> <p>When <code>True</code>, the LLM is instructed to avoid returning :data:<code>GeneralConfig.NOT_FOUND_TEXT</code> for any feature.</p> <code>False</code> <code>allow_multiple</code> <code>bool</code> <p>Allow a report to be assigned to multiple categories when <code>True</code>.</p> <code>False</code> <code>schema_detail</code> <code>('full', 'minimal')</code> <p>Level of detail for the feature schema included in the prompt.</p> <code>\"full\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions injected into each prompt before the schema.</p> <code>None</code> <code>skip_if_present</code> <code>bool</code> <p>When <code>True</code> (default), skip rows when any feature column already holds a non-missing value that is not equal to :data:<code>GeneralConfig.NOT_FOUND_TEXT</code>. This assumes the row has been processed previously and is logged in an instance of <code>Extractor.cache</code></p> <code>True</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.import_cache","title":"import_cache","text":"<pre><code>import_cache(path='extractor_cache.pkl')\n</code></pre> <p>Load cache from <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be loaded from inside it.</p> <code>'extractor_cache.pkl'</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset internal caches and intermediate state.</p> <p>This clears any cached feature extraction results and token estimations so that <code>extract_features</code> can be run again on the same reports. The instance itself is returned to allow method chaining, e.g. <code>extractor.reset().extract_features()</code>.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.summarise","title":"summarise","text":"<pre><code>summarise(\n    result_col_name=\"summary\",\n    trim_intensity=\"medium\",\n    extra_instructions=None,\n)\n</code></pre> <p>Summarise selected report fields into one column using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>result_col_name</code> <code>str</code> <p>Name of the summary column. Defaults to <code>\"summary\"</code>.</p> <code>'summary'</code> <code>trim_intensity</code> <code>('low', 'medium', 'high', 'very high')</code> <p>Controls how concise the summary should be. Defaults to <code>\"medium\"</code>.</p> <code>\"low\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions to append to the prompt before the report excerpt.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame identical to the one provided at initialisation with an extra summary column.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.tabulate","title":"tabulate","text":"<pre><code>tabulate(\n    columns=None,\n    labels=None,\n    *,\n    count_col=\"Count\",\n    pct_col=\"Percentage\",\n    df=None,\n)\n</code></pre> <p>Return a simple frequency table for extracted feature columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str or list[str]</code> <p>Column name or list of column names to summarise. Defaults to all feature columns added by :meth:<code>extract_features</code> (excluding any <code>spans_</code> columns).</p> <code>None</code> <code>labels</code> <code>str or list[str]</code> <p>Human friendly label or list of labels corresponding to <code>columns</code>. If omitted, column names are used.</p> <code>None</code> <code>count_col</code> <code>str</code> <p>Column names for the count and percentage values in the output DataFrame. Defaults to <code>\"Count\"</code> and <code>\"Percentage\"</code>.</p> <code>'Count'</code> <code>pct_col</code> <code>str</code> <p>Column names for the count and percentage values in the output DataFrame. Defaults to <code>\"Count\"</code> and <code>\"Percentage\"</code>.</p> <code>'Count'</code> <code>df</code> <code>DataFrame</code> <p>DataFrame containing the columns to tabulate. Defaults to the reports stored on the instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame summarising the frequencies of the specified columns.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If :meth:<code>extract_features</code> has not been run yet.</p>"},{"location":"reference/llm/","title":"<code>LLM</code>","text":"<p>Wrapper around the OpenAI Python SDK for batch prompting.</p> <p>The helper provides:</p> <ul> <li><code>generate</code> for plain or vision-enabled prompts with optional pydantic   validation.</li> <li><code>_call_llm_fallback</code> used by the scraper when HTML and PDF heuristics   fail.</li> <li>Built-in back-off and host-wide throttling via a semaphore.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI (or proxy) API key. Defaults to <code>None</code> which expects the environment variable to be set.</p> <code>None</code> <code>model</code> <code>str</code> <p>Chat model name. Defaults to <code>\"gpt-4.1\"</code>.</p> <code>'gpt-4.1'</code> <code>base_url</code> <code>str or None</code> <p>Override the OpenAI endpoint. Defaults to <code>None</code>.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum parallel workers for batch calls and for the global semaphore. Defaults to <code>8</code>.</p> <code>8</code> <code>temperature</code> <code>float</code> <p>Sampling temperature used for all requests. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>seed</code> <code>int or None</code> <p>Deterministic seed value passed to the API. Defaults to <code>None</code>.</p> <code>None</code> <code>validation_attempts</code> <code>int</code> <p>Number of times to retry parsing LLM output into a pydantic model. Defaults to <code>2</code>.</p> <code>2</code> <code>timeout</code> <code>float | Timeout | None</code> <p>Override the HTTP timeout in seconds. <code>None</code> uses the OpenAI client default of 600 seconds.</p> <code>120</code> <p>Attributes:</p> Name Type Description <code>_sem</code> <code>Semaphore</code> <p>Global semaphore that limits concurrent requests to max_workers.</p> <code>client</code> <code>Client</code> <p>Low-level SDK client configured with key and base URL.</p> <p>Examples:</p> <pre><code>llm_client = LLM(api_key=\"sk-...\", model=\"gpt-4o-mini\", temperature=0.2,\n          timeout=600)\n</code></pre>"},{"location":"reference/llm/#pfd_toolkit.LLM.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(texts, model=None)\n</code></pre> <p>Return token counts for text using <code>tiktoken</code>.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str] | str</code> <p>Input strings to tokenise.</p> required <code>model</code> <code>str</code> <p>Model name for selecting the encoding. Defaults to <code>self.model</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>Token counts in the same order as <code>texts</code>.</p>"},{"location":"reference/llm/#pfd_toolkit.LLM.generate","title":"generate","text":"<pre><code>generate(\n    prompts,\n    images_list=None,\n    response_format=None,\n    max_workers=None,\n    tqdm_extra_kwargs=None,\n)\n</code></pre> <p>Run many prompts either sequentially or in parallel.</p> <pre><code>    Parameters:\n</code></pre> <pre><code>    prompts : list[str]\n        List of user prompts. One prompt per model call.\n\n    images_list : list[list[bytes]] or None, optional\n        For vision models: a parallel list where each inner list\n        holds **base64-encoded** JPEG pages for that prompt.  Use\n        *None* to send no images.\n\n    response_format : type[pydantic.BaseModel] or None, optional\n        If provided, each response is parsed into that model via the\n        *beta/parse* endpoint; otherwise a raw string is returned.\n\n    max_workers : int or None, optional\n        Thread count just for this batch. ``None`` uses the instance-wide\n        ``max_workers`` value. Defaults to ``None``.\n</code></pre> <pre><code>    Returns:\n</code></pre> <pre><code>    list[Union[pydantic.BaseModel, str]]\n        Results in the same order as `prompts`.\n</code></pre> <pre><code>    Raises:\n</code></pre> <pre><code>    openai.RateLimitError\n        Raised only if the exponential back-off exhausts all retries.\n    openai.APIConnectionError\n        Raised if network issues persist beyond the retry window.\n    openai.APITimeoutError\n        Raised if the API repeatedly times out.\n</code></pre> <pre><code>    Examples:\n</code></pre> <pre><code>        msgs = [\"Summarise:\n</code></pre> <p>\" + txt for txt in docs]             summaries = llm.generate(msgs)</p>"},{"location":"reference/loader/","title":"<code>load_reports</code>","text":"<p>Load Prevention of Future Death reports as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2000-01-01\"</code>.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2050-01-01\"</code>.</p> <code>'2050-01-01'</code> <code>n_reports</code> <code>int or None</code> <p>Keep only the most recent <code>n_reports</code> rows after filtering by date. <code>None</code> (the default) returns all rows.</p> <code>None</code> <code>refresh</code> <code>bool</code> <p>If <code>True</code>, force a fresh download of the dataset instead of using the cached copy. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Reports filtered by date, sorted newest first and optionally limited to <code>n_reports</code> rows.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If start_date is after end_date.</p> <code>FileNotFoundError</code> <p>If the dataset cannot be downloaded.</p> <p>Examples:</p> <pre><code>from pfd_toolkit import load_reports\ndf = load_reports(start_date=\"2020-01-01\", end_date=\"2022-12-31\", n_reports=100)\ndf.head()\n</code></pre>"},{"location":"reference/scraper/","title":"<code>Scraper</code>","text":"<p>Scrape UK \u201cPrevention of Future Death\u201d (PFD) reports into a pandas.DataFrame.</p> <p>The extractor runs in three cascading layers (<code>html \u2192 pdf \u2192 llm</code>), each independently switchable.</p> <ol> <li>HTML scrape \u2013 parse metadata and rich sections directly from    the web page.</li> <li>PDF fallback \u2013 download the attached PDF and extract text with    PyMuPDF for any missing fields.</li> <li>LLM fallback \u2013 delegate unresolved gaps to a Large Language    Model supplied via llm.</li> </ol> <p>Each layer can be enabled or disabled via <code>scraping_strategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM | None</code> <p>Client implementing <code>_call_llm_fallback()</code>; required only when the LLM stage is enabled.</p> <code>None</code> <code>category</code> <code>str</code> <p>Judiciary category slug (e.g. <code>\"suicide\"</code>, <code>\"hospital_deaths\"</code>) or <code>\"all\"</code>.</p> <code>'all'</code> <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2050-01-01'</code> <code>max_workers</code> <code>int</code> <p>Thread-pool size for concurrent scraping.</p> <code>10</code> <code>max_requests</code> <code>int</code> <p>Maximum simultaneous requests per host (enforced with a semaphore).</p> <code>5</code> <code>delay_range</code> <code>tuple[float, float] | None</code> <p>Random delay (seconds) before every request. Use <code>None</code> to disable (not recommended).</p> <code>(1, 2)</code> <code>timeout</code> <code>int</code> <p>Per-request timeout in seconds.</p> <code>60</code> <code>scraping_strategy</code> <code>list[int] | tuple[int, int, int]</code> <p>Defines the order in which HTML, PDF and LLM scraping are attempted. The sequence indexes correspond to <code>(HTML, PDF, LLM)</code>. Provide <code>-1</code> to disable a stage.  For example <code>[1, 2, -1]</code> runs HTML first, then PDF, and disables LLM scraping.</p> <code>[1, 2, 3]</code> <code>include_url</code> <code>bool</code> <p>Include the <code>url</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_id</code> <code>bool</code> <p>Include the <code>id</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_date</code> <code>bool</code> <p>Include the <code>date</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Include the <code>area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>investigation</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>circumstances</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>concerns</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_time_stamp</code> <code>bool</code> <p>Include a <code>date_scraped</code> column. Defaults to <code>False</code>.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Emit debug-level logs when True.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>reports</code> <code>DataFrame | None</code> <p>Cached result of the last call to <code>scrape_reports</code> or <code>top_up</code>.</p> <code>report_links</code> <code>list[str]</code> <p>URLs discovered by <code>get_report_links</code>.</p> <code>NOT_FOUND_TEXT</code> <code>str</code> <p>Placeholder value set when a field cannot be extracted.</p> <p>Examples:</p> <pre><code>from pfd_toolkit import Scraper\nscraper = Scraper(\n    category=\"suicide\",\n    start_date=\"2020-01-01\",\n    end_date=\"2022-12-31\",\n    scraping_strategy=[1, 2, 3],\n    llm=my_llm_client,\n)\ndf = scraper.scrape_reports()          # full scrape\nnewer_df = scraper.top_up(df)          # later \"top-up\"\nadded_llm_df = scraper.run_llm_fallback(df)  # apply LLM retro-actively\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.get_report_links","title":"get_report_links","text":"<pre><code>get_report_links()\n</code></pre> <p>Discover individual report URLs for the current query, across all pages.</p> <p>Iterates through _get_report_href_values (which collects URLs for a single page).</p> <p>Pagination continues until a page yields zero new links.</p> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>All discovered URLs, or None if no links were found for the given category/date window.</p>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.run_llm_fallback","title":"run_llm_fallback","text":"<pre><code>run_llm_fallback(reports_df=None)\n</code></pre> <p>Ask the LLM to fill cells still set to <code>self.NOT_FOUND_TEXT</code>.</p> <p>Only the missing fields requested via <code>include_*</code> flags are sent to the model, along with the report\u2019s PDF bytes (when available).</p> <p>Parameters:</p> Name Type Description Default <code>reports_df</code> <code>DataFrame | None</code> <p>DataFrame to process. Defaults to <code>self.reports</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same shape as <code>reports_df</code>, updated in place and re-cached to <code>self.reports</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no LLM client was supplied at construction time.</p> <p>Examples:</p> <p>Run the fallback step after scraping::</p> <pre><code>updated_df = scraper.run_llm_fallback()\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.scrape_reports","title":"scrape_reports","text":"<pre><code>scrape_reports()\n</code></pre> <p>Execute a full scrape with the Class configuration.</p> Workflow <ol> <li>Call <code>get_report_links</code>.</li> <li>Extract each report according to <code>scraping_strategy</code>.</li> <li>Cache the final DataFrame to <code>self.reports</code>.</li> </ol> <p>Returns:</p> Type Description <code>DataFrame</code> <p>One row per report.  Column presence matches the <code>include_*</code> flags. The DataFrame is empty if nothing was scraped.</p> <p>Examples:</p> <p>Scrape reports and inspect columns::</p> <pre><code>df = scraper.scrape_reports()\ndf.columns\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.top_up","title":"top_up","text":"<pre><code>top_up(\n    old_reports=None,\n    start_date=None,\n    end_date=None,\n    clean=False,\n)\n</code></pre> <p>Check for and append new PFD reports within the current parameters.</p> <p>If new links are found they are scraped and appended to <code>self.reports</code>. Any URL (or ID) already present in old_reports is skipped.</p> <p>Optionally, you can override the start_date and end_date parameters from <code>self</code> for this call only.</p> <p>Parameters:</p> Name Type Description Default <code>old_reports</code> <code>DataFrame | None</code> <p>Existing DataFrame. Defaults to <code>self.reports</code>.</p> <code>None</code> <code>start_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>end_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>clean</code> <code>bool</code> <p>When <code>True</code>, run the <code>Cleaner</code> on the newly scraped rows before merging them with existing reports.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>Updated DataFrame if new reports were added; None if no new records were found and old_reports was None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If old_reports lacks columns required for duplicate checks.</p> <p>Examples:</p> <p>Add new reports to an existing DataFrame::</p> <pre><code>updated = scraper.top_up(df, end_date=\"2023-01-01\")\nlen(updated) - len(df)  # number of new reports\n</code></pre>"},{"location":"reference/screener/","title":"<code>Screener</code>","text":"<p>Classifies a list of report texts against a user-defined topic using an LLM.</p> <p>This class takes a DataFrame of reports, a search query, and various configuration options to classify whether each report matches the query. It can either filter the DataFrame to return only matching reports or add a classification column to the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>An instance of the LLM class from <code>pfd_toolkit</code>.</p> <code>None</code> <code>reports</code> <code>DataFrame</code> <p>A DataFrame containing Prevention of Future Death reports.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print more detailed logs. Defaults to False.</p> <code>False</code> <code>include_date</code> <code>bool</code> <p>Flag to determine if the 'date' column is included. Defaults to False.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Flag to determine if the 'coroner' column is included. Defaults to False.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Flag to determine if the 'area' column is included. Defaults to False.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Flag to determine if the 'receiver' column is included. Defaults to False.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Flag to determine if the 'investigation' column is included. Defaults to True.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Flag to determine if the 'circumstances' column is included. Defaults to True.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Flag to determine if the 'concerns' column is included. Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>user_topic = \"medication errors\"\nllm_client = LLM()\nscreener = Screener(llm=llm_client, reports=reports_df)\nscreened_reports = screener.screen_reports(search_query=user_topic)\nprint(f\"Found {len(screened_reports)} report(s) on '{user_topic}'.\")\n</code></pre>"},{"location":"reference/screener/#pfd_toolkit.Screener.screen_reports","title":"screen_reports","text":"<pre><code>screen_reports(\n    reports=None,\n    search_query=None,\n    user_query=None,\n    filter_df=True,\n    result_col_name=\"matches_query\",\n    produce_spans=False,\n    drop_spans=False,\n)\n</code></pre> <p>Classifies reports in the DataFrame against the user-defined topic using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>If provided, this DataFrame will be used for screening, replacing any DataFrame stored in the instance for this call.</p> <code>None</code> <code>search_query</code> <code>str</code> <p>Query string describing the reports you want to find. Overrides any query stored on the instance for this call. The prompt template will be rebuilt.</p> <code>None</code> <code>user_query</code> <code>str</code> <p>Deprecated alias for <code>search_query</code>. Will be removed in a future release.</p> <code>None</code> <code>filter_df</code> <code>bool</code> <p>If <code>True</code> the returned DataFrame is filtered to only matching reports. Defaults to <code>True</code>.</p> <code>True</code> <code>result_col_name</code> <code>str</code> <p>Name of the boolean column added when <code>filter_df</code> is <code>False</code>. Defaults to <code>\"matches_query\"</code>.</p> <code>'matches_query'</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code> a <code>spans_matches_topic</code> column is created containing the text snippet that justified the classification. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, the <code>spans_</code> column corresponding to <code>result_col_name</code> created during this call is removed from the returned DataFrame. Spans columns from other sources remain intact. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Either a filtered DataFrame (if <code>filter_df</code> is <code>True</code>), or the original DataFrame with an added classification column.</p> <p>Examples:</p> <pre><code>reports_df = pd.DataFrame(data)\nscreener = Screener(LLM(), reports=reports_df)\n\n# Screen reports with the initial query\nfiltered_df = screener.screen_reports(search_query=\"medication safety\")\n\n# Screen the same reports with a new query and add a classification column\nclassified_df = screener.screen_reports(search_query=\"tree safety\", filter_df=False)\n</code></pre>"},{"location":"screener/","title":"Filter reports with a query","text":"<p>The <code>Screener</code> class filters PFD reports according to a plain\u2011English query. This page shows how to run your first screen and explains why an LLM outperforms keyword search.</p> <p>See also Additional options and Tips for writing a good search query.</p>"},{"location":"screener/#a-minimal-example","title":"A minimal example","text":"<p>First, import the necessary modules, load reports and set up an <code>LLM</code> client:</p> <pre><code>from pfd_toolkit import load_reports, LLM, Screener\n\n# Grab all reports from 2024\nreports = load_reports(start_date=\"2023-01-01\",\n                       end_date=\"2023-12-31\")\n\n# Set up your LLM client\nllm_client = LLM(api_key=YOUR-API-KEY)\n</code></pre> <p>Then define a <code>search_query</code> which describes the reports you're interested in. Pass this query as an argument to <code>screen_reports()</code> and you'll be given a filtered dataset containing only reports which the LLM judged to have matched your query.</p> <pre><code>search_query = \"Deaths in police custody **only**.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports\n)\n\npolice_df = screener.screen_reports(\n    search_query=search_query)\n</code></pre> <p><code>police_df</code> will now only contain reports related to your query.</p>"},{"location":"screener/#why-not-just-have-a-normal-search-function","title":"Why not just have a \"normal\" search function?","text":"<p>A keyword search is only as good as the exact words you type. Coroners, however, don't always follow a shared vocabulary. The same idea can surface in wildly different forms:</p> <ul> <li>Under-staffing might be written as \"staff shortages,\" \"inadequate nurse cover,\" or even \"resource constraints.\"</li> <li>Suicide in prisons may masquerade as \"self-inflicted injury while remanded,\" \"ligature event in cell,\" or may not even appear together in the same sentence.</li> </ul> <p>A keyword filter misses these variants unless you identify every synonym in advance. By contrast, an <code>LLM</code> understands the context behind your query and links the phrasing for you, which is exactly what <code>Screener</code> taps into.</p>"},{"location":"screener/options/","title":"Additional options","text":""},{"location":"screener/options/#annotation-vs-filtering","title":"Annotation vs. filtering","text":"<p>If <code>filter_df</code> is True (the default) <code>Screener</code> returns a trimmed DataFrame that contains only the reports the LLM marked as relevant to your query.</p> <p>Setting it to False activates annotate mode: every report/row from your original DataFrame is kept, and a boolean column is added denoting whether the report met your query or not. You can also rename this column with <code>result_col_name</code>.</p> <p>Annotate mode is useful where you want to add a column denoting whether the report matched your query, but you don't want to lose the non-matching reports from your dataset.</p> <pre><code>screener = Screener(\n    llm=llm_client,\n    reports=reports,\n)\n\nannotated = screener.screen_reports(\n    search_query=search_query,\n    filter_df=False,    # &lt;--- create annotation column instead of filtering\n    result_col_name='custody_match'     # &lt;--- name of annotation column\n)\n</code></pre>"},{"location":"screener/options/#choosing-which-columns-the-llm-sees","title":"Choosing which columns the LLM 'sees'","text":"<p>By default the LLM model reads the narrative heavyweight sections of each report: investigation, circumstances and concerns. You can expose or hide any field with <code>include_*</code> flags.</p> <p>For example, if you are screening based on a specific cause of death, then you should consider setting <code>include_concerns</code> to False, as including this won't benefit your search.</p> <p>By contrast, if you are searching for a specific concern, then setting <code>include_investigation</code> and <code>include_circumstances</code> to False may improve accuracy, speed up your code, and lead to cheaper LLM calls.</p> <pre><code>search_query = \"Death from insulin overdose due to misprogrammed insulin pumps.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n    include_concerns=False    # &lt;--- Our query doesn't need this section\n)\n\nresult = screener.screen_reports(search_query=search_query)\n</code></pre> <p>In another example, let's say we are only interested in reports sent to a Member of Parliament. We'll want to turn off all default sections and only read from the receiver column.</p> <pre><code>search_query = \"Whether the report was sent to a Member of Parliament (MP)\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n\n    # Turn off the defaults...\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=False,\n\n    include_receiver=True       # &lt;--- Read from receiver section\n)\n\nresult = screener.screen_reports(search_query=search_query)\n</code></pre>"},{"location":"screener/options/#all-options-and-defaults","title":"All options and defaults","text":"Flag Report section What it's useful for Default <code>include_coroner</code> Coroner\u2019s name Simply the name of the coroner. Rarely needed for screening. <code>False</code> <code>include_area</code> Coroner\u2019s area Useful for geographic questions, e.g.\u00a0deaths in South-East England. <code>False</code> <code>include_receiver</code> Receiver(s) of the report Great for accountability queries, e.g. reports sent to NHS Wales. <code>False</code> <code>include_investigation</code> \u201cInvestigation &amp; Inquest\u201d section Contains procedural detail about the inquest. <code>True</code> <code>include_circumstances</code> \u201cCircumstances of Death\u201d section Describes what actually happened; holds key facts about the death. <code>True</code> <code>include_concerns</code> \u201cCoroner\u2019s Concerns\u201d section Lists the issues the coroner wants addressed \u2014 ideal for risk screening. <code>True</code>"},{"location":"screener/options/#returning-text-spans","title":"Returning text spans","text":"<p>Set <code>produce_spans=True</code> when calling <code>screen_reports()</code> to capture the exact snippets from the report text that justified whether or not a report was returned as relevant or not. A new column called <code>spans_matches_topic</code> will be created containing these verbatim snippets. </p> <pre><code>screener = Screener(llm=llm_client, reports=reports)\n\nfiltered_reports = screener.screen_reports(\n    search_query=\"Where the cause of death was determined to be suicide\", \n    produce_spans=True, \n    drop_spans=False)\n</code></pre> <p>If you only want to use the spans internally, pass <code>drop_spans=True</code> to remove the column from the returned dataset after screening.</p> <p>Note</p> <p>Producing but then dropping spans might seem a bit pointless, but it's actually likely a great way of improving performance. The LLM will generate these spans before deciding whether a report matches the query, allowing it to judge whether these spans truly capture the search criteria.</p>"},{"location":"screener/tips/","title":"Tips for writing a good search query","text":"<ol> <li>Stick to one core idea. Give the LLM a single, clear subject: \u201cfalls from hospital beds,\u201d \u201ccarbon-monoxide poisoning at home.\u201d In general, the shorter the prompt, the less room for misinterpretation.</li> <li>Avoid nested logic. Complex clauses like \u201csuicide and medication error but not in custody\u201d dilute the signal. Consider running separate screens (suicide; medication error; in custody) and combine or subtract results later with pandas.</li> <li>Let the model handle synonyms. You don\u2019t need \u201cdefective, faulty, malfunctioning\u201d all in the same query; \u201cmalfunctioning defibrillators\u201d is enough.</li> <li>Use positive phrasing. Negations (e.g. \u201cnot related to COVID-19\u201d) can flip the model\u2019s reasoning. Screen positively, set <code>filter_df</code> to False, then drop rows in pandas.</li> <li>Keep it readable. If your query needs multiple commas or parentheses, break it up. A one-line statement without side notes usually performs best.</li> <li>Use limiting words to restrict scope. Words like \u201conly\" can focus the LLM on a specific case, reducing the chance it will infer extra details. For example, \u201cfalls from beds **only**\u201d signals to the model not to include corridor or bathroom falls. However, avoid overusing them; too many limiters can make the model miss relevant edge cases.</li> </ol> <p>Examples:</p>  Less-effective query Why it struggles  Better query \u201cDeaths where someone slipped or fell in hospital corridors or patient rooms and maybe had fractures but not clinics\u201d Too long, multiple settings, negative clause \u201cFalls on inpatient wards\u201d \u201cFires or explosions causing death at home including gas leaks but not industrial accidents\u201d Mixes two ideas (home vs. industrial) plus a negation \u201cDomestic gas explosions\u201d \u201cCases involving children and allergic reactions to nuts during school outings\u201d Several concepts (age, allergen, setting) \u201cFatal nut allergy on school trip\u201d \u201cRailway incidents that resulted in death due to being hit by train while trespassing or at crossings\u201d Two scenarios joined by \u201cor\u201d; verbose \u201cTrespasser struck by train\u201d \u201cPatients dying because an ambulance was late or there was delay in emergency services arrival or they couldn't get one\u201d Chain of synonyms and clauses \u201cDeath from delayed ambulance\u201d \u201cErrors in giving anaesthesia, like too much anaesthetic, wrong drug, problems with intubation, etc.\u201d Long list invites confusion; \u201cetc.\u201d is vague \u201cAnaesthesia error\u201d"}]}