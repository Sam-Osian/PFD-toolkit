{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>PFD Toolkit is an open-source Python package created to transform how researchers, policymakers, and analysts access and analyse Prevention of Future Death (PFD) reports from coroners in England and Wales.</p> <p>Out of the box, you can:</p> <ol> <li> <p>Load live PFD data in seconds</p> </li> <li> <p>Query and filter reports with natural language</p> </li> <li> <p>Summarise reports to highlight key messages</p> </li> <li> <p>Automatically discover recurring themes</p> </li> <li> <p>Extract other kinds of information, such as age, sex and cause of death</p> </li> </ol> <p>Here is a sample of the PFD dataset:</p> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m... <p>Each row is a unique report, while each column reflects a section of the report. For more information on the structure of these reports, see here.</p>"},{"location":"#why-does-this-package-exist","title":"Why does this package exist?","text":"<p>PFD reports have long served as urgent public warnings \u2014 issued when coroners identified risks that could, if ignored, lead to further deaths. Yet despite being freely available, these reports are chronically underused. This is for one simple reason: PFD reports are a pain to analyse. </p> <p>Common issues include:</p> <ul> <li> <p>No straightforward way to download report content in bulk</p> </li> <li> <p>No reliable way of querying reports to find cases relevant to a specific research question</p> </li> <li> <p>Reports being inconsistent in format (e.g. many reports are low quality digital scans)</p> </li> <li> <p>No system for surfacing recurring issues raised across multiple reports</p> </li> <li> <p>Widespread miscategorisation of reports, creating research limitations</p> </li> </ul> <p>As a result, research involving PFD reports demands months, or even years, of manual admin. Researchers are forced to sift through hundreds/thousands of reports one-by-one, wrestle with absent metadata, and code themes by hand. </p> <p>PFD Toolkit offers a solution to each of these issues, helping researchers load, screen and analyse PFD report data - all in a matter of minutes.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install PFD Toolkit using pip:</p> <pre><code>pip install pfd_toolkit\n</code></pre>"},{"location":"#licence","title":"Licence","text":"<p>This project is distributed under the GNU Affero General Public License v3.0 (AGPL-3.0), available here.</p> <p>Note</p> <ul> <li>You are welcome to use, modify, and share this code under the terms of the AGPL-3.0.</li> <li>If you use this code to provide a networked service, you are required to make the complete source code available to users of that service.</li> <li>Some project dependencies may have their own licence terms, which could affect certain types of use (e.g. commercial use).</li> </ul>"},{"location":"#contribute","title":"Contribute","text":"<p>PFD Toolkit is designed as a research-enabling tool, and we\u2019re keen to work with the community to make sure it genuinely meets your needs. If you have feedback, ideas, or want to get involved, head to our Feedback &amp; contributions page.</p>"},{"location":"changelog/","title":"\ud83d\udcc6 Changelog","text":"<p>Welcome to the project changelog. All notable changes to this project will be documented below.</p>"},{"location":"changelog/#032-2025-06-23","title":"0.3.2 - 2025-06-23","text":"<ul> <li>Fix typos and improve documentation.</li> <li>Small tweaks to the source code.</li> </ul>"},{"location":"changelog/#031-2025-06-19","title":"0.3.1 - 2025-06-19","text":"<p>Improve reliability of weekly dataset top-ups.</p>"},{"location":"changelog/#030-2025-06-18","title":"0.3.0 - 2025-06-18","text":"<p>First public release! \u2728</p>"},{"location":"contact/","title":"How to get in touch","text":"<p>Reach out to the lead developer Sam via email or LinkedIn.</p>"},{"location":"contribute/","title":"Feedback &amp; contributions","text":"<p>Thank you for your interest in contributing to PFD Toolkit! We welcome input from researchers, data scientists, developers, and anyone passionate about improving access to coroners\u2019 Prevention of Future Death reports.</p>"},{"location":"contribute/#how-you-can-get-involved","title":"How you can get involved","text":""},{"location":"contribute/#report-issues-bugs","title":"Report issues &amp; bugs","text":"<p>If you encounter a bug, data problem, or unexpected behaviour, please open an issue on Github.</p> <p>Include a clear description and, if possible, steps to reproduce, your Python version, and environment details.</p>"},{"location":"contribute/#suggest-features-or-improvements","title":"Suggest features or improvements","text":"<p>Have an idea for a new feature, a better workflow, or an additional data cleaning/categorisation option?</p> <p>Submit a feature request as a GitHub issue with as much detail as possible.</p>"},{"location":"contribute/#code-contributions","title":"Code contributions","text":"<p>Contributions are welcome - whether fixing bugs, adding new features, improving documentation, or expanding tests.</p> <p>If your change is significant, consider opening an issue first to discuss it.</p> <p>Note</p> <p>Code contributors should: </p> <ul> <li>Fork the repository on GitHub using the Fork button on the repo home page.</li> <li>Clone your fork locally:</li> </ul> <pre><code>git clone https://github.com/&lt;your-username&gt;/PFD-toolkit.git\n</code></pre> <ul> <li>Install <code>uv</code></li> <li>Install project dependencies with <code>uv</code>:</li> </ul> <pre><code>uv sync\n</code></pre>"},{"location":"contribute/#anything-else-get-in-touch","title":"Anything else, get in touch \ud83d\udcac","text":"<p>If you have any questions, feedback or would like to contribute in a way not outlined above, please contact Sam on samoand@liverpool.ac.uk.</p>"},{"location":"llm_setup/","title":"Creating an LLM Client","text":"<p>PFD Toolkit uses a Large Language Model (LLM) client for advanced features. This page explains how to set up your LLM, what an API key is, and why you might need these features.</p>"},{"location":"llm_setup/#setting-up-your-llm-client","title":"Setting up your LLM client","text":"<p>To use AI-powered features, you need to create an LLM client and supply your OpenAI API key (how to get one below). You do not need an LLM client to simply load report data (using <code>load_reports</code>).</p> <p>Basic setup:</p> <pre><code>from pfd_toolkit import LLM\n\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace YOUR-API-KEY with your real API key\n</code></pre> <p>You can now use LLM-powered features! For example, to screen for reports about medication purchased online:</p> <pre><code>from pfd_toolkit import Screener\n\nquery = \"Deaths that followed ordering medication(s) online.\"\n\nscreener = Screener(llm=llm_client, reports=reports)\nonline_med_reports = screener.screen_reports(user_query=query)\n</code></pre>"},{"location":"llm_setup/#how-do-i-get-an-openai-api-key","title":"How do I get an OpenAI API key?","text":"<ol> <li>Sign up or log in at OpenAI Platform.</li> <li>Go to API Keys.</li> <li>Click \u201cCreate new secret key\u201d and copy the string.</li> <li>Store your key somewhere safe. Never share or publish it.</li> <li>Add credit to your account (just $5 is enough for most research uses).</li> </ol>"},{"location":"llm_setup/#for-more-information-about-usage-costs-see-openai-pricing","title":"For more information about usage costs, see OpenAI pricing.","text":""},{"location":"llm_setup/#speed-up-your-llm","title":"Speed up your LLM","text":"<p>Process more reports in parallel by increasing the <code>max_workers</code> parameter. By default, this is set to <code>8</code>, but larger values can lead to faster run-times.</p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    max_workers=30      # Increase parallelisation\n)\n</code></pre> <p>Note</p> <p>OpenAI enforces rate limits for each account and model. If you set <code>max_workers</code> too high, you may hit these limits and see errors or slowdowns. PFD Toolkit will automatically pause and retry if a rate limit is reached, but it\u2019s best to keep <code>max_workers</code> within a reasonable range (usually 8 to 20 for most users). </p> <p>Your exact rate limit may depend on the 'tier' of your OpenAI account as well as the model you're using. If you need higher limits, you may be able to apply for an increase in your OpenAI account settings.</p>"},{"location":"llm_setup/#change-your-model","title":"Change your model","text":"<p>By default, PFD Toolkit uses <code>gpt-4.1-mini</code>. We love this model as it balances cost, speed, and accuracy. We also recommend its larger equivalent, <code>gpt-4.1</code>, which may offer improved performance, though with additional API costs and less forgiving rate limits.</p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    model=\"gpt-4.1\"     # Set model here\n)\n</code></pre> <p>See OpenAI's documentation for a complete list of their models.</p>"},{"location":"llm_setup/#use-a-custom-endpoint","title":"Use a custom endpoint","text":"<p>You can set a custom endpoint (e.g. for Azure, Ollama, etc.) if it supports the OpenAI SDK:</p> <pre><code>llm_client = LLM(\n    api_key=openai_api_key,\n    base_url=\"https://...\"   # Set your custom endpoint\n)\n</code></pre>"},{"location":"non_coders/","title":"For Non-Coders","text":"<p>We appreciate that not all researchers are Python coders, and we also believe that this shouldn't stop you from unlocking the insights contained within PFD reports.</p> <p>If you have a research question, policy project, or just curiosity about PFD reports, please get in touch with me (Sam).</p>"},{"location":"non_coders/#how-i-can-help","title":"How I can help","text":"<p>If you want to use the PFD Toolkit but are not sure where to start, or you find the technical side a bit daunting, I am genuinely very happy to help. Here is what I can offer:</p> <ul> <li>Explain PFD reports. I can tell you more about what kind of information is contained within PFD reports and whether they could support your research or policy goals.</li> <li>Custom Scripts. Tell me what you are trying to do, for example, \"I want all reports related to mental health in the North West from 2022,\" or \"Can I get a table of reports mentioning medication errors?\" I can then write the script for you.</li> <li>Ready-to-Use Results. I will run the toolkit for you and provide you with the outputs you need, whether it is a spreadsheet, a summary, or visualisations.</li> <li>Ongoing Support. If you want to learn how to do it yourself next time, I can guide you at your pace.</li> </ul>"},{"location":"non_coders/#how-to-get-in-touch","title":"How to get in touch","text":"<p>Just reach out via email or LinkedIn.</p> <p>I want the toolkit to be useful to as many people as possible, not just programmers. If you are a policy worker, academic, journalist, campaigner, or someone affected by PFD reports, please reach out. I would be delighted to help you get the most out of this resource.</p>"},{"location":"pfd_reports/","title":"More about PFD reports","text":"<p>Prevention of Future Death (PFD) reports are a unique, under-recognised mechanism within the English and Welsh legal system for flagging hazards that threaten lives. These documents, written by coroners at the close of certain inquests, have the potential to drive real change \u2014 but only if their warnings are heard and acted upon.</p>"},{"location":"pfd_reports/#what-is-a-pfd-report","title":"What is a PFD report?","text":"<p>When a coroner concludes an inquest and believes that action should be taken to prevent future deaths, they are legally obliged (under Regulation 28 of the Coroners (Investigations) Regulations 2013) to issue a PFD report. The report is sent to any person or organisation the coroner thinks could take action. This could be an NHS trust, a regulator, a private company, a local council, or even the government.</p> <p>The aim is simple: to prevent further deaths by highlighting risks, missed opportunities, or avoidable harm that have already claimed a life.</p> <p>Other than through this toolkit, reports are publicly available here.</p>"},{"location":"pfd_reports/#what-do-pfd-reports-look-like","title":"What do PFD reports look like?","text":"<p>PFD reports tend to be short, factual documents. PFD Toolkit collects the following sections from each report:</p> PFD report section What it contains URL The URL of the report. ID The ID number associated with the report. Date The date that the report was published. Note that this is not the date of death. Coroner name Identifies the coroner by name. Coroner area States the area of the coroner. Each area typically covers one or more local authorities. Recipient(s) The addressee list \u2013 every person, body or department the coroner believes has the power to act on the concerns. Investigation and inquest Provides the inquest conclusion (medical cause and verdict). Circumstances of the death A concise, factual summary of how the death occurred, setting the scene for the concerns that follow. Coroner's concerns Lists specific matters revealed by the evidence that give rise to a risk of future deaths. <p>Note</p> <p>The above table is a rough guide. In practice, each coroner may approach the writing of PFD reports slightly differently. For example, there is occasional overlap between the \"Investigation and inquest\" and \"Circumstances of the death\" sections.</p>"},{"location":"pfd_reports/#why-do-pfd-reports-matter","title":"Why do PFD reports matter?","text":"<p>PFD reports offer us a rare window into risks and failures that may not appear in routine data. They can expose themes including, but absolutely not limited to:</p> <ul> <li> <p>Missed diagnoses and medical errors</p> </li> <li> <p>Gaps in mental health or social care provision</p> </li> <li> <p>Unsafe systems or environments (e.g. railway safety, housing, road design)</p> </li> <li> <p>Inadequate policies or regulatory oversight</p> </li> </ul> <p>Because coroners have a statutory duty to write them, PFD reports sometimes identify entirely new risks \u2014 before they turn into trends. For researchers and policymakers, they are therefore a critical early-warning system.</p>"},{"location":"pfd_reports/#how-does-pfd-toolkit-help","title":"How does PFD Toolkit help?","text":"<p>Before PFD Toolkit, there was no automated way of screening, discovering themes, or extracting information from these reports. Researchers would have to manually screen them report-by-report, demanding months or even years of researcher time.</p> <p>Through this, we are hoping to lower the barrier to research for those interested in using PFD reports.</p>"},{"location":"extractor/","title":"Analysing PFD reports","text":"<p>PFD Toolkit ships with an <code>Extractor</code> class to pull \"features\" (i.e. key pieces of information) from Prevention of Future Death (PFD) reports. </p> <p>These features could be recurring themes, or more specific bits of information (e.g. age, sex, cause of death, etc.).</p> <p>The guides below walk through the main features:</p> <ul> <li>Basic usage \u2013 create a basic feature model to identify features from report data.</li> <li>Summaries &amp; token counts \u2013 generate short summaries and estimate the token cost of your data.</li> <li>Tagging reports with themes \u2013 automatically discover recurring themes or label reports with your own taxonomy.</li> <li>Capturing text spans \u2013 keep short excerpts (\"spans\") showing where each feature came from.</li> <li>Caching and exporting results \u2013 reuse completions to save time and API costs.</li> </ul>"},{"location":"extractor/basics/","title":"Basic usage","text":"<p>Start by defining a feature model with <code>pydantic</code>. Each attribute represents a piece of information you want to pull out of the report. <code>Extractor</code> accepts any valid <code>BaseModel</code>, so feel free to mix strings, numbers or more complex types:</p> <pre><code>from pydantic import BaseModel, Field\nfrom pfd_toolkit import load_reports, LLM, Extractor\n\n# Define feature model with pydantic\nclass MyFeatures(BaseModel):\n    age: int\n    cause_of_death: str\n</code></pre> <p>Next, load some report data and set up your LLM client. You then pass the feature model, the reports and the LLM client to an <code>Extractor</code> instance and call <code>.extract_features()</code>:</p> <pre><code>reports = load_reports(category=\"all\", start_date=\"2024-01-01\", end_date=\"2024-12-31\")\nllm_client = LLM(api_key=\"YOUR-API-KEY\")\n\nextractor = Extractor(\n    feature_model=MyFeatures,\n    reports=reports,\n    llm=llm_client\n)\n\nresult_df = extractor.extract_features()\n</code></pre> <p><code>result_df</code> now contains the new <code>age</code> and <code>cause_of_death</code> columns. You can repeat the call with a different feature model to extract further information \u2013 the cached results mean previously processed rows will not be re-sent to the LLM unless you clear the cache with <code>.reset()</code>.</p>"},{"location":"extractor/basics/#choosing-which-sections-the-llm-reads","title":"Choosing which sections the LLM reads","text":"<p><code>Extractor</code> lets you decide exactly which parts of the report are presented to the model. Each <code>include_*</code> flag mirrors one of the columns loaded by <code>load_reports</code>. Turning fields off reduces the amount of text sent to the LLM which often speeds up requests and lowers token usage.</p> <pre><code>extractor = Extractor(\n    llm=llm_client,\n    reports=reports,\n    include_investigation=True,\n    include_circumstances=True,\n    include_concerns=False  # Skip coroner's concerns if not relevant\n)\n</code></pre> <p>In this example only the investigation and circumstances sections are provided to the LLM. The coroner's concerns are omitted entirely. Limiting the excerpt like this often improves accuracy and drastically reduces token costs. However, be careful you're not turning 'off' a report section which is genuinely useful for your query.</p>"},{"location":"extractor/basics/#re-run-the-extraction","title":"Re-run the extraction","text":"<p>By default, the <code>Extractor</code> class won't run on the same data with the same configuration twice. </p> <p>If you want to start fresh, call <code>reset()</code> to clear cached feature values and token estimates, and chain it into a new <code>extract_features()</code> call:</p> <pre><code>clean_df = extractor.reset().extract_features(feature_model=NewModel)\n</code></pre>"},{"location":"extractor/caching/","title":"Caching and exporting configuration","text":"<p><code>Extractor</code> caches every LLM response so repeated calls with the same configuration reuse previous results.</p> <p>Export the cache before you shut down and import it in a future session to avoid running the model on reports that have already been extracted:</p> <pre><code>extractor.export_cache(\"my_cache.pkl\")\n...\nextractor.import_cache(\"my_cache.pkl\")\n</code></pre> <p>If you want to start fresh, call <code>reset()</code> to clear cached feature values and token estimates. This is useful when you wish to re-run <code>extract_features</code> on the same DataFrame with a different feature model. <code>reset</code> returns the instance so you can immediately chain another call:</p> <pre><code>clean_df = extractor.reset().extract_features(feature_model=NewModel)\n</code></pre> <p>The returned DataFrame contains your newly extracted features and an empty cache ready for further runs.</p>"},{"location":"extractor/spans/","title":"Capturing text spans","text":"<p>Sometimes you want to know exactly which lines from the report led the model to assign a particular value to a given report's field. </p> <p>For example, say we asked the model to identify whether the deceased is a child and the model outputs <code>True</code> for a particular report, we might want to know whether this was because age is explicitly recorded (e.g. \"The deceased was aged 16\") or implied based on context (e.g. \"The deceased was being seen by CAMHS prior to their death\").</p> <p><code>Extractor</code> can add these quotations (or 'spans') automatically. This is...</p> <ul> <li>Great for performance, because we're instructing the model to identify evidence for a feature value before it is assigned, reducing the risk of false positives.</li> <li>Great for human verification, because we can easily verify whether the model's evidence matches its assignment for each report.</li> </ul>"},{"location":"extractor/spans/#include-spans","title":"Include spans","text":"<p><code>.extract_features()</code> accepts a <code>produce_spans</code> flag. When enabled, a new column starting with <code>spans_</code> is created for every feature.</p> <p>For example, in our above example where we extract feature \"child\", a separate column called \"spans_child\" will be created. Each <code>spans_</code> column contains verbatim snippets from the report which justify the extracted value.</p> <pre><code>class ChildID(BaseModel):\n    child: bool = Field(..., description=\"Whether the deceased is a child (under 18)\")\n\nresult = extractor.extract_features(\n    feature_model=ChildID,\n    produce_spans=True,\n)\nresult\n</code></pre> <p>The quotes returned in the spans are kept as short as possible but should always match the original text verbatim. Multiple snippets are separated with semicolons.</p>"},{"location":"extractor/spans/#include-drop-spans","title":"Include &amp; drop spans","text":"<p>If you're not interested in verifying the output, you might want to remove the identified spans from the returned DataFrame after extraction. Set <code>drop_spans=True</code> to remove all <code>spans_</code> columns.</p> <p>As mentioned before, producing but later dropping spans is still likely to improve performance, because you're forcing the model to generate evidence as part of its internal workings out.</p> <pre><code>extractor.extract_features(\n    feature_model=DemoModel,\n    produce_spans=True,\n    drop_spans=True,\n)\n</code></pre>"},{"location":"extractor/summarising/","title":"Summaries &amp; token counts","text":"<p>Use <code>.summarise()</code> to condense each report into a short text snippet. The <code>trim_intensity</code> option controls how terse the summary should be. Calling <code>summarise</code> adds a <code>summary</code> column to your stored reports and keeps a copy on the instance under <code>extractor.summarised_reports</code> for later reuse.</p> <pre><code>summary_df = extractor.summarise(trim_intensity=\"medium\")\nsummary_df[[\"summary\"]].head()\n</code></pre> <p>The resulting DataFrame contains a new column (default name <code>summary</code>). You can specify a different column name via <code>result_col_name</code> if desired.</p>"},{"location":"extractor/summarising/#estimating-token-counts","title":"Estimating token counts","text":"<p>Token usage is important when working with paid APIs. The <code>estimate_tokens()</code> helper provides a quick approximation of how many tokens a text column will consume.</p> <pre><code>total = extractor.estimate_tokens()\nprint(f\"Total tokens in summaries: {total}\")\n</code></pre> <p><code>estimate_tokens</code> defaults to the summary column, but you can pass any text series via <code>col_name</code>. Set <code>return_series=True</code> to get a per-row estimate instead of the total.</p>"},{"location":"extractor/themes/","title":"Tagging reports with themes","text":"<p><code>Extractor</code> can be used to label reports with your own themes. Each field on the feature model represents a potential tag. In the model below the <code>falls_in_custody</code> field indicates whether a death occurred in police custody.</p> <p>Set <code>force_assign=True</code> so the LLM always returns either <code>True</code> or <code>False</code> for each field. <code>allow_multiple=True</code> lets a single report be marked with more than one theme if required.</p> <pre><code># For themes, we recommend always using the `bool` flag\nclass Themes(BaseModel):\n    falls_in_custody: bool = Field(description=\"Death occurred in police custody\")\n    medication_error: bool = Field(description=\"Issues with medication or dosing\")\n\nextractor = Extractor(\n    llm=llm_client,\n    feature_model=Themes,\n    reports=reports,\n    force_assign=True,\n    allow_multiple=True,\n)\n\nlabelled = extractor.extract_features()\n</code></pre> <p>The returned DataFrame includes a boolean column for each theme.</p>"},{"location":"extractor/themes/#discovering-themes-automatically","title":"Discovering themes automatically","text":"<p>Instead of having a prescribed list of themes ahead of time, you may wish to automatically discover themes contained within your selection of reports.</p> <p>Once summaries are available you can call <code>.discover_themes()</code> to let the LLM propose a list of recurring themes. <code>.discover_themes()</code> reads the <code>summary</code> column created by <code>.summarise()</code> (see Summaries &amp; token counts).</p> <p>The function returns a <code>pydantic</code> model describing the discovered themes. You can immediately feed that model back into <code>extract_features</code> to label each report.</p> <pre><code>IdentifiedThemes = extractor.discover_themes()\n\n# Optionally, inspect the newly identified themes:\n# print(IdentifiedThemes)\n\nassigned_reports = extractor.extract_features(\n                              feature_model=IdentifiedThemes,\n                              force_assign=True,\n                              allow_multiple=True)\n</code></pre> <p><code>discover_themes</code> accepts several parameters:</p> <ul> <li><code>warn_exceed</code> and <code>error_exceed</code> \u2013 soft and hard limits for the estimated token count of the combined summaries. Exceeding <code>error_exceed</code> raises an exception.</li> <li><code>max_themes</code> / <code>min_themes</code> \u2013 bound the number of themes the model should return.</li> <li><code>seed_topics</code> \u2013 either a string, list or <code>BaseModel</code> of starter topics. The LLM will incorporate these into the final list.</li> <li><code>extra_instructions</code> \u2013 free\u2011form text appended to the prompt, allowing you to steer the LLM towards particular areas of interest.</li> </ul>"},{"location":"getting_started/discover_themes/","title":"Discover themes in your filtered dataset","text":"<p>With our subset of reports screened for Mental Health Act detention concerns (see previous page), we will now uncover underlying themes contained within these reports. This lets you see 'at a glance' what issues the coroners keep raising.</p> <p>We'll use the <code>Extractor</code> class to automatically identify themes from the concerns section of each report.</p>"},{"location":"getting_started/discover_themes/#set-up-the-extractor","title":"Set up the Extractor","text":"<p>The Extractor reads the text from the screened reports you provide. Each <code>include_*</code> flag controls which sections of the reports are sent to the LLM for analysis. </p> <p>In this example, we are only interested in the coroner's concerns, so we set <code>include_concerns</code> to <code>True</code>, while everything else is set to <code>False</code>:</p> <pre><code>from pfd_toolkit import Extractor\n\nextractor = Extractor(\n    llm=llm_client,             # The same client you created earlier\n    reports=filtered_reports,   # Your screened reports\n\n    include_date=False,\n    include_coroner=False,\n    include_area=False,\n    include_receiver=False,\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=True       # &lt;--- Only supply the 'concerns' text\n)\n</code></pre> <p>Note</p> <p>The main reason why we're hiding all reports sections other than the coroners' concerns is to help keep the LLM's instructions short &amp; focused. LLMs often perform better when they are given only relevant information.</p> <p>The sections you'll want to draw from will depend on your specific research question. To understand more about what information is contained within each of the report sections, please see About the data.</p>"},{"location":"getting_started/discover_themes/#summarise-then-discover-themes","title":"Summarise then discover themes","text":"<p>Before discovering themes, we first need to summarise each report. </p> <p>We do this because the length of PFD reports vary from coroner to coroner. By summarising the reports, we're centering on the key messages, keeping the prompt short for the LLM. This may improve performance and increase speed.</p> <p>The report sections that are summarised depend on the <code>include_*</code> flags you set earlier. In this tutorial, we are only summarising the concerns section.</p> <pre><code># Create short summaries of the concerns\nextractor.summarise(trim_intensity=\"low\")\n\n# Ask the LLM to propose recurring themes\nThemeInstructions = extractor.discover_themes(\n    max_themes=6,  # Limit the list to keep things manageable\n)\n</code></pre> <p>Note</p> <p><code>Extractor</code> will warn you if the word count of your summaries is too high. In these cases, you might want to set your <code>trim_intensity</code> to <code>medium</code>, <code>high</code> or <code>very high</code> (though please note that the more we trim, the more detail we lose).</p> <p><code>ThemeInstructions</code> is a Pydantic model containing a set of detailed instructions for the LLM. We'll need this later to categorise each of the reports by theme.</p> <p>But first, you'll likely want to see which themes the model has identified. To see the list of themes (plus a short, automatically generated description for each) we can run:</p> <pre><code>print(extractor.identified_themes)\n</code></pre> <p>...which gives us a JSON with our themes &amp; descriptions:</p> <pre><code>{\n  \"bed_shortage\": {\n    \"type\": \"bool\",\n    \"description\": \"Shortage of inpatient mental health beds causing prolonged waits, inappropriate placements, and increased risks.\"\n  },\n  \"risk_assessment\": {\n    \"type\": \"bool\",\n    \"description\": \"Failures or inadequacies in assessing, documenting, and managing patient risks including suicide, self-harm, and violence.\"\n  },\n  \"communication_failures\": {\n    \"type\": \"bool\",\n    \"description\": \"Breakdowns in communication and information sharing between healthcare staff, agencies, families, and police.\"\n  },\n  \"staff_training\": {\n    \"type\": \"bool\",\n    \"description\": \"Insufficient or inconsistent training of staff on policies, clinical knowledge, risk management, and emergency procedures.\"\n  },\n  \"policy_implementation\": {\n    \"type\": \"bool\",\n    \"description\": \"Lack of or poor adherence to policies, protocols, and guidance leading to unsafe practices and delays.\"\n  },\n  \"observation_monitoring\": {\n    \"type\": \"bool\",\n    \"description\": \"Failures in patient observation practices, including inadequate monitoring, falsification of records, and unclear procedures.\"\n  }\n}\n</code></pre>"},{"location":"getting_started/discover_themes/#tag-the-reports-with-our-themes","title":"Tag the reports with our themes","text":"<p>Above, we've only identified a list of themes: we haven't yet assigned these themes to each of our reports.</p> <p>Here, we take <code>ThemeInstructions</code> that we created earlier and pass it back into the extractor to assign themes to every report in the dataset:</p> <pre><code>labelled_reports = extractor.extract_features(\n    feature_model=ThemeInstructions,\n    force_assign=True,\n    allow_multiple=True  # (A single report might touch on several themes)\n)\n\nlabelled_reports.head()\n</code></pre> <p>The resulting DataFrame now contains a column for each discovered theme, filled with <code>True</code> or <code>False</code> depending on whether that theme was present in the coroner's concerns:</p> url date coroner area receiver investigation circumstances concerns bed_shortage risk_assessment communication_failures staff_training policy_implementation observation_monitoring [...] 2025-04-24 S. Marsh Somerset Somerset Foundation Trust... On sixth December... Anne first presented... Anne was not sent... False True True True False False [...] 2025-04-07 S. Reeves South London South London and Maudsley... On 21 March 2023... Christopher McDonald... The evidence heard... False False False True True False [...] 2025-03-25 F. Wilcox Inner West London Commissioner of the Police... From third March... Mr Omishore had been... That there is an... False True True True False False [...] 2025-03-24 T. Rawden South Yorkshire West South West Yorkshire Partnership... On 27 September... Claire Louise Driver... The inquest heard... False True True True False False [...] 2025-03-17 S. Horstead Essex Chief Executive Officer... On 31 October 2023... On the 23rd September... (a) Failures in care... False True True False True False"},{"location":"getting_started/discover_themes/#tabulate-reports","title":"Tabulate reports","text":"<p>Finally, we can count how often a theme appears in our collection of reports:</p> <pre><code>extractor.tabulate()\n</code></pre> <p><pre><code>| Category              | Count | Percentage |\n|-----------------------|-------|------------|\n| risk_assessment       | 69    | 70.41      |\n| information_sharing   | 50    | 51.02      |\n| bed_shortage          | 18    | 18.37      |\n| staff_training        | 49    | 50.00      |\n| policy_compliance     | 58    | 59.18      |\n| environmental_safety  | 17    | 17.35      |\n</code></pre> That's it! You've gone from a mass of PFD reports, to a focused set of cases relating to Mental Health Act detention, to a theme\u2011tagged dataset ready for deeper exploration - all in a matter of minutes.</p> <p>From here, you might want to export your curated dataset to a .csv for qualitative analysis:</p> <pre><code>labelled_reports.to_csv()\n</code></pre> <p>Alternatively, you might want to check out the other analytical features that PFD Toolkit offers.</p>"},{"location":"getting_started/load_and_screen/","title":"Getting started","text":"<p>This page talks you through an example workflow using PFD Toolkit: loading a dataset and screening for relevant cases related to \"detention under the Mental Health Act\". </p> <p>This is just an example. PFD reports contain a breadth of information across a whole range of topics and domains. But in this workflow, we hope to give you a sense of how the toolkit can be used, and how it might support your own project.</p>"},{"location":"getting_started/load_and_screen/#installation","title":"Installation","text":"<p>PFD Toolkit can be installed from pip as <code>pfd_toolkit</code>:</p> <pre><code>pip install pfd_toolkit\n</code></pre> <p>Note</p> <p>PFD Toolkit is not currently available via Anaconda. If you'd like this to change, please make a GitHub Issue. Personally, we love using <code>uv</code> as an alternative to (Ana)conda for dependency management.</p>"},{"location":"getting_started/load_and_screen/#load-your-first-dataset","title":"Load your first dataset","text":"<p>First, you'll need to load a PFD dataset. These datasets are updated weekly, meaning you always have access to the latest reports with minimal setup.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load all PFD reports from January 2024 to May 2025\nreports = load_reports(\n    start_date=\"2024-01-01\",\n    end_date=\"2025-05-01\")\n\nreports.head(n=5)\n</code></pre> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m..."},{"location":"getting_started/load_and_screen/#screen-for-relevant-reports","title":"Screen for relevant reports","text":"<p>You're likely using PFD Toolkit because you want to answer a specific question. For example: \"Do any PFD reports raise concerns related to detention under the Mental Health Act?\"</p> <p>PFD Toolkit lets you query reports in plain English \u2014 no need to know precise keywords or categories. Just describe the cases you care about, and the toolkit will return matching reports.</p>"},{"location":"getting_started/load_and_screen/#set-up-an-llm-client","title":"Set up an LLM client","text":"<p>Screening and other advanced features use AI, and require you to first set up an LLM client. You'll need to head to platform.openai.com and create an API key. Once you've got this, simply feed it to the <code>LLM</code>.</p> <pre><code>from pfd_toolkit import LLM\n\n# Set up LLM client\nllm_client = LLM(api_key=YOUR-API-KEY) # Replace with actual API key\n</code></pre> <p>Note</p> <p>For a more detailed guide on using LLMs in this toolkit, see Setting up an LLM client.</p>"},{"location":"getting_started/load_and_screen/#screen-reports-in-plain-english","title":"Screen reports in plain English","text":"<p>Now, all we need to do is specify our <code>user_query</code> (the statement the LLM will use to filter reports), and set up our <code>Screener</code> engine.</p> <pre><code>from pfd_toolkit import Screener\n\n# Create a user query to filter\nuser_query = \"Concerns related to detention under the Mental Health Act **only**\"\n\n# Screen reports\nscreener = Screener(llm = llm_client,\n                        reports = reports) # Reports that you loaded earlier\n\nfiltered_reports = screener.screen_reports(user_query=user_query)\n</code></pre> <p><code>filtered_reports</code> returns a filtered version of your original DataFrame, only containing reports that the LLM believed matched your query.</p> <p>Note</p> <p>For more information on Screening reports, see Screening relevant reports.</p> <p>In the next page, we'll go through how to discover recurring themes in these screened PFD reports.</p>"},{"location":"loader/cleaner/","title":"Cleaning scraped data","text":"<p><code>Cleaner</code> provides an optional step for polishing scraped reports with the same LLM used in the fallback stage. It loops over selected columns, sends field specific prompts in batches and writes the corrected text back into a copy of your DataFrame.</p>"},{"location":"loader/cleaner/#basic-usage","title":"Basic usage","text":"<pre><code>from pfd_toolkit import Cleaner\n\ncleaner = Cleaner(df, llm, include_receiver=False)\nclean_df = cleaner.clean_reports(anonymise=True)\n</code></pre> <p>The class works field by field so you decide which columns to modify. Use the boolean flags like <code>include_receiver</code> or <code>include_area</code> to toggle each field. Custom prompt strings allow advanced users to steer how the LLM rewrites the text.</p>"},{"location":"loader/cleaner/#anonymising-sensitive-details","title":"Anonymising sensitive details","text":"<p>When <code>anonymise=True</code> the investigation, circumstances and concerns fields are automatically de\u2011identified by swapping names and pronouns for gender\u2011neutral placeholders.</p> <p>The underlying logic constructs a field-specific prompt for every text snippet, sends them to the LLM in batches and writes the results back into a copy of your DataFrame.</p> <p>Any prompts that return an error marker are ignored so the original text remains untouched. Call <code>generate_prompt_template()</code> to preview the finalised prompts before you run the clean.</p>"},{"location":"loader/cleaner/#prompt-templates-and-error-handling","title":"Prompt templates and error handling","text":"<p><code>Cleaner</code> exposes the final prompt for each field via <code>generate_prompt_template()</code>. This lets you inspect or tweak the wording before sending anything to the model. If the LLM returns a recognised error marker, the class reverts to the original text rather than introducing blank values.</p> <p>See the API reference for a detailed breakdown of all arguments and attributes.</p>"},{"location":"loader/load_reports/","title":"Loading report data","text":""},{"location":"loader/load_reports/#get-reports","title":"Get reports","text":"<p><code>load_reports()</code> is the quickest way to access PFD reports.  It loads a clean CSV and returns a pandas <code>DataFrame</code>. Each row represents a single report, with columns reflect the main sections of the report.</p> <pre><code>from pfd_toolkit import load_reports\n\n# Load all PFD reports from January 2024 to May 2025\nreports = load_reports(\n    start_date=\"2024-01-01\",\n    end_date=\"2025-05-01\")\n\nreports.head()\n</code></pre> url date coroner area receiver investigation circumstances concerns [...] 2025-05-01 A. Hodson Birmingham and... NHS England; The Rob... On 9th December 2024... At 10.45am on 23rd November... To The Robert Jones... [...] 2025-04-30 J. Andrews West Sussex, Br... West Sussex C... On 2 November 2024 I... They drove their car into... The inquest was told t... [...] 2025-04-30 A. Mutch Manchester Sou... Fluxton Road Medical... On 1 October 2024 I... They were prescribed long... The inquest heard evide... [...] 2025-04-25 J. Heath North Yorkshire... Townhead Surgery On 4th June 2024 I... On 15 March 2024, Richar... When a referral docume... [...] 2025-04-25 M. Hassell Inner North Lo... The President Royal... On 23 August 2024, on... They were a big baby and... With the benefit of a m... <p>Pass a <code>start_date</code> and <code>end_date</code> to restrict the date range, and optionally use <code>n_reports</code> to trim the DataFrame to the most recent n entries. For example...</p> <pre><code>reports = load_reports(\n    n_reports=1000)\n</code></pre> <p>...loads the 1000 latest reports.</p>"},{"location":"loader/load_reports/#refresh-reports","title":"Refresh reports","text":"<p>Reports are updated once a week (Monday 1:00am, universal time). <code>load_reports()</code> caches reports for faster loading, so to retrieve the latest reports you'll need to set <code>clear_cache</code> to <code>True</code>:</p> <pre><code>reports = load_reports(clear_cache=True)\n</code></pre> <p>Note</p> <p>The dataset loaded when you call <code>load_reports()</code> is cleaned and fully processed. This means spelling and grammatical errors have been corrected and boilerplate text removed.</p> <p>If you wish to load an uncleaned version of the dataset, we suggest running your own scrape via <code>Scraper</code>.</p>"},{"location":"loader/scraper/","title":"Scraping module","text":"<p><code>Scraper</code> lets you download PFD reports straight from the judiciary website and control each step of the extraction process. For most projects <code>load_reports()</code> is sufficient, but the scraping module gives you full transparency over how reports are gathered and how missing values are filled in. Use it when you need to customise request behaviour, adjust fallback logic or troubleshoot tricky reports.</p>"},{"location":"loader/scraper/#why-run-a-custom-scrape","title":"Why run a custom scrape?","text":"<p>The weekly datasets cover the majority of use cases. However there are two scenarios when direct scraping may be preferable:</p> <ul> <li>Rapid updates \u2013 the PFD Toolkit dataset lags up to a week behind new publications. Running your own scrape means you can see the newest reports immediately.</li> <li>Custom logic \u2013 while the bundled dataset is a product of Vision-LLM scraping, you may also wish to enable HTML and .pdf scraping.</li> </ul>"},{"location":"loader/scraper/#creating-a-scraper","title":"Creating a scraper","text":"<pre><code>from pfd_toolkit import Scraper\n\nscraper = Scraper(\n    category=\"suicide\",           # judiciary.uk slug or \"all\"\n    llm=llm_client,         # assumes you've already set up your LLM client\n    start_date=\"2024-01-01\",\n    end_date=\"2024-12-31\",\n    scraping_strategy=[1, 2, 3],   # html \u2192 pdf \u2192 llm\n    max_workers=10,\n    delay_range=(1, 2),\n)\n</code></pre> <p>Pass in a category slug (or use <code>\"all\"</code>), a date range and any optional settings such as worker count, request delay or timeout. The <code>scraping_strategy</code> list defines which stages run and in what order. Each entry refers to the HTML, PDF and LLM steps respectively \u2013 set an index to <code>-1</code> to skip a step entirely.</p> <p>Note</p> <p>For example, setting <code>scraping_strategy</code> to <code>[2, 1, -1]</code> runs HTML scraping first, .pdf scraping second, and disables Vision-LLM scraping. Setting it to <code>[2, -1, 1]</code> runs Vision-LLM scraping first, HTML scraping second, and disables .pdf scraping. </p> <p>This latter configuration is exactly what PFD Toolkit uses under the hood to construct the dataset you see when you call <code>load_reports()</code>.</p>"},{"location":"loader/scraper/#a-closer-look-at-the-pipeline","title":"A closer look at the pipeline","text":"<ol> <li>HTML scraping collects data directly from the report landing page. This is the fastest approach and usually recovers most metadata fields (e.g. coroner name, area, receiver) but struggles where the HTML make up of a given report differs, even slightly, from the majority of reports.</li> <li>.pdf scraping downloads the report .pdf and extracts text with PyMuPDF. This approach also recovers most fields, but will often scrape page numbers, footnotes and other .pdf 'juice'. It will fail where a report uses a non-standard heading (e.g. uses just \"Concerns\" instead of the more common \"Coroner's concerns\").</li> <li>Vision-LLM scraping is by far the most reliable method, but also the longest. The LLM understands the reports in context, meaning it doesn't matter if a report has unusual formatting or different section headings.</li> </ol> <p>The stages cascade automatically \u2014 if HTML scraping gathers everything you need, the PDF and LLM steps are skipped. You can reorder or disable steps entirely by tweaking <code>scraping_strategy</code>.</p>"},{"location":"loader/scraper/#running-a-scrape","title":"Running a scrape","text":"<p>After initialisation, call <code>scrape_reports()</code> to run the full scrape:</p> <pre><code>df = scraper.scrape_reports()\n</code></pre> <p>The results are cached on <code>scraper.reports</code> as a pandas DataFrame. This cache lets you rerun individual stages without hitting the network again. If more reports are published later you can update the existing DataFrame with <code>top_up()</code>:</p> <pre><code>updated = scraper.top_up(existing_df=df, end_date=\"2025-01-31\", clean=True)\n</code></pre> <p><code>top_up()</code> only fetches new pages, meaning you avoid repeating work and keep the original ordering intact. When <code>clean=True</code> the new and existing rows are passed through <code>Cleaner.clean_reports()</code> for optional LLM-powered tidying.</p>"},{"location":"loader/scraper/#applying-the-llm-fallback-separately","title":"Applying the LLM fallback separately","text":"<p>Sometimes you may want to review scraped results before running the LLM stage. <code>run_llm_fallback()</code> accepts a DataFrame (typically the output of <code>scrape_reports()</code> or <code>top_up()</code>) and attempts to fill any remaining blanks using your configured language model:</p> <pre><code>llm_df = scraper.run_llm_fallback(df)\n</code></pre>"},{"location":"loader/scraper/#cleaning-scraped-data","title":"Cleaning scraped data","text":"<p>To tidy up scraped fields using the same language model, see the dedicated Cleaner page. It explains how to batch correct scraped text, anonymise personal information and fine\u2011tune prompts for each column.</p>"},{"location":"loader/scraper/#threading-and-polite-scraping","title":"Threading and polite scraping","text":"<p><code>Scraper</code> uses a thread pool to speed up network requests. The <code>max_workers</code> and <code>delay_range</code> settings let you tune throughput and avoid overloading the server. The default one\u2013two second delay between requests mirrors human browsing behaviour and greatly reduces the risk of your IP address being flagged.</p>"},{"location":"loader/scraper/#inspecting-results","title":"Inspecting results","text":"<p>Every scrape writes a timestamp column when <code>include_time_stamp=True</code>. This can be useful for auditing your scraping pipeline. </p> <p>All fields that could not be extracted are set to missing values, making gaps explicit in the final dataset.</p>"},{"location":"reference/cleaner/","title":"<code>Cleaner</code>","text":"<p>Batch-clean PFD report fields with an LLM.</p> <p>The cleaner loops over selected columns, builds field-specific prompts and writes the returned text back into a copy of the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>Input DataFrame to clean.</p> required <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>include_coroner</code> <code>bool</code> <p>Clean the <code>coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Clean the <code>receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Clean the <code>area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Clean the <code>investigation</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Clean the <code>circumstances</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Clean the <code>concerns</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>coroner_prompt</code> <code>str or None</code> <p>Custom prompt for the coroner field. Defaults to <code>None</code>.</p> <code>None</code> <code>area_prompt</code> <code>str or None</code> <p>Custom prompt for the area field. Defaults to <code>None</code>.</p> <code>None</code> <code>receiver_prompt</code> <code>str or None</code> <p>Custom prompt for the receiver field. Defaults to <code>None</code>.</p> <code>None</code> <code>investigation_prompt</code> <code>str or None</code> <p>Custom prompt for the investigation field. Defaults to <code>None</code>.</p> <code>None</code> <code>circumstances_prompt</code> <code>str or None</code> <p>Custom prompt for the circumstances field. Defaults to <code>None</code>.</p> <code>None</code> <code>concerns_prompt</code> <code>str or None</code> <p>Custom prompt for the concerns field. Defaults to <code>None</code>.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Emit info-level logs for each batch when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>cleaned_reports</code> <code>DataFrame</code> <p>Result of the last call to <code>clean_reports</code>.</p> <code>coroner_prompt_template, area_prompt_template, ...</code> <code>str</code> <p>Finalised prompt strings actually sent to the model.</p> <p>Examples:</p> <pre><code>cleaner = Cleaner(df, llm, include_coroner=False, verbose=True)\ncleaned_df = cleaner.clean_reports()\ncleaned_df.head()\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.clean_reports","title":"clean_reports","text":"<pre><code>clean_reports(anonymise=False)\n</code></pre> <p>Run LLM-based cleaning for the configured columns.</p> <p>The method operates in place on a copy of <code>self.reports</code> so the original DataFrame is never mutated.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame in which the selected columns have been replaced by the LLM output (or left unchanged when the model returns an error marker).</p> <p>Parameters:</p> Name Type Description Default <code>anonymise</code> <code>bool</code> <p>When <code>True</code> append an instruction to anonymise names and pronouns in the investigation, circumstances and concerns fields. Defaults to <code>False</code>.</p> <code>False</code> <p>Examples:</p> <pre><code>cleaner = Cleaner(llm=llm_client, reports=reports)\ncleaned = cleaner.clean_reports()\n</code></pre>"},{"location":"reference/cleaner/#pfd_toolkit.Cleaner.generate_prompt_template","title":"generate_prompt_template","text":"<pre><code>generate_prompt_template()\n</code></pre> <p>Return the prompt templates used for each field.</p> <p>The returned dictionary maps DataFrame column names to the full prompt text with a <code>[TEXT]</code> placeholder appended to illustrate how the prompt will look during <code>clean_reports</code>.</p>"},{"location":"reference/extractor/","title":"<code>Extractor</code>","text":"<p>Extract custom features from Prevention of Future Death reports using an LLM.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>Instance of the <code>LLM</code> helper used for prompting.</p> required <code>reports</code> <code>DataFrame</code> <p>DataFrame of PFD reports. When provided it is copied and stored on the instance. Defaults to <code>None</code>.</p> <code>None</code> <code>include_date</code> <code>bool</code> <p>Include the <code>date</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>coroner</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Include the <code>area</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>receiver</code> column in prompts. Defaults to <code>False</code>.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>investigation</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>circumstances</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>concerns</code> column in prompts. Defaults to <code>True</code>.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Emit extra logging when <code>True</code>. Defaults to <code>False</code>.</p> <code>False</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.discover_themes","title":"discover_themes","text":"<pre><code>discover_themes(\n    *,\n    warn_exceed=100000,\n    error_exceed=500000,\n    max_themes=None,\n    min_themes=None,\n    extra_instructions=None,\n    seed_topics=None,\n)\n</code></pre> <p>Use an LLM to automatically discover report themes.</p> <p>The method expects <code>summarise</code> to have been run so that a summary column exists. All summaries are concatenated into one prompt sent to the LLM. The LLM should return a JSON object mapping theme names to descriptions. A new <code>pydantic</code> model is built from this mapping and stored as <code>feature_model</code>.</p> <p>Parameters:</p> Name Type Description Default <code>warn_exceed</code> <code>int</code> <p>Emit a warning if the estimated token count exceeds this value. Defaults to <code>100000</code>.</p> <code>100000</code> <code>error_exceed</code> <code>int</code> <p>Raise a <code>ValueError</code> if the estimated token count exceeds this value. Defaults to <code>500000</code>.</p> <code>500000</code> <code>max_themes</code> <code>int or None</code> <p>Instruct the LLM to identify no more than this number of themes when provided.</p> <code>None</code> <code>min_themes</code> <code>int or None</code> <p>Instruct the LLM to identify at least this number of themes when provided.</p> <code>None</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions appended to the theme discovery prompt.</p> <code>None</code> <code>seed_topics</code> <code>str | list[str] | BaseModel</code> <p>Optional seed topics to include in the prompt. These are treated as starting suggestions and the model should incorporate them into a broader list of themes.</p> <code>None</code> <p>Returns:</p> Type Description <code>type[BaseModel]</code> <p>The generated feature model containing discovered themes.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(col_name=None, return_series=False)\n</code></pre> <p>Estimate token counts for all rows of a given column using the <code>tiktoken</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Name of the column containing report summaries. Defaults to <code>summary_col</code>, which is generated after running <code>summarise</code>.</p> <code>None</code> <code>return_series</code> <code>bool</code> <p>Returns a pandas.Series of per-row token counts for that field if <code>True</code>, or an integer if <code>False</code>. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[int, Series]</code> <p>If <code>return_series</code> is <code>False</code>, returns an <code>int</code> representing the total sum of all token counts across all rows for the provided field. If <code>return_series</code> is <code>True</code>, returns a <code>pandas.Series</code> of token counts aligned to <code>self.reports</code> for the provided field.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.export_cache","title":"export_cache","text":"<pre><code>export_cache(path='extractor_cache.pkl')\n</code></pre> <p>Save the current cache to <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be created inside it.</p> <code>'extractor_cache.pkl'</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the written cache file.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.extract_features","title":"extract_features","text":"<pre><code>extract_features(\n    reports=None,\n    *,\n    feature_model=None,\n    produce_spans=False,\n    drop_spans=False,\n    force_assign=False,\n    allow_multiple=False,\n    schema_detail=\"minimal\",\n    extra_instructions=None,\n    skip_if_present=True,\n)\n</code></pre> <p>Run feature extraction for the given reports.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>DataFrame of reports to process. Defaults to the instance's stored reports if omitted.</p> <code>None</code> <code>feature_model</code> <code>type[BaseModel]</code> <p>Pydantic model describing the features to extract. Must be provided on first call or after calling <code>discover_themes</code>.</p> <code>None</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code>, create <code>spans_</code> versions of each feature to capture the supporting text snippets. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, remove all <code>spans_</code> columns from the returned DataFrame after extraction. If <code>produce_spans</code> is <code>False</code> a warning is emitted and no columns are dropped. Defaults to <code>False</code>.</p> <code>False</code> <code>force_assign</code> <code>bool</code> <p>When <code>True</code>, the LLM is instructed to avoid returning :data:<code>GeneralConfig.NOT_FOUND_TEXT</code> for any feature.</p> <code>False</code> <code>allow_multiple</code> <code>bool</code> <p>Allow a report to be assigned to multiple categories when <code>True</code>.</p> <code>False</code> <code>schema_detail</code> <code>('full', 'minimal')</code> <p>Level of detail for the feature schema included in the prompt.</p> <code>\"full\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions injected into each prompt before the schema.</p> <code>None</code> <code>skip_if_present</code> <code>bool</code> <p>When <code>True</code> (default), skip rows when any feature column already holds a non-missing value that is not equal to :data:<code>GeneralConfig.NOT_FOUND_TEXT</code>. This assumes the row has been processed previously and is logged in an instance of <code>Extractor.cache</code></p> <code>True</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.import_cache","title":"import_cache","text":"<pre><code>import_cache(path='extractor_cache.pkl')\n</code></pre> <p>Load cache from <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Full path to the cache file including the filename. If <code>path</code> is a directory, <code>extractor_cache.pkl</code> will be loaded from inside it.</p> <code>'extractor_cache.pkl'</code>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Reset internal caches and intermediate state.</p> <p>This clears any cached feature extraction results and token estimations so that <code>extract_features</code> can be run again on the same reports. The instance itself is returned to allow method chaining, e.g. <code>extractor.reset().extract_features()</code>.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.summarise","title":"summarise","text":"<pre><code>summarise(\n    result_col_name=\"summary\",\n    trim_intensity=\"medium\",\n    extra_instructions=None,\n)\n</code></pre> <p>Summarise selected report fields into one column using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>result_col_name</code> <code>str</code> <p>Name of the summary column. Defaults to <code>\"summary\"</code>.</p> <code>'summary'</code> <code>trim_intensity</code> <code>('low', 'medium', 'high', 'very high')</code> <p>Controls how concise the summary should be. Defaults to <code>\"medium\"</code>.</p> <code>\"low\"</code> <code>extra_instructions</code> <code>str</code> <p>Additional instructions to append to the prompt before the report excerpt.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new DataFrame identical to the one provided at initialisation with an extra summary column.</p>"},{"location":"reference/extractor/#pfd_toolkit.Extractor.tabulate","title":"tabulate","text":"<pre><code>tabulate(\n    columns=None,\n    labels=None,\n    *,\n    count_col=\"Count\",\n    pct_col=\"Percentage\",\n    df=None,\n)\n</code></pre> <p>Return a simple frequency table for extracted feature columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>str or list[str]</code> <p>Column name or list of column names to summarise. Defaults to all feature columns added by :meth:<code>extract_features</code> (excluding any <code>spans_</code> columns).</p> <code>None</code> <code>labels</code> <code>str or list[str]</code> <p>Human friendly label or list of labels corresponding to <code>columns</code>. If omitted, column names are used.</p> <code>None</code> <code>count_col</code> <code>str</code> <p>Column names for the count and percentage values in the output DataFrame. Defaults to <code>\"Count\"</code> and <code>\"Percentage\"</code>.</p> <code>'Count'</code> <code>pct_col</code> <code>str</code> <p>Column names for the count and percentage values in the output DataFrame. Defaults to <code>\"Count\"</code> and <code>\"Percentage\"</code>.</p> <code>'Count'</code> <code>df</code> <code>DataFrame</code> <p>DataFrame containing the columns to tabulate. Defaults to the reports stored on the instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame summarising the frequencies of the specified columns.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If :meth:<code>extract_features</code> has not been run yet.</p>"},{"location":"reference/llm/","title":"<code>LLM</code>","text":"<p>Wrapper around the OpenAI Python SDK for batch prompting.</p> <p>The helper provides:</p> <ul> <li><code>generate</code> for plain or vision-enabled prompts with optional pydantic   validation.</li> <li><code>_call_llm_fallback</code> used by the scraper when HTML and PDF heuristics   fail.</li> <li>Built-in back-off and host-wide throttling via a semaphore.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenAI (or proxy) API key. Defaults to <code>None</code> which expects the environment variable to be set.</p> <code>None</code> <code>model</code> <code>str</code> <p>Chat model name. Defaults to <code>\"gpt-4.1-mini\"</code>.</p> <code>'gpt-4.1-mini'</code> <code>base_url</code> <code>str or None</code> <p>Override the OpenAI endpoint. Defaults to <code>None</code>.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum parallel workers for batch calls and for the global semaphore. Defaults to <code>8</code>.</p> <code>8</code> <code>temperature</code> <code>float</code> <p>Sampling temperature used for all requests. Defaults to <code>0.0</code>.</p> <code>0.0</code> <code>seed</code> <code>int or None</code> <p>Deterministic seed value passed to the API. Defaults to <code>None</code>.</p> <code>None</code> <code>validation_attempts</code> <code>int</code> <p>Number of times to retry parsing LLM output into a pydantic model. Defaults to <code>2</code>.</p> <code>2</code> <code>timeout</code> <code>float | Timeout | None</code> <p>Override the HTTP timeout in seconds. <code>None</code> uses the OpenAI client default of 600 seconds.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>_sem</code> <code>Semaphore</code> <p>Global semaphore that limits concurrent requests to max_workers.</p> <code>client</code> <code>Client</code> <p>Low-level SDK client configured with key and base URL.</p> <p>Examples:</p> <pre><code>llm_client = LLM(api_key=\"sk-...\", model=\"gpt-4o-mini\", temperature=0.2,\n          timeout=600)\n</code></pre>"},{"location":"reference/llm/#pfd_toolkit.LLM.estimate_tokens","title":"estimate_tokens","text":"<pre><code>estimate_tokens(texts, model=None)\n</code></pre> <p>Return token counts for text using <code>tiktoken</code>.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>list[str] | str</code> <p>Input strings to tokenise.</p> required <code>model</code> <code>str</code> <p>Model name for selecting the encoding. Defaults to <code>self.model</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[int]</code> <p>Token counts in the same order as <code>texts</code>.</p>"},{"location":"reference/llm/#pfd_toolkit.LLM.generate","title":"generate","text":"<pre><code>generate(\n    prompts,\n    images_list=None,\n    response_format=None,\n    max_workers=None,\n    tqdm_extra_kwargs=None,\n)\n</code></pre> <p>Run many prompts either sequentially or in parallel.</p> <pre><code>    Parameters\n</code></pre> <pre><code>    prompts : list[str]\n        List of user prompts. One prompt per model call.\n\n    images_list : list[list[bytes]] or None, optional\n        For vision models: a parallel list where each inner list\n        holds **base64-encoded** JPEG pages for that prompt.  Use\n        *None* to send no images.\n\n    response_format : type[pydantic.BaseModel] or None, optional\n        If provided, each response is parsed into that model via the\n        *beta/parse* endpoint; otherwise a raw string is returned.\n\n    max_workers : int or None, optional\n        Thread count just for this batch. ``None`` uses the instance-wide\n        ``max_workers`` value. Defaults to ``None``.\n</code></pre> <pre><code>    Returns\n</code></pre> <pre><code>    list[Union[pydantic.BaseModel, str]]\n        Results in the same order as `prompts`.\n</code></pre> <pre><code>    Raises\n</code></pre> <pre><code>    openai.RateLimitError\n        Raised only if the exponential back-off exhausts all retries.\n    openai.APIConnectionError\n        Raised if network issues persist beyond the retry window.\n    openai.APITimeoutError\n        Raised if the API repeatedly times out.\n</code></pre> <pre><code>    Examples\n</code></pre> <pre><code>    Generate multiple summaries::\n\n        msgs = [\"Summarise:\n</code></pre> <p>\" + txt for txt in docs]             summaries = llm.generate(msgs, max_workers=8)</p>"},{"location":"reference/loader/","title":"<code>load_reports</code>","text":"<p>Load Prevention of Future Death reports as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2000-01-01\"</code>.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in <code>YYYY-MM-DD</code> format. Defaults to <code>\"2050-01-01\"</code>.</p> <code>'2050-01-01'</code> <code>n_reports</code> <code>int or None</code> <p>Keep only the most recent <code>n_reports</code> rows after filtering by date. <code>None</code> (the default) returns all rows.</p> <code>None</code> <code>clear_cache</code> <code>bool</code> <p>If <code>True</code>, force a fresh download of the dataset instead of using the cached copy. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Reports filtered by date, sorted newest first and optionally limited to <code>n_reports</code> rows.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If start_date is after end_date.</p> <code>FileNotFoundError</code> <p>If the dataset cannot be downloaded.</p> <p>Examples:</p> <pre><code>from pfd_toolkit import load_reports\ndf = load_reports(start_date=\"2020-01-01\", end_date=\"2022-12-31\", n_reports=100)\ndf.head()\n</code></pre>"},{"location":"reference/scraper/","title":"<code>Scraper</code>","text":"<p>Scrape UK \u201cPrevention of Future Death\u201d (PFD) reports into a pandas.DataFrame.</p> <p>The extractor runs in three cascading layers (<code>html \u2192 pdf \u2192 llm</code>), each independently switchable.</p> <ol> <li>HTML scrape \u2013 parse metadata and rich sections directly from    the web page.</li> <li>PDF fallback \u2013 download the attached PDF and extract text with    PyMuPDF for any missing fields.</li> <li>LLM fallback \u2013 delegate unresolved gaps to a Large Language    Model supplied via llm.</li> </ol> <p>Each layer can be enabled or disabled via <code>scraping_strategy</code>.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM | None</code> <p>Client implementing <code>_call_llm_fallback()</code>; required only when the LLM stage is enabled.</p> <code>None</code> <code>category</code> <code>str</code> <p>Judiciary category slug (e.g. <code>\"suicide\"</code>, <code>\"hospital_deaths\"</code>) or <code>\"all\"</code>.</p> <code>'all'</code> <code>start_date</code> <code>str</code> <p>Inclusive lower bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2000-01-01'</code> <code>end_date</code> <code>str</code> <p>Inclusive upper bound for the report date in the <code>YYYY-MM-DD</code> format.</p> <code>'2050-01-01'</code> <code>max_workers</code> <code>int</code> <p>Thread-pool size for concurrent scraping.</p> <code>10</code> <code>max_requests</code> <code>int</code> <p>Maximum simultaneous requests per host (enforced with a semaphore).</p> <code>5</code> <code>delay_range</code> <code>tuple[float, float] | None</code> <p>Random delay (seconds) before every request. Use <code>None</code> to disable (not recommended).</p> <code>(1, 2)</code> <code>timeout</code> <code>int</code> <p>Per-request timeout in seconds.</p> <code>60</code> <code>scraping_strategy</code> <code>list[int] | tuple[int, int, int]</code> <p>Defines the order in which HTML, PDF and LLM scraping are attempted. The sequence indexes correspond to <code>(HTML, PDF, LLM)</code>. Provide <code>-1</code> to disable a stage.  For example <code>[1, 2, -1]</code> runs HTML first, then PDF, and disables LLM scraping.</p> <code>[1, 2, 3]</code> <code>include_url</code> <code>bool</code> <p>Include the <code>url</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_id</code> <code>bool</code> <p>Include the <code>id</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_date</code> <code>bool</code> <p>Include the <code>date</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_coroner</code> <code>bool</code> <p>Include the <code>coroner</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_area</code> <code>bool</code> <p>Include the <code>area</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_receiver</code> <code>bool</code> <p>Include the <code>receiver</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_investigation</code> <code>bool</code> <p>Include the <code>investigation</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Include the <code>circumstances</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Include the <code>concerns</code> column. Defaults to <code>True</code>.</p> <code>True</code> <code>include_time_stamp</code> <code>bool</code> <p>Include a <code>date_scraped</code> column. Defaults to <code>False</code>.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Emit debug-level logs when True.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>reports</code> <code>DataFrame | None</code> <p>Cached result of the last call to <code>scrape_reports</code> or <code>top_up</code>.</p> <code>report_links</code> <code>list[str]</code> <p>URLs discovered by <code>get_report_links</code>.</p> <code>NOT_FOUND_TEXT</code> <code>str</code> <p>Placeholder value set when a field cannot be extracted.</p> <p>Examples:</p> <pre><code>from pfd_toolkit import Scraper\nscraper = Scraper(\n    category=\"suicide\",\n    start_date=\"2020-01-01\",\n    end_date=\"2022-12-31\",\n    scraping_strategy=[1, 2, 3],\n    llm=my_llm_client,\n)\ndf = scraper.scrape_reports()          # full scrape\nnewer_df = scraper.top_up(df)          # later \"top-up\"\nadded_llm_df = scraper.run_llm_fallback(df)  # apply LLM retro-actively\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.get_report_links","title":"get_report_links","text":"<pre><code>get_report_links()\n</code></pre> <p>Discover individual report URLs for the current query, across all pages.</p> <p>Iterates through _get_report_href_values (which collects URLs for a single page).</p> <p>Pagination continues until a page yields zero new links.</p> <p>Returns:</p> Type Description <code>list[str] | None</code> <p>All discovered URLs, or None if no links were found for the given category/date window.</p>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.run_llm_fallback","title":"run_llm_fallback","text":"<pre><code>run_llm_fallback(reports_df=None)\n</code></pre> <p>Ask the LLM to fill cells still set to <code>self.NOT_FOUND_TEXT</code>.</p> <p>Only the missing fields requested via <code>include_*</code> flags are sent to the model, along with the report\u2019s PDF bytes (when available).</p> <p>Parameters:</p> Name Type Description Default <code>reports_df</code> <code>DataFrame | None</code> <p>DataFrame to process. Defaults to <code>self.reports</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Same shape as <code>reports_df</code>, updated in place and re-cached to <code>self.reports</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no LLM client was supplied at construction time.</p> <p>Examples:</p> <p>Run the fallback step after scraping::</p> <pre><code>updated_df = scraper.run_llm_fallback()\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.scrape_reports","title":"scrape_reports","text":"<pre><code>scrape_reports()\n</code></pre> <p>Execute a full scrape with the Class configuration.</p> Workflow <ol> <li>Call <code>get_report_links</code>.</li> <li>Extract each report according to <code>scraping_strategy</code>.</li> <li>Cache the final DataFrame to <code>self.reports</code>.</li> </ol> <p>Returns:</p> Type Description <code>DataFrame</code> <p>One row per report.  Column presence matches the <code>include_*</code> flags. The DataFrame is empty if nothing was scraped.</p> <p>Examples:</p> <p>Scrape reports and inspect columns::</p> <pre><code>df = scraper.scrape_reports()\ndf.columns\n</code></pre>"},{"location":"reference/scraper/#pfd_toolkit.Scraper.top_up","title":"top_up","text":"<pre><code>top_up(\n    old_reports=None,\n    start_date=None,\n    end_date=None,\n    clean=False,\n)\n</code></pre> <p>Check for and append new PFD reports within the current parameters.</p> <p>If new links are found they are scraped and appended to <code>self.reports</code>. Any URL (or ID) already present in old_reports is skipped.</p> <p>Optionally, you can override the start_date and end_date parameters from <code>self</code> for this call only.</p> <p>Parameters:</p> Name Type Description Default <code>old_reports</code> <code>DataFrame | None</code> <p>Existing DataFrame. Defaults to <code>self.reports</code>.</p> <code>None</code> <code>start_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>end_date</code> <code>str | None</code> <p>Optionally override the scraper\u2019s date window for this call only.</p> <code>None</code> <code>clean</code> <code>bool</code> <p>When <code>True</code>, run the <code>Cleaner</code> on the newly scraped rows before merging them with existing reports.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame | None</code> <p>Updated DataFrame if new reports were added; None if no new records were found and old_reports was None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If old_reports lacks columns required for duplicate checks.</p> <p>Examples:</p> <p>Add new reports to an existing DataFrame::</p> <pre><code>updated = scraper.top_up(df, end_date=\"2023-01-01\")\nlen(updated) - len(df)  # number of new reports\n</code></pre>"},{"location":"reference/screener/","title":"<code>Screener</code>","text":"<p>Classifies a list of report texts against a user-defined topic using an LLM.</p> <p>This class takes a DataFrame of reports, a user query, and various configuration options to classify whether each report matches the query. It can either filter the DataFrame to return only matching reports or add a classification column to the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>LLM</code> <p>An instance of the LLM class from <code>pfd_toolkit</code>.</p> <code>None</code> <code>reports</code> <code>DataFrame</code> <p>A DataFrame containing Prevention of Future Death reports.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If True, print more detailed logs. Defaults to False.</p> <code>False</code> <code>include_date</code> <code>bool</code> <p>Flag to determine if the 'date' column is included. Defaults to False.</p> <code>False</code> <code>include_coroner</code> <code>bool</code> <p>Flag to determine if the 'coroner' column is included. Defaults to False.</p> <code>False</code> <code>include_area</code> <code>bool</code> <p>Flag to determine if the 'area' column is included. Defaults to False.</p> <code>False</code> <code>include_receiver</code> <code>bool</code> <p>Flag to determine if the 'receiver' column is included. Defaults to False.</p> <code>False</code> <code>include_investigation</code> <code>bool</code> <p>Flag to determine if the 'investigation' column is included. Defaults to True.</p> <code>True</code> <code>include_circumstances</code> <code>bool</code> <p>Flag to determine if the 'circumstances' column is included. Defaults to True.</p> <code>True</code> <code>include_concerns</code> <code>bool</code> <p>Flag to determine if the 'concerns' column is included. Defaults to True.</p> <code>True</code> <p>Examples:</p> <pre><code>user_topic = \"medication errors\"\nllm_client = LLM()\nscreener = Screener(llm=llm_client, reports=reports_df)\nscreened_reports = screener.screen_reports(user_query=user_topic)\nprint(f\"Found {len(screened_reports)} report(s) on '{user_topic}'.\")\n</code></pre>"},{"location":"reference/screener/#pfd_toolkit.Screener.screen_reports","title":"screen_reports","text":"<pre><code>screen_reports(\n    reports=None,\n    user_query=None,\n    filter_df=True,\n    result_col_name=\"matches_query\",\n    produce_spans=False,\n    drop_spans=False,\n)\n</code></pre> <p>Classifies reports in the DataFrame against the user-defined topic using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>reports</code> <code>DataFrame</code> <p>If provided, this DataFrame will be used for screening, replacing any DataFrame stored in the instance for this call.</p> <code>None</code> <code>user_query</code> <code>str</code> <p>If provided, this query will be used, overriding any query stored in the instance for this call. The prompt template will be rebuilt.</p> <code>None</code> <code>filter_df</code> <code>bool</code> <p>If <code>True</code> the returned DataFrame is filtered to only matching reports. Defaults to <code>True</code>.</p> <code>True</code> <code>result_col_name</code> <code>str</code> <p>Name of the boolean column added when <code>filter_df</code> is <code>False</code>. Defaults to <code>\"matches_query\"</code>.</p> <code>'matches_query'</code> <code>produce_spans</code> <code>bool</code> <p>When <code>True</code> a <code>spans_matches_topic</code> column is created containing the text snippet that justified the classification. Defaults to <code>False</code>.</p> <code>False</code> <code>drop_spans</code> <code>bool</code> <p>When <code>True</code> and <code>produce_spans</code> is also <code>True</code>, the <code>spans_matches_topic</code> column is removed from the returned DataFrame. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Either a filtered DataFrame (if <code>filter_df</code> is <code>True</code>), or the original DataFrame with an added classification column.</p> <p>Examples:</p> <pre><code>reports_df = pd.DataFrame(data)\nscreener = Screener(LLM(), reports=reports_df)\n\n# Screen reports with the initial query\nfiltered_df = screener.screen_reports(user_query=\"medication safety\")\n\n# Screen the same reports with a new query and add a classification column\nclassified_df = screener.screen_reports(user_query=\"tree safety\", filter_df=False)\n</code></pre>"},{"location":"screener/","title":"Screen reports for relevancy","text":"<p>The <code>Screener</code> class filters PFD reports according to a plain\u2011English query. This class lets you submit a user-query (e.g. \"Deaths in police custody\") and have the model screen thousands of reports for possible matches.</p> <p>The following pages cover how to use it:</p> <ul> <li>Getting started \u2013 run your first screen and understand why an LLM beats simple keyword search.</li> <li>Additional options \u2013 annotate instead of filtering and control which columns are read.</li> <li>Tips for writing a good user query \u2013 craft prompts that get accurate results.</li> </ul>"},{"location":"screener/basics/","title":"Getting started","text":"<p>Natural-language filtering is one of the headline features of PFD Toolkit. The <code>Screener</code> class lets you describe a topic in plain English \u2013 e.g. \"deaths in police custody\" \u2013 and have an LLM screen reports, delivering you a curated dataset.</p> <p>To use the <code>Screener</code> you'll first need to set up an LLM client if you haven't already.</p>"},{"location":"screener/basics/#a-minimal-example","title":"A minimal example","text":"<p>First, import the necessary modules, load reports and set up an <code>LLM</code> client:</p> <pre><code>from pfd_toolkit import load_reports, LLM, Screener\n\n# Grab all reports from 2024\nreports = load_reports(start_date=\"2023-01-01\",\n                       end_date=\"2023-12-31\")\n\n# Set up your LLM client\nllm_client = LLM(api_key=YOUR-API-KEY)\n</code></pre> <p>Then define a <code>user_query</code> which describes the reports you're interested in. Pass this query as an argument to <code>screen_reports()</code> and you'll be given a filtered dataset containing matching reports.</p> <pre><code>user_query = \"Deaths in police custody.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports\n)\n\npolice_df = screener.screen_reports(user_query=user_query, filter_df=True)\n</code></pre> <p><code>police_df</code> will now only contain reports which the LLM believes matches your query.</p>"},{"location":"screener/basics/#why-not-just-have-a-normal-search-function","title":"Why not just have a normal \"search\" function?","text":"<p>A keyword search is only as good as the exact words you type. Coroners, however, don't always follow a shared vocabulary. The same idea can surface in wildly different forms:</p> <ul> <li>Under-staffing might be written as \"staff shortages,\" \"inadequate nurse cover,\" or even \"resource constraints.\"</li> <li>Suicide in prisons may masquerade as \"self-inflicted injury while remanded,\" \"ligature event in cell,\" or may not even appear together in the same sentence.</li> </ul> <p>A keyword filter misses these variants unless you identify every synonym in advance. By contrast, an <code>LLM</code> understands the context behind your query and links the phrasing for you, which is exactly what <code>Screener</code> taps into.</p>"},{"location":"screener/options/","title":"Additional options","text":""},{"location":"screener/options/#annotation-vs-filtering","title":"Annotation vs. filtering","text":"<p>If <code>filter_df</code> is True (the default) <code>Screener</code> returns a trimmed DataFrame that contains only the reports the LLM marked as relevant to your query.</p> <p>Setting it to False activates annotate mode: every report/row from your original DataFrame is kept, and a boolean column is added denoting whether the report met your query or not. You can also rename this column with <code>result_col_name</code>.</p> <p>A common workflow is to screen once with <code>filter_df=False</code>, inspect a few borderline cases, then rerun with <code>filter_df=True</code> once you trust the settings.</p> <pre><code>screener = Screener(\n    llm=llm_client,\n    reports=reports,\n)\n\nannotated = screener.screen_reports(\n    user_query=user_query,\n    filter_df=False,    # &lt;--- create annotation column; don't filter out\n    result_col_name='custody_match'     # &lt;--- name of annotation column\n)\n</code></pre>"},{"location":"screener/options/#choosing-which-columns-the-llm-sees","title":"Choosing which columns the LLM 'sees'","text":"<p>By default the LLM model reads the narrative heavyweight sections of each report: investigation, circumstances and concerns. You can expose or hide any field with <code>include_*</code> flags.</p> <p>For example, if you are screening based on a specific cause of death, then you should consider setting <code>include_concerns</code> to False, as including this won't benefit your search.</p> <p>By contrast, if you are searching for a specific concern, then setting <code>include_investigation</code> and <code>include_circumstances</code> to False may improve accuracy, speed up your code, and lead to cheaper LLM calls.</p> <pre><code>user_query = \"Death from insulin overdose due to misprogrammed insulin pumps.\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n    include_concerns=False    # &lt;--- Our query doesn't need this section\n)\n\nresult = screener.screen_reports(user_query=user_query)\n</code></pre> <p>In another example, let's say we are only interested in reports sent to a Member of Parliament. We'll want to turn off all default sections and only read from the receiver column.</p> <pre><code>user_query = \"Whether the report was sent to a Member of Parliament (MP)\"\n\nscreener = Screener(\n    llm=llm_client,\n    reports=reports,\n\n    # Turn off the defaults...\n    include_investigation=False,\n    include_circumstances=False,\n    include_concerns=False,\n\n    include_receiver=True       # &lt;--- Read from receiver section\n)\n\nresult = screener.screen_reports(user_query=user_query)\n</code></pre>"},{"location":"screener/options/#all-options-and-defaults","title":"All options and defaults","text":"Flag Report section What it's useful for Default <code>include_coroner</code> Coroner\u2019s name Simply the name of the coroner. Rarely needed for screening. <code>False</code> <code>include_area</code> Coroner\u2019s area Useful for geographic questions, e.g.\u00a0deaths in South-East England. <code>False</code> <code>include_receiver</code> Receiver(s) of the report Great for accountability queries, e.g. reports sent to NHS Wales. <code>False</code> <code>include_investigation</code> \u201cInvestigation &amp; Inquest\u201d section Contains procedural detail about the inquest. <code>True</code> <code>include_circumstances</code> \u201cCircumstances of Death\u201d section Describes what actually happened; holds key facts about the death. <code>True</code> <code>include_concerns</code> \u201cCoroner\u2019s Concerns\u201d section Lists the issues the coroner wants addressed \u2014 ideal for risk screening. <code>True</code>"},{"location":"screener/options/#returning-text-spans","title":"Returning text spans","text":"<p>Set <code>produce_spans=True</code> when calling <code>.screen_reports()</code> to capture the exact lines from the report that justified each classification. A new column called <code>spans_matches_topic</code> will be created containing these verbatim snippets. If you only want to use the spans internally, pass <code>drop_spans=True</code> to remove the column from the returned DataFrame after screening.</p> <pre><code>screener = Screener(llm=llm_client,\n                    reports=reports)\nannotated = screener.screen_reports(user_query=\"needle\", produce_spans=True, drop_spans=False)\n</code></pre>"},{"location":"screener/tips/","title":"Tips for writing a good user query","text":"<ol> <li>Stick to one core idea. Give the LLM a single, clear subject: \u201cfalls from hospital beds,\u201d \u201ccarbon-monoxide poisoning at home.\u201d In general, the shorter the prompt, the less room for misinterpretation.</li> <li>Avoid nested logic. Complex clauses like \u201csuicide and medication error but not in custody\u201d dilute the signal. Consider running separate screens (suicide; medication error; in custody) and combine or subtract results later with pandas.</li> <li>Let the model handle synonyms. You don\u2019t need \u201cdefective, faulty, malfunctioning\u201d all in the same query; \u201cmalfunctioning defibrillators\u201d is enough.</li> <li>Use positive phrasing. Negations (e.g. \u201cnot related to COVID-19\u201d) can flip the model\u2019s reasoning. Screen positively, set <code>filter_df</code> to False, then drop rows in pandas.</li> <li>Keep it readable. If your query needs multiple commas or parentheses, break it up. A one-line statement without side notes usually performs best.</li> </ol> <p>Examples:</p>  Less-effective query Why it struggles  Better query \u201cDeaths where someone slipped or fell in hospital corridors or patient rooms and maybe had fractures but not clinics\u201d Too long, multiple settings, negative clause \u201cFalls on inpatient wards\u201d \u201cFires or explosions causing death at home including gas leaks but not industrial accidents\u201d Mixes two ideas (home vs. industrial) plus a negation \u201cDomestic gas explosions\u201d \u201cCases involving children and allergic reactions to nuts during school outings\u201d Several concepts (age, allergen, setting) \u201cFatal nut allergy on school trip\u201d \u201cRailway incidents that resulted in death due to being hit by train while trespassing or at crossings\u201d Two scenarios joined by \u201cor\u201d; verbose \u201cTrespasser struck by train\u201d \u201cPatients dying because an ambulance was late or there was delay in emergency services arrival or they couldn't get one\u201d Chain of synonyms and clauses \u201cDeath from delayed ambulance\u201d \u201cErrors in giving anaesthesia, like too much anaesthetic, wrong drug, problems with intubation, etc.\u201d Long list invites confusion; \u201cetc.\u201d is vague \u201cAnaesthesia error\u201d"}]}